Unable to compile CoConv C++ implementation. Falling back to Python version.
[Errno 2] No such file or directory: '/home/lh/.conda/envs/oadtr/lib/python3.8/site-packages/continual/conv.cpp'
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'conv_flops_counter_hook'
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'MODULES_MAPPING'
Not using distributed mode
lr:0.0001
batch_size:128
weight_decay:0.0001
epochs:5
resize_feature:False
lr_drop:1
clip_max_norm:1.0
dataparallel:False
removelog:False
version:v3
query_num:8
decoder_layers:5
decoder_embedding_dim:1024
decoder_embedding_dim_out:1024
decoder_attn_dropout_rate:0.1
decoder_num_heads:4
classification_pred_loss_coef:0.5
enc_layers:64
lr_backbone:0.0001
feature:tvseries_kin_features.pickle
dim_feature:4096
patch_dim:1
embedding_dim:1024
num_heads:8
num_layers:2
attn_dropout_rate:0.1
positional_encoding_type:recycling_fixed
num_embeddings:127
hidden_dim:1024
dropout_rate:0.1
numclass:31
classification_x_loss_coef:0.3
classification_h_loss_coef:1
similar_loss_coef:0.1
margin:1.0
dataset:tvseries
dataset_file:data/data_info_new.json
frozen_weights:None
thumos_data_path:/home/dancer/mycode/Temporal.Online.Detection/Online.TRN.Pytorch/preprocess/
thumos_anno_path:data/thumos_{}_anno.pickle
remove_difficult:False
device:cuda
output_dir:models
seed:1
resume:
start_epoch:1
eval:False
num_workers:8
world_size:1
dist_url:tcp://127.0.0.1:12342
train_session_set:['24_ep1', '24_ep2', '24_ep3', 'Breaking_Bad_ep1', 'Breaking_Bad_ep2', 'How_I_Met_Your_Mother_ep1', 'How_I_Met_Your_Mother_ep2', 'How_I_Met_Your_Mother_ep3', 'How_I_Met_Your_Mother_ep4', 'How_I_Met_Your_Mother_ep5', 'How_I_Met_Your_Mother_ep6', 'Mad_Men_ep1', 'Mad_Men_ep2', 'Modern_Family_ep1', 'Modern_Family_ep2', 'Modern_Family_ep3', 'Modern_Family_ep4', 'Modern_Family_ep6', 'Sons_of_Anarchy_ep1', 'Sons_of_Anarchy_ep2']
test_session_set:['24_ep4', 'Breaking_Bad_ep3', 'Mad_Men_ep3', 'How_I_Met_Your_Mother_ep7', 'How_I_Met_Your_Mother_ep8', 'Modern_Family_ep5', 'Sons_of_Anarchy_ep3']
class_index:['background', 'Pick something up', 'Point', 'Drink', 'Stand up', 'Run', 'Sit down', 'Read', 'Smoke', 'Drive car', 'Open door', 'Give something', 'Use computer', 'Write', 'Go down stairway', 'Close door', 'Throw something', 'Go up stairway', 'Get in/out of car', 'Hang up phone', 'Eat', 'Answer phone', 'Dress up', 'Clap', 'Undress', 'Kiss', 'Fall/trip', 'Wave', 'Pour', 'Punch', 'Fire weapon']
distributed:False
position encoding : recycling_fixed
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Sequential(
  16.814 M, 99.939% Params, 0.412 GMac, 100.000% MACs, 
  (0): Linear(4.195 M, 24.936% Params, 0.004 GMac, 1.018% MACs, in_features=4096, out_features=1024, bias=True, channel_dim=1)
  (1): RecyclingPositionalEncoding(
    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 
    (pe): CyclicPositionalEncoding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
  )
  (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
  (3): Sequential(
    12.587 M, 74.814% Params, 0.408 GMac, 98.974% MACs, 
    (0): Sequential(
      6.294 M, 37.407% Params, 0.205 GMac, 49.804% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, reduce=reduce_sum
        (0): RetroactiveUnity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 63)
        (1): CoReMultiheadAttention(
          4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(Sequential(
        2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
        (0): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
        (1): Residual(
          2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
          (fn): Sequential(
            2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
            (0): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
            (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
            (3): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          )
        )
        (2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
      ))
    )
    (1): Lambda(Sequential(
      6.294 M, 37.407% Params, 0.203 GMac, 49.170% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, reduce=sum_last_pairs
        (0): SelectOrDelay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): CoSiMultiheadAttention(
          4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
      (2): BroadcastReduce(
        2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, reduce=reduce_sum
        (0): Delay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): Sequential(
          2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, 
          (0): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
          (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          (3): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
        )
      )
      (3): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
    ), takes_time=True)
    (2): Lambda(unity, squeeze_last, squeeze_last, takes_time=True)
  )
  (4): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
  (5): Linear(0.032 M, 0.189% Params, 0.0 GMac, 0.008% MACs, in_features=1024, out_features=31, bias=True, channel_dim=1)
)
Model FLOPs: 411997217.0
Model params: 16824351
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| Active memory         |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   83968 KB |   83968 KB |   83968 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16445 KB |   16445 KB |  302954 KB |  286508 KB |
|---------------------------------------------------------------------------|
| Allocations           |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| Active allocs         |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |    1458    |    1448    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Memory state pre, max, post inference: 69143040 71518720 69143040
Loaded tvseries_kin_features.pickle
Loaded tvseries_kin_features.pickle
Start training
Epoch: [1]  [   0/1511]  eta: 1:16:55  lr: 0.000100  loss: 2.6369 (2.6369)  labels_encoder: 2.6369 (2.6369)  labels_encoder_unscaled: 2.6369 (2.6369)  time: 3.0548  data: 2.4167  max mem: 1013
Epoch: [1]  [  50/1511]  eta: 0:04:30  lr: 0.000100  loss: 1.0150 (1.1121)  labels_encoder: 1.0150 (1.1121)  labels_encoder_unscaled: 1.0150 (1.1121)  time: 0.1276  data: 0.0002  max mem: 1206
Epoch: [1]  [ 100/1511]  eta: 0:03:37  lr: 0.000100  loss: 0.9775 (1.0541)  labels_encoder: 0.9775 (1.0541)  labels_encoder_unscaled: 0.9775 (1.0541)  time: 0.1129  data: 0.0002  max mem: 1206
Epoch: [1]  [ 150/1511]  eta: 0:03:18  lr: 0.000100  loss: 0.9194 (1.0070)  labels_encoder: 0.9194 (1.0070)  labels_encoder_unscaled: 0.9194 (1.0070)  time: 0.1364  data: 0.0002  max mem: 1206
Epoch: [1]  [ 200/1511]  eta: 0:03:05  lr: 0.000100  loss: 0.8476 (0.9728)  labels_encoder: 0.8476 (0.9728)  labels_encoder_unscaled: 0.8476 (0.9728)  time: 0.1202  data: 0.0002  max mem: 1206
Epoch: [1]  [ 250/1511]  eta: 0:02:57  lr: 0.000100  loss: 0.8304 (0.9522)  labels_encoder: 0.8304 (0.9522)  labels_encoder_unscaled: 0.8304 (0.9522)  time: 0.1391  data: 0.0002  max mem: 1206
Epoch: [1]  [ 300/1511]  eta: 0:02:49  lr: 0.000100  loss: 0.9066 (0.9432)  labels_encoder: 0.9066 (0.9432)  labels_encoder_unscaled: 0.9066 (0.9432)  time: 0.1236  data: 0.0002  max mem: 1206
Epoch: [1]  [ 350/1511]  eta: 0:02:39  lr: 0.000100  loss: 0.8654 (0.9316)  labels_encoder: 0.8654 (0.9316)  labels_encoder_unscaled: 0.8654 (0.9316)  time: 0.1227  data: 0.0002  max mem: 1206
Epoch: [1]  [ 400/1511]  eta: 0:02:30  lr: 0.000100  loss: 0.7976 (0.9184)  labels_encoder: 0.7976 (0.9184)  labels_encoder_unscaled: 0.7976 (0.9184)  time: 0.1191  data: 0.0002  max mem: 1206
Epoch: [1]  [ 450/1511]  eta: 0:02:22  lr: 0.000100  loss: 0.7530 (0.9080)  labels_encoder: 0.7530 (0.9080)  labels_encoder_unscaled: 0.7530 (0.9080)  time: 0.1283  data: 0.0002  max mem: 1206
Epoch: [1]  [ 500/1511]  eta: 0:02:15  lr: 0.000100  loss: 0.8138 (0.8987)  labels_encoder: 0.8138 (0.8987)  labels_encoder_unscaled: 0.8138 (0.8987)  time: 0.1321  data: 0.0002  max mem: 1206
Epoch: [1]  [ 550/1511]  eta: 0:02:07  lr: 0.000100  loss: 0.7908 (0.8882)  labels_encoder: 0.7908 (0.8882)  labels_encoder_unscaled: 0.7908 (0.8882)  time: 0.1198  data: 0.0002  max mem: 1206
Epoch: [1]  [ 600/1511]  eta: 0:02:00  lr: 0.000100  loss: 0.7865 (0.8803)  labels_encoder: 0.7865 (0.8803)  labels_encoder_unscaled: 0.7865 (0.8803)  time: 0.1231  data: 0.0002  max mem: 1206
Epoch: [1]  [ 650/1511]  eta: 0:01:53  lr: 0.000100  loss: 0.7284 (0.8731)  labels_encoder: 0.7284 (0.8731)  labels_encoder_unscaled: 0.7284 (0.8731)  time: 0.1323  data: 0.0002  max mem: 1206
Epoch: [1]  [ 700/1511]  eta: 0:01:47  lr: 0.000100  loss: 0.7298 (0.8654)  labels_encoder: 0.7298 (0.8654)  labels_encoder_unscaled: 0.7298 (0.8654)  time: 0.1259  data: 0.0002  max mem: 1206
Epoch: [1]  [ 750/1511]  eta: 0:01:40  lr: 0.000100  loss: 0.7944 (0.8607)  labels_encoder: 0.7944 (0.8607)  labels_encoder_unscaled: 0.7944 (0.8607)  time: 0.1361  data: 0.0002  max mem: 1206
Epoch: [1]  [ 800/1511]  eta: 0:01:34  lr: 0.000100  loss: 0.6968 (0.8518)  labels_encoder: 0.6968 (0.8518)  labels_encoder_unscaled: 0.6968 (0.8518)  time: 0.1359  data: 0.0002  max mem: 1206
Epoch: [1]  [ 850/1511]  eta: 0:01:27  lr: 0.000100  loss: 0.7275 (0.8455)  labels_encoder: 0.7275 (0.8455)  labels_encoder_unscaled: 0.7275 (0.8455)  time: 0.1234  data: 0.0002  max mem: 1206
Epoch: [1]  [ 900/1511]  eta: 0:01:20  lr: 0.000100  loss: 0.6781 (0.8375)  labels_encoder: 0.6781 (0.8375)  labels_encoder_unscaled: 0.6781 (0.8375)  time: 0.1409  data: 0.0002  max mem: 1206
Epoch: [1]  [ 950/1511]  eta: 0:01:14  lr: 0.000100  loss: 0.6434 (0.8309)  labels_encoder: 0.6434 (0.8309)  labels_encoder_unscaled: 0.6434 (0.8309)  time: 0.1261  data: 0.0002  max mem: 1206
Epoch: [1]  [1000/1511]  eta: 0:01:07  lr: 0.000100  loss: 0.7084 (0.8237)  labels_encoder: 0.7084 (0.8237)  labels_encoder_unscaled: 0.7084 (0.8237)  time: 0.1305  data: 0.0002  max mem: 1206
Epoch: [1]  [1050/1511]  eta: 0:01:00  lr: 0.000100  loss: 0.7228 (0.8186)  labels_encoder: 0.7228 (0.8186)  labels_encoder_unscaled: 0.7228 (0.8186)  time: 0.1246  data: 0.0002  max mem: 1206
Epoch: [1]  [1100/1511]  eta: 0:00:54  lr: 0.000100  loss: 0.6923 (0.8142)  labels_encoder: 0.6923 (0.8142)  labels_encoder_unscaled: 0.6923 (0.8142)  time: 0.1254  data: 0.0002  max mem: 1206
Epoch: [1]  [1150/1511]  eta: 0:00:47  lr: 0.000100  loss: 0.6615 (0.8083)  labels_encoder: 0.6615 (0.8083)  labels_encoder_unscaled: 0.6615 (0.8083)  time: 0.1320  data: 0.0002  max mem: 1206
Epoch: [1]  [1200/1511]  eta: 0:00:40  lr: 0.000100  loss: 0.6757 (0.8036)  labels_encoder: 0.6757 (0.8036)  labels_encoder_unscaled: 0.6757 (0.8036)  time: 0.1341  data: 0.0002  max mem: 1206
Epoch: [1]  [1250/1511]  eta: 0:00:34  lr: 0.000100  loss: 0.6981 (0.7993)  labels_encoder: 0.6981 (0.7993)  labels_encoder_unscaled: 0.6981 (0.7993)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [1]  [1300/1511]  eta: 0:00:27  lr: 0.000100  loss: 0.7220 (0.7956)  labels_encoder: 0.7220 (0.7956)  labels_encoder_unscaled: 0.7220 (0.7956)  time: 0.1324  data: 0.0002  max mem: 1206
Epoch: [1]  [1350/1511]  eta: 0:00:21  lr: 0.000100  loss: 0.6457 (0.7915)  labels_encoder: 0.6457 (0.7915)  labels_encoder_unscaled: 0.6457 (0.7915)  time: 0.1411  data: 0.0002  max mem: 1206
Epoch: [1]  [1400/1511]  eta: 0:00:14  lr: 0.000100  loss: 0.7089 (0.7878)  labels_encoder: 0.7089 (0.7878)  labels_encoder_unscaled: 0.7089 (0.7878)  time: 0.1356  data: 0.0002  max mem: 1206
Epoch: [1]  [1450/1511]  eta: 0:00:08  lr: 0.000100  loss: 0.6960 (0.7838)  labels_encoder: 0.6960 (0.7838)  labels_encoder_unscaled: 0.6960 (0.7838)  time: 0.1261  data: 0.0002  max mem: 1206
Epoch: [1]  [1500/1511]  eta: 0:00:01  lr: 0.000100  loss: 0.6096 (0.7788)  labels_encoder: 0.6096 (0.7788)  labels_encoder_unscaled: 0.6096 (0.7788)  time: 0.1158  data: 0.0003  max mem: 1206
Epoch: [1]  [1510/1511]  eta: 0:00:00  lr: 0.000100  loss: 0.6240 (0.7781)  labels_encoder: 0.6240 (0.7781)  labels_encoder_unscaled: 0.6240 (0.7781)  time: 0.1136  data: 0.0002  max mem: 1206
Epoch: [1] Total time: 0:03:19 (0.1320 s / it)
Averaged stats: lr: 0.000100  loss: 0.6240 (0.7781)  labels_encoder: 0.6240 (0.7781)  labels_encoder_unscaled: 0.6240 (0.7781)
Test:  [  0/559]  eta: 0:15:18  loss: 0.6077 (0.6077)  labels_encoder: 0.6077 (0.6077)  labels_encoder_unscaled: 0.6077 (0.6077)  time: 1.6432  data: 1.5730  max mem: 1206
Test:  [ 50/559]  eta: 0:00:47  loss: 0.9063 (0.9345)  labels_encoder: 0.9063 (0.9345)  labels_encoder_unscaled: 0.9063 (0.9345)  time: 0.0577  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:34  loss: 0.3545 (0.8706)  labels_encoder: 0.3545 (0.8706)  labels_encoder_unscaled: 0.3545 (0.8706)  time: 0.0572  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.5409 (0.8851)  labels_encoder: 0.5409 (0.8851)  labels_encoder_unscaled: 0.5409 (0.8851)  time: 0.0592  data: 0.0010  max mem: 1206
Test:  [200/559]  eta: 0:00:24  loss: 0.7204 (0.8352)  labels_encoder: 0.7204 (0.8352)  labels_encoder_unscaled: 0.7204 (0.8352)  time: 0.0636  data: 0.0010  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.8238 (0.9486)  labels_encoder: 0.8238 (0.9486)  labels_encoder_unscaled: 0.8238 (0.9486)  time: 0.0623  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.2141 (1.0115)  labels_encoder: 1.2141 (1.0115)  labels_encoder_unscaled: 1.2141 (1.0115)  time: 0.0598  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7896 (1.0395)  labels_encoder: 0.7896 (1.0395)  labels_encoder_unscaled: 0.7896 (1.0395)  time: 0.0531  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3844 (1.0177)  labels_encoder: 0.3844 (1.0177)  labels_encoder_unscaled: 0.3844 (1.0177)  time: 0.0540  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.5345 (0.9972)  labels_encoder: 0.5345 (0.9972)  labels_encoder_unscaled: 0.5345 (0.9972)  time: 0.0564  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5464 (0.9732)  labels_encoder: 0.5464 (0.9732)  labels_encoder_unscaled: 0.5464 (0.9732)  time: 0.0605  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.8062 (0.9449)  labels_encoder: 0.8062 (0.9449)  labels_encoder_unscaled: 0.8062 (0.9449)  time: 0.0471  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.4560 (0.9378)  labels_encoder: 0.4560 (0.9378)  labels_encoder_unscaled: 0.4560 (0.9378)  time: 0.0371  data: 0.0001  max mem: 1206
Test: Total time: 0:00:34 (0.0609 s / it)
Averaged stats: loss: 0.4560 (0.9378)  labels_encoder: 0.4560 (0.9378)  labels_encoder_unscaled: 0.4560 (0.9378)
(21, 71496)
(21, 71496)
[Epoch-1] [IDU-tvseries_kin_features.pickle] mAP: 0.1105, mcAP: 0.8698

BaseballPitch: 0.0244
BasketballDunk: 0.0848
Billiards: 0.0053
CleanAndJerk: 0.3968
CliffDiving: 0.4560
CricketBowling: 0.0482
CricketShot: 0.0676
Diving: 0.0073
FrisbeeCatch: 0.1084
GolfSwing: 0.0621
HammerThrow: 0.0826
HighJump: 0.0236
JavelinThrow: 0.0495
LongJump: 0.2179
PoleVault: 0.0828
Shotput: 0.1117
SoccerPenalty: 0.0372
TennisSwing: 0.1881
ThrowDiscus: 0.0330
VolleyballSpiking: 0.1230
Epoch: [2]  [   0/1511]  eta: 0:51:23  lr: 0.000010  loss: 0.8297 (0.8297)  labels_encoder: 0.8297 (0.8297)  labels_encoder_unscaled: 0.8297 (0.8297)  time: 2.0406  data: 1.8697  max mem: 1206
Epoch: [2]  [  50/1511]  eta: 0:03:54  lr: 0.000010  loss: 0.5733 (0.6078)  labels_encoder: 0.5733 (0.6078)  labels_encoder_unscaled: 0.5733 (0.6078)  time: 0.1183  data: 0.0002  max mem: 1206
Epoch: [2]  [ 100/1511]  eta: 0:03:19  lr: 0.000010  loss: 0.6185 (0.6262)  labels_encoder: 0.6185 (0.6262)  labels_encoder_unscaled: 0.6185 (0.6262)  time: 0.1204  data: 0.0002  max mem: 1206
Epoch: [2]  [ 150/1511]  eta: 0:03:07  lr: 0.000010  loss: 0.6114 (0.6240)  labels_encoder: 0.6114 (0.6240)  labels_encoder_unscaled: 0.6114 (0.6240)  time: 0.1377  data: 0.0002  max mem: 1206
Epoch: [2]  [ 200/1511]  eta: 0:02:59  lr: 0.000010  loss: 0.5664 (0.6192)  labels_encoder: 0.5664 (0.6192)  labels_encoder_unscaled: 0.5664 (0.6192)  time: 0.1286  data: 0.0002  max mem: 1206
Epoch: [2]  [ 250/1511]  eta: 0:02:51  lr: 0.000010  loss: 0.6218 (0.6161)  labels_encoder: 0.6218 (0.6161)  labels_encoder_unscaled: 0.6218 (0.6161)  time: 0.1343  data: 0.0002  max mem: 1206
Epoch: [2]  [ 300/1511]  eta: 0:02:41  lr: 0.000010  loss: 0.5562 (0.6108)  labels_encoder: 0.5562 (0.6108)  labels_encoder_unscaled: 0.5562 (0.6108)  time: 0.1238  data: 0.0002  max mem: 1206
Epoch: [2]  [ 350/1511]  eta: 0:02:33  lr: 0.000010  loss: 0.6044 (0.6103)  labels_encoder: 0.6044 (0.6103)  labels_encoder_unscaled: 0.6044 (0.6103)  time: 0.1259  data: 0.0002  max mem: 1206
Epoch: [2]  [ 400/1511]  eta: 0:02:26  lr: 0.000010  loss: 0.5429 (0.6071)  labels_encoder: 0.5429 (0.6071)  labels_encoder_unscaled: 0.5429 (0.6071)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [2]  [ 450/1511]  eta: 0:02:19  lr: 0.000010  loss: 0.5369 (0.6021)  labels_encoder: 0.5369 (0.6021)  labels_encoder_unscaled: 0.5369 (0.6021)  time: 0.1286  data: 0.0002  max mem: 1206
Epoch: [2]  [ 500/1511]  eta: 0:02:12  lr: 0.000010  loss: 0.5183 (0.5997)  labels_encoder: 0.5183 (0.5997)  labels_encoder_unscaled: 0.5183 (0.5997)  time: 0.1268  data: 0.0002  max mem: 1206
Epoch: [2]  [ 550/1511]  eta: 0:02:06  lr: 0.000010  loss: 0.5498 (0.5967)  labels_encoder: 0.5498 (0.5967)  labels_encoder_unscaled: 0.5498 (0.5967)  time: 0.1272  data: 0.0002  max mem: 1206
Epoch: [2]  [ 600/1511]  eta: 0:01:59  lr: 0.000010  loss: 0.6203 (0.5952)  labels_encoder: 0.6203 (0.5952)  labels_encoder_unscaled: 0.6203 (0.5952)  time: 0.1402  data: 0.0002  max mem: 1206
Epoch: [2]  [ 650/1511]  eta: 0:01:53  lr: 0.000010  loss: 0.6317 (0.5969)  labels_encoder: 0.6317 (0.5969)  labels_encoder_unscaled: 0.6317 (0.5969)  time: 0.1309  data: 0.0002  max mem: 1206
Epoch: [2]  [ 700/1511]  eta: 0:01:46  lr: 0.000010  loss: 0.5775 (0.5940)  labels_encoder: 0.5775 (0.5940)  labels_encoder_unscaled: 0.5775 (0.5940)  time: 0.1253  data: 0.0002  max mem: 1206
Epoch: [2]  [ 750/1511]  eta: 0:01:40  lr: 0.000010  loss: 0.5729 (0.5924)  labels_encoder: 0.5729 (0.5924)  labels_encoder_unscaled: 0.5729 (0.5924)  time: 0.1414  data: 0.0002  max mem: 1206
Epoch: [2]  [ 800/1511]  eta: 0:01:34  lr: 0.000010  loss: 0.5317 (0.5914)  labels_encoder: 0.5317 (0.5914)  labels_encoder_unscaled: 0.5317 (0.5914)  time: 0.1414  data: 0.0002  max mem: 1206
Epoch: [2]  [ 850/1511]  eta: 0:01:27  lr: 0.000010  loss: 0.5948 (0.5917)  labels_encoder: 0.5948 (0.5917)  labels_encoder_unscaled: 0.5948 (0.5917)  time: 0.1336  data: 0.0002  max mem: 1206
Epoch: [2]  [ 900/1511]  eta: 0:01:20  lr: 0.000010  loss: 0.5282 (0.5894)  labels_encoder: 0.5282 (0.5894)  labels_encoder_unscaled: 0.5282 (0.5894)  time: 0.1207  data: 0.0002  max mem: 1206
Epoch: [2]  [ 950/1511]  eta: 0:01:13  lr: 0.000010  loss: 0.5134 (0.5874)  labels_encoder: 0.5134 (0.5874)  labels_encoder_unscaled: 0.5134 (0.5874)  time: 0.1323  data: 0.0002  max mem: 1206
Epoch: [2]  [1000/1511]  eta: 0:01:07  lr: 0.000010  loss: 0.5241 (0.5858)  labels_encoder: 0.5241 (0.5858)  labels_encoder_unscaled: 0.5241 (0.5858)  time: 0.1359  data: 0.0002  max mem: 1206
Epoch: [2]  [1050/1511]  eta: 0:01:00  lr: 0.000010  loss: 0.5761 (0.5848)  labels_encoder: 0.5761 (0.5848)  labels_encoder_unscaled: 0.5761 (0.5848)  time: 0.1342  data: 0.0002  max mem: 1206
Epoch: [2]  [1100/1511]  eta: 0:00:54  lr: 0.000010  loss: 0.5445 (0.5834)  labels_encoder: 0.5445 (0.5834)  labels_encoder_unscaled: 0.5445 (0.5834)  time: 0.1377  data: 0.0002  max mem: 1206
Epoch: [2]  [1150/1511]  eta: 0:00:47  lr: 0.000010  loss: 0.5602 (0.5828)  labels_encoder: 0.5602 (0.5828)  labels_encoder_unscaled: 0.5602 (0.5828)  time: 0.1231  data: 0.0002  max mem: 1206
Epoch: [2]  [1200/1511]  eta: 0:00:40  lr: 0.000010  loss: 0.4743 (0.5818)  labels_encoder: 0.4743 (0.5818)  labels_encoder_unscaled: 0.4743 (0.5818)  time: 0.1338  data: 0.0002  max mem: 1206
Epoch: [2]  [1250/1511]  eta: 0:00:34  lr: 0.000010  loss: 0.5533 (0.5813)  labels_encoder: 0.5533 (0.5813)  labels_encoder_unscaled: 0.5533 (0.5813)  time: 0.1342  data: 0.0003  max mem: 1206
Epoch: [2]  [1300/1511]  eta: 0:00:27  lr: 0.000010  loss: 0.5565 (0.5808)  labels_encoder: 0.5565 (0.5808)  labels_encoder_unscaled: 0.5565 (0.5808)  time: 0.1350  data: 0.0002  max mem: 1206
Epoch: [2]  [1350/1511]  eta: 0:00:21  lr: 0.000010  loss: 0.5389 (0.5793)  labels_encoder: 0.5389 (0.5793)  labels_encoder_unscaled: 0.5389 (0.5793)  time: 0.1472  data: 0.0003  max mem: 1206
Epoch: [2]  [1400/1511]  eta: 0:00:14  lr: 0.000010  loss: 0.5242 (0.5780)  labels_encoder: 0.5242 (0.5780)  labels_encoder_unscaled: 0.5242 (0.5780)  time: 0.1395  data: 0.0002  max mem: 1206
Epoch: [2]  [1450/1511]  eta: 0:00:08  lr: 0.000010  loss: 0.5548 (0.5776)  labels_encoder: 0.5548 (0.5776)  labels_encoder_unscaled: 0.5548 (0.5776)  time: 0.1380  data: 0.0002  max mem: 1206
Epoch: [2]  [1500/1511]  eta: 0:00:01  lr: 0.000010  loss: 0.5705 (0.5769)  labels_encoder: 0.5705 (0.5769)  labels_encoder_unscaled: 0.5705 (0.5769)  time: 0.1368  data: 0.0004  max mem: 1206
Epoch: [2]  [1510/1511]  eta: 0:00:00  lr: 0.000010  loss: 0.5502 (0.5766)  labels_encoder: 0.5502 (0.5766)  labels_encoder_unscaled: 0.5502 (0.5766)  time: 0.1230  data: 0.0003  max mem: 1206
Epoch: [2] Total time: 0:03:20 (0.1327 s / it)
Averaged stats: lr: 0.000010  loss: 0.5502 (0.5766)  labels_encoder: 0.5502 (0.5766)  labels_encoder_unscaled: 0.5502 (0.5766)
Test:  [  0/559]  eta: 0:17:25  loss: 0.7460 (0.7460)  labels_encoder: 0.7460 (0.7460)  labels_encoder_unscaled: 0.7460 (0.7460)  time: 1.8707  data: 1.8136  max mem: 1206
Test:  [ 50/559]  eta: 0:00:47  loss: 0.8369 (0.8949)  labels_encoder: 0.8369 (0.8949)  labels_encoder_unscaled: 0.8369 (0.8949)  time: 0.0558  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:34  loss: 0.4950 (0.8527)  labels_encoder: 0.4950 (0.8527)  labels_encoder_unscaled: 0.4950 (0.8527)  time: 0.0502  data: 0.0001  max mem: 1206
Test:  [150/559]  eta: 0:00:27  loss: 0.5366 (0.8771)  labels_encoder: 0.5366 (0.8771)  labels_encoder_unscaled: 0.5366 (0.8771)  time: 0.0512  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:22  loss: 0.4840 (0.8207)  labels_encoder: 0.4840 (0.8207)  labels_encoder_unscaled: 0.4840 (0.8207)  time: 0.0455  data: 0.0001  max mem: 1206
Test:  [250/559]  eta: 0:00:18  loss: 0.6554 (0.9433)  labels_encoder: 0.6554 (0.9433)  labels_encoder_unscaled: 0.6554 (0.9433)  time: 0.0582  data: 0.0072  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.2744 (1.0001)  labels_encoder: 1.2744 (1.0001)  labels_encoder_unscaled: 1.2744 (1.0001)  time: 0.0589  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:12  loss: 0.7477 (1.0189)  labels_encoder: 0.7477 (1.0189)  labels_encoder_unscaled: 0.7477 (1.0189)  time: 0.0574  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3222 (0.9928)  labels_encoder: 0.3222 (0.9928)  labels_encoder_unscaled: 0.3222 (0.9928)  time: 0.0547  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.3986 (0.9677)  labels_encoder: 0.3986 (0.9677)  labels_encoder_unscaled: 0.3986 (0.9677)  time: 0.0583  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5628 (0.9451)  labels_encoder: 0.5628 (0.9451)  labels_encoder_unscaled: 0.5628 (0.9451)  time: 0.0496  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7135 (0.9209)  labels_encoder: 0.7135 (0.9209)  labels_encoder_unscaled: 0.7135 (0.9209)  time: 0.0519  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.4190 (0.9144)  labels_encoder: 0.4190 (0.9144)  labels_encoder_unscaled: 0.4190 (0.9144)  time: 0.0413  data: 0.0001  max mem: 1206
Test: Total time: 0:00:33 (0.0595 s / it)
Averaged stats: loss: 0.4190 (0.9144)  labels_encoder: 0.4190 (0.9144)  labels_encoder_unscaled: 0.4190 (0.9144)
(21, 71496)
(21, 71496)
[Epoch-2] [IDU-tvseries_kin_features.pickle] mAP: 0.1250, mcAP: 0.8828

BaseballPitch: 0.0303
BasketballDunk: 0.0934
Billiards: 0.0055
CleanAndJerk: 0.3808
CliffDiving: 0.3901
CricketBowling: 0.1059
CricketShot: 0.0979
Diving: 0.0057
FrisbeeCatch: 0.1454
GolfSwing: 0.0706
HammerThrow: 0.1749
HighJump: 0.0427
JavelinThrow: 0.0733
LongJump: 0.2378
PoleVault: 0.0996
Shotput: 0.1305
SoccerPenalty: 0.0497
TennisSwing: 0.2056
ThrowDiscus: 0.0416
VolleyballSpiking: 0.1197
Epoch: [3]  [   0/1511]  eta: 0:48:27  lr: 0.000001  loss: 0.4164 (0.4164)  labels_encoder: 0.4164 (0.4164)  labels_encoder_unscaled: 0.4164 (0.4164)  time: 1.9240  data: 1.7473  max mem: 1206
Epoch: [3]  [  50/1511]  eta: 0:04:00  lr: 0.000001  loss: 0.5565 (0.5466)  labels_encoder: 0.5565 (0.5466)  labels_encoder_unscaled: 0.5565 (0.5466)  time: 0.1218  data: 0.0002  max mem: 1206
Epoch: [3]  [ 100/1511]  eta: 0:03:26  lr: 0.000001  loss: 0.5237 (0.5355)  labels_encoder: 0.5237 (0.5355)  labels_encoder_unscaled: 0.5237 (0.5355)  time: 0.1258  data: 0.0002  max mem: 1206
Epoch: [3]  [ 150/1511]  eta: 0:03:13  lr: 0.000001  loss: 0.5610 (0.5462)  labels_encoder: 0.5610 (0.5462)  labels_encoder_unscaled: 0.5610 (0.5462)  time: 0.1321  data: 0.0002  max mem: 1206
Epoch: [3]  [ 200/1511]  eta: 0:03:02  lr: 0.000001  loss: 0.4937 (0.5376)  labels_encoder: 0.4937 (0.5376)  labels_encoder_unscaled: 0.4937 (0.5376)  time: 0.1377  data: 0.0002  max mem: 1206
Epoch: [3]  [ 250/1511]  eta: 0:02:53  lr: 0.000001  loss: 0.5486 (0.5369)  labels_encoder: 0.5486 (0.5369)  labels_encoder_unscaled: 0.5486 (0.5369)  time: 0.1281  data: 0.0002  max mem: 1206
Epoch: [3]  [ 300/1511]  eta: 0:02:44  lr: 0.000001  loss: 0.5339 (0.5384)  labels_encoder: 0.5339 (0.5384)  labels_encoder_unscaled: 0.5339 (0.5384)  time: 0.1211  data: 0.0002  max mem: 1206
Epoch: [3]  [ 350/1511]  eta: 0:02:36  lr: 0.000001  loss: 0.5166 (0.5394)  labels_encoder: 0.5166 (0.5394)  labels_encoder_unscaled: 0.5166 (0.5394)  time: 0.1251  data: 0.0002  max mem: 1206
Epoch: [3]  [ 400/1511]  eta: 0:02:29  lr: 0.000001  loss: 0.4810 (0.5385)  labels_encoder: 0.4810 (0.5385)  labels_encoder_unscaled: 0.4810 (0.5385)  time: 0.1345  data: 0.0002  max mem: 1206
Epoch: [3]  [ 450/1511]  eta: 0:02:22  lr: 0.000001  loss: 0.5211 (0.5377)  labels_encoder: 0.5211 (0.5377)  labels_encoder_unscaled: 0.5211 (0.5377)  time: 0.1294  data: 0.0002  max mem: 1206
Epoch: [3]  [ 500/1511]  eta: 0:02:15  lr: 0.000001  loss: 0.4794 (0.5356)  labels_encoder: 0.4794 (0.5356)  labels_encoder_unscaled: 0.4794 (0.5356)  time: 0.1318  data: 0.0002  max mem: 1206
Epoch: [3]  [ 550/1511]  eta: 0:02:09  lr: 0.000001  loss: 0.5664 (0.5397)  labels_encoder: 0.5664 (0.5397)  labels_encoder_unscaled: 0.5664 (0.5397)  time: 0.1453  data: 0.0002  max mem: 1206
Epoch: [3]  [ 600/1511]  eta: 0:02:03  lr: 0.000001  loss: 0.5467 (0.5399)  labels_encoder: 0.5467 (0.5399)  labels_encoder_unscaled: 0.5467 (0.5399)  time: 0.1388  data: 0.0002  max mem: 1206
Epoch: [3]  [ 650/1511]  eta: 0:01:56  lr: 0.000001  loss: 0.5393 (0.5385)  labels_encoder: 0.5393 (0.5385)  labels_encoder_unscaled: 0.5393 (0.5385)  time: 0.1190  data: 0.0002  max mem: 1206
Epoch: [3]  [ 700/1511]  eta: 0:01:49  lr: 0.000001  loss: 0.4933 (0.5362)  labels_encoder: 0.4933 (0.5362)  labels_encoder_unscaled: 0.4933 (0.5362)  time: 0.1366  data: 0.0002  max mem: 1206
Epoch: [3]  [ 750/1511]  eta: 0:01:42  lr: 0.000001  loss: 0.5383 (0.5368)  labels_encoder: 0.5383 (0.5368)  labels_encoder_unscaled: 0.5383 (0.5368)  time: 0.1393  data: 0.0003  max mem: 1206
Epoch: [3]  [ 800/1511]  eta: 0:01:35  lr: 0.000001  loss: 0.5219 (0.5364)  labels_encoder: 0.5219 (0.5364)  labels_encoder_unscaled: 0.5219 (0.5364)  time: 0.1209  data: 0.0002  max mem: 1206
Epoch: [3]  [ 850/1511]  eta: 0:01:28  lr: 0.000001  loss: 0.5201 (0.5370)  labels_encoder: 0.5201 (0.5370)  labels_encoder_unscaled: 0.5201 (0.5370)  time: 0.1416  data: 0.0002  max mem: 1206
Epoch: [3]  [ 900/1511]  eta: 0:01:22  lr: 0.000001  loss: 0.5235 (0.5364)  labels_encoder: 0.5235 (0.5364)  labels_encoder_unscaled: 0.5235 (0.5364)  time: 0.1301  data: 0.0002  max mem: 1206
Epoch: [3]  [ 950/1511]  eta: 0:01:15  lr: 0.000001  loss: 0.5436 (0.5360)  labels_encoder: 0.5436 (0.5360)  labels_encoder_unscaled: 0.5436 (0.5360)  time: 0.1292  data: 0.0002  max mem: 1206
Epoch: [3]  [1000/1511]  eta: 0:01:08  lr: 0.000001  loss: 0.5244 (0.5352)  labels_encoder: 0.5244 (0.5352)  labels_encoder_unscaled: 0.5244 (0.5352)  time: 0.1281  data: 0.0002  max mem: 1206
Epoch: [3]  [1050/1511]  eta: 0:01:01  lr: 0.000001  loss: 0.5379 (0.5367)  labels_encoder: 0.5379 (0.5367)  labels_encoder_unscaled: 0.5379 (0.5367)  time: 0.1391  data: 0.0002  max mem: 1206
Epoch: [3]  [1100/1511]  eta: 0:00:55  lr: 0.000001  loss: 0.4945 (0.5361)  labels_encoder: 0.4945 (0.5361)  labels_encoder_unscaled: 0.4945 (0.5361)  time: 0.1386  data: 0.0002  max mem: 1206
Epoch: [3]  [1150/1511]  eta: 0:00:48  lr: 0.000001  loss: 0.5293 (0.5366)  labels_encoder: 0.5293 (0.5366)  labels_encoder_unscaled: 0.5293 (0.5366)  time: 0.1191  data: 0.0002  max mem: 1206
Epoch: [3]  [1200/1511]  eta: 0:00:41  lr: 0.000001  loss: 0.4746 (0.5356)  labels_encoder: 0.4746 (0.5356)  labels_encoder_unscaled: 0.4746 (0.5356)  time: 0.1298  data: 0.0002  max mem: 1206
Epoch: [3]  [1250/1511]  eta: 0:00:34  lr: 0.000001  loss: 0.5373 (0.5361)  labels_encoder: 0.5373 (0.5361)  labels_encoder_unscaled: 0.5373 (0.5361)  time: 0.1242  data: 0.0002  max mem: 1206
Epoch: [3]  [1300/1511]  eta: 0:00:28  lr: 0.000001  loss: 0.4857 (0.5356)  labels_encoder: 0.4857 (0.5356)  labels_encoder_unscaled: 0.4857 (0.5356)  time: 0.1296  data: 0.0002  max mem: 1206
Epoch: [3]  [1350/1511]  eta: 0:00:21  lr: 0.000001  loss: 0.5217 (0.5357)  labels_encoder: 0.5217 (0.5357)  labels_encoder_unscaled: 0.5217 (0.5357)  time: 0.1235  data: 0.0002  max mem: 1206
Epoch: [3]  [1400/1511]  eta: 0:00:14  lr: 0.000001  loss: 0.4991 (0.5352)  labels_encoder: 0.4991 (0.5352)  labels_encoder_unscaled: 0.4991 (0.5352)  time: 0.1184  data: 0.0002  max mem: 1206
Epoch: [3]  [1450/1511]  eta: 0:00:08  lr: 0.000001  loss: 0.5250 (0.5356)  labels_encoder: 0.5250 (0.5356)  labels_encoder_unscaled: 0.5250 (0.5356)  time: 0.1295  data: 0.0002  max mem: 1206
Epoch: [3]  [1500/1511]  eta: 0:00:01  lr: 0.000001  loss: 0.5090 (0.5358)  labels_encoder: 0.5090 (0.5358)  labels_encoder_unscaled: 0.5090 (0.5358)  time: 0.1236  data: 0.0003  max mem: 1206
Epoch: [3]  [1510/1511]  eta: 0:00:00  lr: 0.000001  loss: 0.5090 (0.5359)  labels_encoder: 0.5090 (0.5359)  labels_encoder_unscaled: 0.5090 (0.5359)  time: 0.1199  data: 0.0003  max mem: 1206
Epoch: [3] Total time: 0:03:19 (0.1321 s / it)
Averaged stats: lr: 0.000001  loss: 0.5090 (0.5359)  labels_encoder: 0.5090 (0.5359)  labels_encoder_unscaled: 0.5090 (0.5359)
Test:  [  0/559]  eta: 0:15:05  loss: 0.7075 (0.7075)  labels_encoder: 0.7075 (0.7075)  labels_encoder_unscaled: 0.7075 (0.7075)  time: 1.6201  data: 1.5494  max mem: 1206
Test:  [ 50/559]  eta: 0:00:48  loss: 0.8222 (0.8822)  labels_encoder: 0.8222 (0.8822)  labels_encoder_unscaled: 0.8222 (0.8822)  time: 0.0562  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:34  loss: 0.5600 (0.8432)  labels_encoder: 0.5600 (0.8432)  labels_encoder_unscaled: 0.5600 (0.8432)  time: 0.0552  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.5597 (0.8661)  labels_encoder: 0.5597 (0.8661)  labels_encoder_unscaled: 0.5597 (0.8661)  time: 0.0575  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.7247 (0.8095)  labels_encoder: 0.7247 (0.8095)  labels_encoder_unscaled: 0.7247 (0.8095)  time: 0.0573  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:19  loss: 0.6804 (0.9251)  labels_encoder: 0.6804 (0.9251)  labels_encoder_unscaled: 0.6804 (0.9251)  time: 0.0595  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.0962 (0.9814)  labels_encoder: 1.0962 (0.9814)  labels_encoder_unscaled: 1.0962 (0.9814)  time: 0.0631  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7335 (1.0046)  labels_encoder: 0.7335 (1.0046)  labels_encoder_unscaled: 0.7335 (1.0046)  time: 0.0529  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3361 (0.9768)  labels_encoder: 0.3361 (0.9768)  labels_encoder_unscaled: 0.3361 (0.9768)  time: 0.0574  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4067 (0.9522)  labels_encoder: 0.4067 (0.9522)  labels_encoder_unscaled: 0.4067 (0.9522)  time: 0.0582  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6056 (0.9332)  labels_encoder: 0.6056 (0.9332)  labels_encoder_unscaled: 0.6056 (0.9332)  time: 0.0590  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7904 (0.9067)  labels_encoder: 0.7904 (0.9067)  labels_encoder_unscaled: 0.7904 (0.9067)  time: 0.0556  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3709 (0.9005)  labels_encoder: 0.3709 (0.9005)  labels_encoder_unscaled: 0.3709 (0.9005)  time: 0.0444  data: 0.0001  max mem: 1206
Test: Total time: 0:00:34 (0.0614 s / it)
Averaged stats: loss: 0.3709 (0.9005)  labels_encoder: 0.3709 (0.9005)  labels_encoder_unscaled: 0.3709 (0.9005)
(21, 71496)
(21, 71496)
[Epoch-3] [IDU-tvseries_kin_features.pickle] mAP: 0.1298, mcAP: 0.8820

BaseballPitch: 0.0486
BasketballDunk: 0.0952
Billiards: 0.0050
CleanAndJerk: 0.3997
CliffDiving: 0.4499
CricketBowling: 0.0791
CricketShot: 0.0967
Diving: 0.0056
FrisbeeCatch: 0.1438
GolfSwing: 0.0568
HammerThrow: 0.1143
HighJump: 0.0556
JavelinThrow: 0.0502
LongJump: 0.2966
PoleVault: 0.0939
Shotput: 0.1467
SoccerPenalty: 0.0520
TennisSwing: 0.2005
ThrowDiscus: 0.0554
VolleyballSpiking: 0.1512
Epoch: [4]  [   0/1511]  eta: 0:49:21  lr: 0.000000  loss: 0.7056 (0.7056)  labels_encoder: 0.7056 (0.7056)  labels_encoder_unscaled: 0.7056 (0.7056)  time: 1.9598  data: 1.7957  max mem: 1206
Epoch: [4]  [  50/1511]  eta: 0:03:57  lr: 0.000000  loss: 0.5345 (0.5235)  labels_encoder: 0.5345 (0.5235)  labels_encoder_unscaled: 0.5345 (0.5235)  time: 0.1168  data: 0.0002  max mem: 1206
Epoch: [4]  [ 100/1511]  eta: 0:03:22  lr: 0.000000  loss: 0.5805 (0.5350)  labels_encoder: 0.5805 (0.5350)  labels_encoder_unscaled: 0.5805 (0.5350)  time: 0.1227  data: 0.0002  max mem: 1206
Epoch: [4]  [ 150/1511]  eta: 0:03:08  lr: 0.000000  loss: 0.5504 (0.5330)  labels_encoder: 0.5504 (0.5330)  labels_encoder_unscaled: 0.5504 (0.5330)  time: 0.1208  data: 0.0002  max mem: 1206
Epoch: [4]  [ 200/1511]  eta: 0:02:57  lr: 0.000000  loss: 0.5132 (0.5331)  labels_encoder: 0.5132 (0.5331)  labels_encoder_unscaled: 0.5132 (0.5331)  time: 0.1233  data: 0.0002  max mem: 1206
Epoch: [4]  [ 250/1511]  eta: 0:02:47  lr: 0.000000  loss: 0.5214 (0.5309)  labels_encoder: 0.5214 (0.5309)  labels_encoder_unscaled: 0.5214 (0.5309)  time: 0.1241  data: 0.0002  max mem: 1206
Epoch: [4]  [ 300/1511]  eta: 0:02:39  lr: 0.000000  loss: 0.4820 (0.5276)  labels_encoder: 0.4820 (0.5276)  labels_encoder_unscaled: 0.4820 (0.5276)  time: 0.1302  data: 0.0002  max mem: 1206
Epoch: [4]  [ 350/1511]  eta: 0:02:33  lr: 0.000000  loss: 0.5373 (0.5300)  labels_encoder: 0.5373 (0.5300)  labels_encoder_unscaled: 0.5373 (0.5300)  time: 0.1386  data: 0.0002  max mem: 1206
Epoch: [4]  [ 400/1511]  eta: 0:02:26  lr: 0.000000  loss: 0.5023 (0.5278)  labels_encoder: 0.5023 (0.5278)  labels_encoder_unscaled: 0.5023 (0.5278)  time: 0.1327  data: 0.0002  max mem: 1206
Epoch: [4]  [ 450/1511]  eta: 0:02:20  lr: 0.000000  loss: 0.5337 (0.5266)  labels_encoder: 0.5337 (0.5266)  labels_encoder_unscaled: 0.5337 (0.5266)  time: 0.1254  data: 0.0002  max mem: 1206
Epoch: [4]  [ 500/1511]  eta: 0:02:13  lr: 0.000000  loss: 0.5421 (0.5277)  labels_encoder: 0.5421 (0.5277)  labels_encoder_unscaled: 0.5421 (0.5277)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [4]  [ 550/1511]  eta: 0:02:06  lr: 0.000000  loss: 0.5182 (0.5271)  labels_encoder: 0.5182 (0.5271)  labels_encoder_unscaled: 0.5182 (0.5271)  time: 0.1209  data: 0.0002  max mem: 1206
Epoch: [4]  [ 600/1511]  eta: 0:01:59  lr: 0.000000  loss: 0.5145 (0.5252)  labels_encoder: 0.5145 (0.5252)  labels_encoder_unscaled: 0.5145 (0.5252)  time: 0.1339  data: 0.0002  max mem: 1206
Epoch: [4]  [ 650/1511]  eta: 0:01:52  lr: 0.000000  loss: 0.4896 (0.5257)  labels_encoder: 0.4896 (0.5257)  labels_encoder_unscaled: 0.4896 (0.5257)  time: 0.1380  data: 0.0002  max mem: 1206
Epoch: [4]  [ 700/1511]  eta: 0:01:46  lr: 0.000000  loss: 0.5270 (0.5259)  labels_encoder: 0.5270 (0.5259)  labels_encoder_unscaled: 0.5270 (0.5259)  time: 0.1385  data: 0.0002  max mem: 1206
Epoch: [4]  [ 750/1511]  eta: 0:01:39  lr: 0.000000  loss: 0.5141 (0.5256)  labels_encoder: 0.5141 (0.5256)  labels_encoder_unscaled: 0.5141 (0.5256)  time: 0.1287  data: 0.0002  max mem: 1206
Epoch: [4]  [ 800/1511]  eta: 0:01:33  lr: 0.000000  loss: 0.5367 (0.5245)  labels_encoder: 0.5367 (0.5245)  labels_encoder_unscaled: 0.5367 (0.5245)  time: 0.1256  data: 0.0002  max mem: 1206
Epoch: [4]  [ 850/1511]  eta: 0:01:26  lr: 0.000000  loss: 0.5406 (0.5248)  labels_encoder: 0.5406 (0.5248)  labels_encoder_unscaled: 0.5406 (0.5248)  time: 0.1411  data: 0.0002  max mem: 1206
Epoch: [4]  [ 900/1511]  eta: 0:01:20  lr: 0.000000  loss: 0.5485 (0.5253)  labels_encoder: 0.5485 (0.5253)  labels_encoder_unscaled: 0.5485 (0.5253)  time: 0.1350  data: 0.0002  max mem: 1206
Epoch: [4]  [ 950/1511]  eta: 0:01:13  lr: 0.000000  loss: 0.4994 (0.5242)  labels_encoder: 0.4994 (0.5242)  labels_encoder_unscaled: 0.4994 (0.5242)  time: 0.1350  data: 0.0002  max mem: 1206
Epoch: [4]  [1000/1511]  eta: 0:01:07  lr: 0.000000  loss: 0.5347 (0.5241)  labels_encoder: 0.5347 (0.5241)  labels_encoder_unscaled: 0.5347 (0.5241)  time: 0.1352  data: 0.0002  max mem: 1206
Epoch: [4]  [1050/1511]  eta: 0:01:00  lr: 0.000000  loss: 0.5508 (0.5250)  labels_encoder: 0.5508 (0.5250)  labels_encoder_unscaled: 0.5508 (0.5250)  time: 0.1258  data: 0.0002  max mem: 1206
Epoch: [4]  [1100/1511]  eta: 0:00:54  lr: 0.000000  loss: 0.4893 (0.5246)  labels_encoder: 0.4893 (0.5246)  labels_encoder_unscaled: 0.4893 (0.5246)  time: 0.1291  data: 0.0002  max mem: 1206
Epoch: [4]  [1150/1511]  eta: 0:00:47  lr: 0.000000  loss: 0.4739 (0.5241)  labels_encoder: 0.4739 (0.5241)  labels_encoder_unscaled: 0.4739 (0.5241)  time: 0.1394  data: 0.0003  max mem: 1206
Epoch: [4]  [1200/1511]  eta: 0:00:41  lr: 0.000000  loss: 0.5437 (0.5243)  labels_encoder: 0.5437 (0.5243)  labels_encoder_unscaled: 0.5437 (0.5243)  time: 0.1388  data: 0.0002  max mem: 1206
Epoch: [4]  [1250/1511]  eta: 0:00:34  lr: 0.000000  loss: 0.5244 (0.5248)  labels_encoder: 0.5244 (0.5248)  labels_encoder_unscaled: 0.5244 (0.5248)  time: 0.1336  data: 0.0002  max mem: 1206
Epoch: [4]  [1300/1511]  eta: 0:00:27  lr: 0.000000  loss: 0.4886 (0.5245)  labels_encoder: 0.4886 (0.5245)  labels_encoder_unscaled: 0.4886 (0.5245)  time: 0.1409  data: 0.0002  max mem: 1206
Epoch: [4]  [1350/1511]  eta: 0:00:21  lr: 0.000000  loss: 0.5253 (0.5252)  labels_encoder: 0.5253 (0.5252)  labels_encoder_unscaled: 0.5253 (0.5252)  time: 0.1471  data: 0.0002  max mem: 1206
Epoch: [4]  [1400/1511]  eta: 0:00:14  lr: 0.000000  loss: 0.5166 (0.5259)  labels_encoder: 0.5166 (0.5259)  labels_encoder_unscaled: 0.5166 (0.5259)  time: 0.1288  data: 0.0002  max mem: 1206
Epoch: [4]  [1450/1511]  eta: 0:00:08  lr: 0.000000  loss: 0.5614 (0.5261)  labels_encoder: 0.5614 (0.5261)  labels_encoder_unscaled: 0.5614 (0.5261)  time: 0.1304  data: 0.0002  max mem: 1206
Epoch: [4]  [1500/1511]  eta: 0:00:01  lr: 0.000000  loss: 0.5176 (0.5264)  labels_encoder: 0.5176 (0.5264)  labels_encoder_unscaled: 0.5176 (0.5264)  time: 0.1258  data: 0.0004  max mem: 1206
Epoch: [4]  [1510/1511]  eta: 0:00:00  lr: 0.000000  loss: 0.5047 (0.5262)  labels_encoder: 0.5047 (0.5262)  labels_encoder_unscaled: 0.5047 (0.5262)  time: 0.1180  data: 0.0003  max mem: 1206
Epoch: [4] Total time: 0:03:20 (0.1329 s / it)
Averaged stats: lr: 0.000000  loss: 0.5047 (0.5262)  labels_encoder: 0.5047 (0.5262)  labels_encoder_unscaled: 0.5047 (0.5262)
Test:  [  0/559]  eta: 0:19:03  loss: 0.7025 (0.7025)  labels_encoder: 0.7025 (0.7025)  labels_encoder_unscaled: 0.7025 (0.7025)  time: 2.0452  data: 1.9818  max mem: 1206
Test:  [ 50/559]  eta: 0:00:52  loss: 0.7592 (0.8858)  labels_encoder: 0.7592 (0.8858)  labels_encoder_unscaled: 0.7592 (0.8858)  time: 0.0646  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:37  loss: 0.5446 (0.8481)  labels_encoder: 0.5446 (0.8481)  labels_encoder_unscaled: 0.5446 (0.8481)  time: 0.0635  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:30  loss: 0.5529 (0.8668)  labels_encoder: 0.5529 (0.8668)  labels_encoder_unscaled: 0.5529 (0.8668)  time: 0.0559  data: 0.0001  max mem: 1206
Test:  [200/559]  eta: 0:00:25  loss: 0.5744 (0.8082)  labels_encoder: 0.5744 (0.8082)  labels_encoder_unscaled: 0.5744 (0.8082)  time: 0.0565  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.7078 (0.9237)  labels_encoder: 0.7078 (0.9237)  labels_encoder_unscaled: 0.7078 (0.9237)  time: 0.0596  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.1143 (0.9804)  labels_encoder: 1.1143 (0.9804)  labels_encoder_unscaled: 1.1143 (0.9804)  time: 0.0577  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7810 (1.0019)  labels_encoder: 0.7810 (1.0019)  labels_encoder_unscaled: 0.7810 (1.0019)  time: 0.0636  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3108 (0.9747)  labels_encoder: 0.3108 (0.9747)  labels_encoder_unscaled: 0.3108 (0.9747)  time: 0.0580  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:07  loss: 0.4180 (0.9501)  labels_encoder: 0.4180 (0.9501)  labels_encoder_unscaled: 0.4180 (0.9501)  time: 0.0670  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6954 (0.9307)  labels_encoder: 0.6954 (0.9307)  labels_encoder_unscaled: 0.6954 (0.9307)  time: 0.0521  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7796 (0.9041)  labels_encoder: 0.7796 (0.9041)  labels_encoder_unscaled: 0.7796 (0.9041)  time: 0.0510  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3414 (0.8979)  labels_encoder: 0.3414 (0.8979)  labels_encoder_unscaled: 0.3414 (0.8979)  time: 0.0394  data: 0.0001  max mem: 1206
Test: Total time: 0:00:35 (0.0631 s / it)
Averaged stats: loss: 0.3414 (0.8979)  labels_encoder: 0.3414 (0.8979)  labels_encoder_unscaled: 0.3414 (0.8979)
(21, 71496)
(21, 71496)
[Epoch-4] [IDU-tvseries_kin_features.pickle] mAP: 0.1292, mcAP: 0.8833

BaseballPitch: 0.0376
BasketballDunk: 0.1072
Billiards: 0.0047
CleanAndJerk: 0.4019
CliffDiving: 0.4500
CricketBowling: 0.0894
CricketShot: 0.1016
Diving: 0.0059
FrisbeeCatch: 0.1267
GolfSwing: 0.0699
HammerThrow: 0.1372
HighJump: 0.0603
JavelinThrow: 0.0550
LongJump: 0.2714
PoleVault: 0.0944
Shotput: 0.1496
SoccerPenalty: 0.0543
TennisSwing: 0.1960
ThrowDiscus: 0.0315
VolleyballSpiking: 0.1398
Training time 0:15:48
