Unable to compile CoConv C++ implementation. Falling back to Python version.
[Errno 2] No such file or directory: '/home/lh/.conda/envs/oadtr/lib/python3.8/site-packages/continual/conv.cpp'
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'conv_flops_counter_hook'
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'MODULES_MAPPING'
Not using distributed mode
lr:0.0001
batch_size:128
weight_decay:0.0001
epochs:5
resize_feature:False
lr_drop:1
clip_max_norm:1.0
dataparallel:False
removelog:False
version:v3
query_num:8
decoder_layers:5
decoder_embedding_dim:1024
decoder_embedding_dim_out:1024
decoder_attn_dropout_rate:0.1
decoder_num_heads:4
classification_pred_loss_coef:0.5
enc_layers:64
lr_backbone:0.0001
feature:tvseries_anet_features.pickle
dim_feature:4096
patch_dim:1
embedding_dim:1024
num_heads:8
num_layers:2
attn_dropout_rate:0.1
positional_encoding_type:recycling_fixed
num_embeddings:127
hidden_dim:1024
dropout_rate:0.1
numclass:31
classification_x_loss_coef:0.3
classification_h_loss_coef:1
similar_loss_coef:0.1
margin:1.0
dataset:tvseries
dataset_file:data/data_info_new.json
frozen_weights:None
thumos_data_path:/home/dancer/mycode/Temporal.Online.Detection/Online.TRN.Pytorch/preprocess/
thumos_anno_path:data/thumos_{}_anno.pickle
remove_difficult:False
device:cuda
output_dir:models
seed:5
resume:
start_epoch:1
eval:False
num_workers:8
world_size:1
dist_url:tcp://127.0.0.1:12342
train_session_set:['24_ep1', '24_ep2', '24_ep3', 'Breaking_Bad_ep1', 'Breaking_Bad_ep2', 'How_I_Met_Your_Mother_ep1', 'How_I_Met_Your_Mother_ep2', 'How_I_Met_Your_Mother_ep3', 'How_I_Met_Your_Mother_ep4', 'How_I_Met_Your_Mother_ep5', 'How_I_Met_Your_Mother_ep6', 'Mad_Men_ep1', 'Mad_Men_ep2', 'Modern_Family_ep1', 'Modern_Family_ep2', 'Modern_Family_ep3', 'Modern_Family_ep4', 'Modern_Family_ep6', 'Sons_of_Anarchy_ep1', 'Sons_of_Anarchy_ep2']
test_session_set:['24_ep4', 'Breaking_Bad_ep3', 'Mad_Men_ep3', 'How_I_Met_Your_Mother_ep7', 'How_I_Met_Your_Mother_ep8', 'Modern_Family_ep5', 'Sons_of_Anarchy_ep3']
class_index:['background', 'Pick something up', 'Point', 'Drink', 'Stand up', 'Run', 'Sit down', 'Read', 'Smoke', 'Drive car', 'Open door', 'Give something', 'Use computer', 'Write', 'Go down stairway', 'Close door', 'Throw something', 'Go up stairway', 'Get in/out of car', 'Hang up phone', 'Eat', 'Answer phone', 'Dress up', 'Clap', 'Undress', 'Kiss', 'Fall/trip', 'Wave', 'Pour', 'Punch', 'Fire weapon']
distributed:False
position encoding : recycling_fixed
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Sequential(
  16.814 M, 99.939% Params, 0.412 GMac, 100.000% MACs, 
  (0): Linear(4.195 M, 24.936% Params, 0.004 GMac, 1.018% MACs, in_features=4096, out_features=1024, bias=True, channel_dim=1)
  (1): RecyclingPositionalEncoding(
    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 
    (pe): CyclicPositionalEncoding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
  )
  (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
  (3): Sequential(
    12.587 M, 74.814% Params, 0.408 GMac, 98.974% MACs, 
    (0): Sequential(
      6.294 M, 37.407% Params, 0.205 GMac, 49.804% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, reduce=reduce_sum
        (0): RetroactiveUnity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 63)
        (1): CoReMultiheadAttention(
          4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(Sequential(
        2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
        (0): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
        (1): Residual(
          2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
          (fn): Sequential(
            2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
            (0): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
            (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
            (3): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          )
        )
        (2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
      ))
    )
    (1): Lambda(Sequential(
      6.294 M, 37.407% Params, 0.203 GMac, 49.170% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, reduce=sum_last_pairs
        (0): SelectOrDelay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): CoSiMultiheadAttention(
          4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
      (2): BroadcastReduce(
        2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, reduce=reduce_sum
        (0): Delay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): Sequential(
          2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, 
          (0): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
          (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          (3): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
        )
      )
      (3): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
    ), takes_time=True)
    (2): Lambda(unity, squeeze_last, squeeze_last, takes_time=True)
  )
  (4): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
  (5): Linear(0.032 M, 0.189% Params, 0.0 GMac, 0.008% MACs, in_features=1024, out_features=31, bias=True, channel_dim=1)
)
Model FLOPs: 411997217.0
Model params: 16824351
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| Active memory         |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   83968 KB |   83968 KB |   83968 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16445 KB |   16445 KB |  302954 KB |  286508 KB |
|---------------------------------------------------------------------------|
| Allocations           |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| Active allocs         |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |    1458    |    1448    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Memory state pre, max, post inference: 69143040 71518720 69143040
Loaded tvseries_anet_features.pickle
Loaded tvseries_anet_features.pickle
Start training
Epoch: [1]  [   0/1510]  eta: 1:06:59  lr: 0.000100  loss: 3.4812 (3.4812)  labels_encoder: 3.4812 (3.4812)  labels_encoder_unscaled: 3.4812 (3.4812)  time: 2.6621  data: 2.0634  max mem: 1013
Epoch: [1]  [  50/1510]  eta: 0:04:16  lr: 0.000100  loss: 1.0308 (1.1471)  labels_encoder: 1.0308 (1.1471)  labels_encoder_unscaled: 1.0308 (1.1471)  time: 0.1155  data: 0.0002  max mem: 1206
Epoch: [1]  [ 100/1510]  eta: 0:03:27  lr: 0.000100  loss: 0.9860 (1.0743)  labels_encoder: 0.9860 (1.0743)  labels_encoder_unscaled: 0.9860 (1.0743)  time: 0.1156  data: 0.0002  max mem: 1206
Epoch: [1]  [ 150/1510]  eta: 0:03:07  lr: 0.000100  loss: 0.9153 (1.0232)  labels_encoder: 0.9153 (1.0232)  labels_encoder_unscaled: 0.9153 (1.0232)  time: 0.1216  data: 0.0002  max mem: 1206
Epoch: [1]  [ 200/1510]  eta: 0:02:55  lr: 0.000100  loss: 0.8984 (0.9889)  labels_encoder: 0.8984 (0.9889)  labels_encoder_unscaled: 0.8984 (0.9889)  time: 0.1253  data: 0.0002  max mem: 1206
Epoch: [1]  [ 250/1510]  eta: 0:02:45  lr: 0.000100  loss: 0.8259 (0.9667)  labels_encoder: 0.8259 (0.9667)  labels_encoder_unscaled: 0.8259 (0.9667)  time: 0.1176  data: 0.0002  max mem: 1206
Epoch: [1]  [ 300/1510]  eta: 0:02:36  lr: 0.000100  loss: 0.7891 (0.9492)  labels_encoder: 0.7891 (0.9492)  labels_encoder_unscaled: 0.7891 (0.9492)  time: 0.1197  data: 0.0002  max mem: 1206
Epoch: [1]  [ 350/1510]  eta: 0:02:28  lr: 0.000100  loss: 0.8167 (0.9347)  labels_encoder: 0.8167 (0.9347)  labels_encoder_unscaled: 0.8167 (0.9347)  time: 0.1187  data: 0.0002  max mem: 1206
Epoch: [1]  [ 400/1510]  eta: 0:02:21  lr: 0.000100  loss: 0.7561 (0.9175)  labels_encoder: 0.7561 (0.9175)  labels_encoder_unscaled: 0.7561 (0.9175)  time: 0.1259  data: 0.0002  max mem: 1206
Epoch: [1]  [ 450/1510]  eta: 0:02:15  lr: 0.000100  loss: 0.7386 (0.9071)  labels_encoder: 0.7386 (0.9071)  labels_encoder_unscaled: 0.7386 (0.9071)  time: 0.1267  data: 0.0002  max mem: 1206
Epoch: [1]  [ 500/1510]  eta: 0:02:08  lr: 0.000100  loss: 0.7868 (0.8930)  labels_encoder: 0.7868 (0.8930)  labels_encoder_unscaled: 0.7868 (0.8930)  time: 0.1254  data: 0.0002  max mem: 1206
Epoch: [1]  [ 550/1510]  eta: 0:02:02  lr: 0.000100  loss: 0.7721 (0.8846)  labels_encoder: 0.7721 (0.8846)  labels_encoder_unscaled: 0.7721 (0.8846)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [1]  [ 600/1510]  eta: 0:01:55  lr: 0.000100  loss: 0.7740 (0.8732)  labels_encoder: 0.7740 (0.8732)  labels_encoder_unscaled: 0.7740 (0.8732)  time: 0.1264  data: 0.0002  max mem: 1206
Epoch: [1]  [ 650/1510]  eta: 0:01:49  lr: 0.000100  loss: 0.7080 (0.8633)  labels_encoder: 0.7080 (0.8633)  labels_encoder_unscaled: 0.7080 (0.8633)  time: 0.1281  data: 0.0002  max mem: 1206
Epoch: [1]  [ 700/1510]  eta: 0:01:43  lr: 0.000100  loss: 0.7678 (0.8545)  labels_encoder: 0.7678 (0.8545)  labels_encoder_unscaled: 0.7678 (0.8545)  time: 0.1278  data: 0.0002  max mem: 1206
Epoch: [1]  [ 750/1510]  eta: 0:01:36  lr: 0.000100  loss: 0.7712 (0.8480)  labels_encoder: 0.7712 (0.8480)  labels_encoder_unscaled: 0.7712 (0.8480)  time: 0.1278  data: 0.0002  max mem: 1206
Epoch: [1]  [ 800/1510]  eta: 0:01:30  lr: 0.000100  loss: 0.6830 (0.8409)  labels_encoder: 0.6830 (0.8409)  labels_encoder_unscaled: 0.6830 (0.8409)  time: 0.1266  data: 0.0001  max mem: 1206
Epoch: [1]  [ 850/1510]  eta: 0:01:23  lr: 0.000100  loss: 0.7473 (0.8357)  labels_encoder: 0.7473 (0.8357)  labels_encoder_unscaled: 0.7473 (0.8357)  time: 0.1209  data: 0.0002  max mem: 1206
Epoch: [1]  [ 900/1510]  eta: 0:01:17  lr: 0.000100  loss: 0.6923 (0.8292)  labels_encoder: 0.6923 (0.8292)  labels_encoder_unscaled: 0.6923 (0.8292)  time: 0.1183  data: 0.0002  max mem: 1206
Epoch: [1]  [ 950/1510]  eta: 0:01:10  lr: 0.000100  loss: 0.6520 (0.8233)  labels_encoder: 0.6520 (0.8233)  labels_encoder_unscaled: 0.6520 (0.8233)  time: 0.1188  data: 0.0001  max mem: 1206
Epoch: [1]  [1000/1510]  eta: 0:01:04  lr: 0.000100  loss: 0.7419 (0.8203)  labels_encoder: 0.7419 (0.8203)  labels_encoder_unscaled: 0.7419 (0.8203)  time: 0.1206  data: 0.0001  max mem: 1206
Epoch: [1]  [1050/1510]  eta: 0:00:57  lr: 0.000100  loss: 0.6688 (0.8135)  labels_encoder: 0.6688 (0.8135)  labels_encoder_unscaled: 0.6688 (0.8135)  time: 0.1188  data: 0.0002  max mem: 1206
Epoch: [1]  [1100/1510]  eta: 0:00:51  lr: 0.000100  loss: 0.7385 (0.8087)  labels_encoder: 0.7385 (0.8087)  labels_encoder_unscaled: 0.7385 (0.8087)  time: 0.1191  data: 0.0001  max mem: 1206
Epoch: [1]  [1150/1510]  eta: 0:00:44  lr: 0.000100  loss: 0.6854 (0.8043)  labels_encoder: 0.6854 (0.8043)  labels_encoder_unscaled: 0.6854 (0.8043)  time: 0.1196  data: 0.0002  max mem: 1206
Epoch: [1]  [1200/1510]  eta: 0:00:38  lr: 0.000100  loss: 0.6980 (0.8003)  labels_encoder: 0.6980 (0.8003)  labels_encoder_unscaled: 0.6980 (0.8003)  time: 0.1186  data: 0.0002  max mem: 1206
Epoch: [1]  [1250/1510]  eta: 0:00:32  lr: 0.000100  loss: 0.6620 (0.7959)  labels_encoder: 0.6620 (0.7959)  labels_encoder_unscaled: 0.6620 (0.7959)  time: 0.1192  data: 0.0001  max mem: 1206
Epoch: [1]  [1300/1510]  eta: 0:00:26  lr: 0.000100  loss: 0.6539 (0.7914)  labels_encoder: 0.6539 (0.7914)  labels_encoder_unscaled: 0.6539 (0.7914)  time: 0.1190  data: 0.0002  max mem: 1206
Epoch: [1]  [1350/1510]  eta: 0:00:19  lr: 0.000100  loss: 0.6224 (0.7862)  labels_encoder: 0.6224 (0.7862)  labels_encoder_unscaled: 0.6224 (0.7862)  time: 0.1285  data: 0.0001  max mem: 1206
Epoch: [1]  [1400/1510]  eta: 0:00:13  lr: 0.000100  loss: 0.6552 (0.7827)  labels_encoder: 0.6552 (0.7827)  labels_encoder_unscaled: 0.6552 (0.7827)  time: 0.1331  data: 0.0002  max mem: 1206
Epoch: [1]  [1450/1510]  eta: 0:00:07  lr: 0.000100  loss: 0.6769 (0.7782)  labels_encoder: 0.6769 (0.7782)  labels_encoder_unscaled: 0.6769 (0.7782)  time: 0.1329  data: 0.0002  max mem: 1206
Epoch: [1]  [1500/1510]  eta: 0:00:01  lr: 0.000100  loss: 0.6347 (0.7749)  labels_encoder: 0.6347 (0.7749)  labels_encoder_unscaled: 0.6347 (0.7749)  time: 0.1267  data: 0.0004  max mem: 1206
Epoch: [1]  [1509/1510]  eta: 0:00:00  lr: 0.000100  loss: 0.6006 (0.7741)  labels_encoder: 0.6006 (0.7741)  labels_encoder_unscaled: 0.6006 (0.7741)  time: 0.1216  data: 0.0003  max mem: 1206
Epoch: [1] Total time: 0:03:08 (0.1251 s / it)
Averaged stats: lr: 0.000100  loss: 0.6006 (0.7741)  labels_encoder: 0.6006 (0.7741)  labels_encoder_unscaled: 0.6006 (0.7741)
Test:  [  0/559]  eta: 0:14:45  loss: 0.7352 (0.7352)  labels_encoder: 0.7352 (0.7352)  labels_encoder_unscaled: 0.7352 (0.7352)  time: 1.5848  data: 1.5218  max mem: 1206
Test:  [ 50/559]  eta: 0:00:47  loss: 0.9066 (0.9383)  labels_encoder: 0.9066 (0.9383)  labels_encoder_unscaled: 0.9066 (0.9383)  time: 0.0595  data: 0.0004  max mem: 1206
Test:  [100/559]  eta: 0:00:34  loss: 0.5387 (0.8871)  labels_encoder: 0.5387 (0.8871)  labels_encoder_unscaled: 0.5387 (0.8871)  time: 0.0596  data: 0.0010  max mem: 1206
Test:  [150/559]  eta: 0:00:29  loss: 0.7520 (0.9168)  labels_encoder: 0.7520 (0.9168)  labels_encoder_unscaled: 0.7520 (0.9168)  time: 0.0607  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:24  loss: 0.8216 (0.8660)  labels_encoder: 0.8216 (0.8660)  labels_encoder_unscaled: 0.8216 (0.8660)  time: 0.0622  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.7024 (0.9883)  labels_encoder: 0.7024 (0.9883)  labels_encoder_unscaled: 0.7024 (0.9883)  time: 0.0602  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.2427 (1.0458)  labels_encoder: 1.2427 (1.0458)  labels_encoder_unscaled: 1.2427 (1.0458)  time: 0.0602  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.8150 (1.0610)  labels_encoder: 0.8150 (1.0610)  labels_encoder_unscaled: 0.8150 (1.0610)  time: 0.0592  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.4162 (1.0355)  labels_encoder: 0.4162 (1.0355)  labels_encoder_unscaled: 0.4162 (1.0355)  time: 0.0574  data: 0.0001  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.6457 (1.0168)  labels_encoder: 0.6457 (1.0168)  labels_encoder_unscaled: 0.6457 (1.0168)  time: 0.0526  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6585 (1.0002)  labels_encoder: 0.6585 (1.0002)  labels_encoder_unscaled: 0.6585 (1.0002)  time: 0.0536  data: 0.0016  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.8069 (0.9704)  labels_encoder: 0.8069 (0.9704)  labels_encoder_unscaled: 0.8069 (0.9704)  time: 0.0550  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3536 (0.9637)  labels_encoder: 0.3536 (0.9637)  labels_encoder_unscaled: 0.3536 (0.9637)  time: 0.0503  data: 0.0018  max mem: 1206
Test: Total time: 0:00:35 (0.0628 s / it)
Averaged stats: loss: 0.3536 (0.9637)  labels_encoder: 0.3536 (0.9637)  labels_encoder_unscaled: 0.3536 (0.9637)
(21, 71496)
(21, 71496)
[Epoch-1] [IDU-tvseries_anet_features.pickle] mAP: 0.1167, mcAP: 0.8682

BaseballPitch: 0.0293
BasketballDunk: 0.1188
Billiards: 0.0056
CleanAndJerk: 0.3689
CliffDiving: 0.3091
CricketBowling: 0.0726
CricketShot: 0.0905
Diving: 0.0042
FrisbeeCatch: 0.1150
GolfSwing: 0.0660
HammerThrow: 0.1039
HighJump: 0.0572
JavelinThrow: 0.0210
LongJump: 0.3135
PoleVault: 0.0915
Shotput: 0.1168
SoccerPenalty: 0.0360
TennisSwing: 0.1835
ThrowDiscus: 0.1162
VolleyballSpiking: 0.1143
Epoch: [2]  [   0/1510]  eta: 0:49:25  lr: 0.000010  loss: 0.6709 (0.6709)  labels_encoder: 0.6709 (0.6709)  labels_encoder_unscaled: 0.6709 (0.6709)  time: 1.9642  data: 1.8167  max mem: 1206
Epoch: [2]  [  50/1510]  eta: 0:04:04  lr: 0.000010  loss: 0.6042 (0.6355)  labels_encoder: 0.6042 (0.6355)  labels_encoder_unscaled: 0.6042 (0.6355)  time: 0.1246  data: 0.0002  max mem: 1206
Epoch: [2]  [ 100/1510]  eta: 0:03:22  lr: 0.000010  loss: 0.5964 (0.6342)  labels_encoder: 0.5964 (0.6342)  labels_encoder_unscaled: 0.5964 (0.6342)  time: 0.1139  data: 0.0002  max mem: 1206
Epoch: [2]  [ 150/1510]  eta: 0:03:11  lr: 0.000010  loss: 0.5849 (0.6424)  labels_encoder: 0.5849 (0.6424)  labels_encoder_unscaled: 0.5849 (0.6424)  time: 0.1377  data: 0.0002  max mem: 1206
Epoch: [2]  [ 200/1510]  eta: 0:03:04  lr: 0.000010  loss: 0.6073 (0.6284)  labels_encoder: 0.6073 (0.6284)  labels_encoder_unscaled: 0.6073 (0.6284)  time: 0.1421  data: 0.0002  max mem: 1206
Epoch: [2]  [ 250/1510]  eta: 0:02:56  lr: 0.000010  loss: 0.5617 (0.6226)  labels_encoder: 0.5617 (0.6226)  labels_encoder_unscaled: 0.5617 (0.6226)  time: 0.1339  data: 0.0002  max mem: 1206
Epoch: [2]  [ 300/1510]  eta: 0:02:49  lr: 0.000010  loss: 0.5887 (0.6178)  labels_encoder: 0.5887 (0.6178)  labels_encoder_unscaled: 0.5887 (0.6178)  time: 0.1446  data: 0.0002  max mem: 1206
Epoch: [2]  [ 350/1510]  eta: 0:02:42  lr: 0.000010  loss: 0.5600 (0.6115)  labels_encoder: 0.5600 (0.6115)  labels_encoder_unscaled: 0.5600 (0.6115)  time: 0.1404  data: 0.0002  max mem: 1206
Epoch: [2]  [ 400/1510]  eta: 0:02:35  lr: 0.000010  loss: 0.5977 (0.6099)  labels_encoder: 0.5977 (0.6099)  labels_encoder_unscaled: 0.5977 (0.6099)  time: 0.1362  data: 0.0002  max mem: 1206
Epoch: [2]  [ 450/1510]  eta: 0:02:27  lr: 0.000010  loss: 0.6005 (0.6047)  labels_encoder: 0.6005 (0.6047)  labels_encoder_unscaled: 0.6005 (0.6047)  time: 0.1340  data: 0.0002  max mem: 1206
Epoch: [2]  [ 500/1510]  eta: 0:02:19  lr: 0.000010  loss: 0.6048 (0.6017)  labels_encoder: 0.6048 (0.6017)  labels_encoder_unscaled: 0.6048 (0.6017)  time: 0.1164  data: 0.0002  max mem: 1206
Epoch: [2]  [ 550/1510]  eta: 0:02:10  lr: 0.000010  loss: 0.5816 (0.6003)  labels_encoder: 0.5816 (0.6003)  labels_encoder_unscaled: 0.5816 (0.6003)  time: 0.1167  data: 0.0002  max mem: 1206
Epoch: [2]  [ 600/1510]  eta: 0:02:02  lr: 0.000010  loss: 0.5698 (0.5988)  labels_encoder: 0.5698 (0.5988)  labels_encoder_unscaled: 0.5698 (0.5988)  time: 0.1228  data: 0.0002  max mem: 1206
Epoch: [2]  [ 650/1510]  eta: 0:01:55  lr: 0.000010  loss: 0.6110 (0.5989)  labels_encoder: 0.6110 (0.5989)  labels_encoder_unscaled: 0.6110 (0.5989)  time: 0.1236  data: 0.0002  max mem: 1206
Epoch: [2]  [ 700/1510]  eta: 0:01:48  lr: 0.000010  loss: 0.6076 (0.5995)  labels_encoder: 0.6076 (0.5995)  labels_encoder_unscaled: 0.6076 (0.5995)  time: 0.1306  data: 0.0002  max mem: 1206
Epoch: [2]  [ 750/1510]  eta: 0:01:41  lr: 0.000010  loss: 0.5654 (0.5988)  labels_encoder: 0.5654 (0.5988)  labels_encoder_unscaled: 0.5654 (0.5988)  time: 0.1203  data: 0.0002  max mem: 1206
Epoch: [2]  [ 800/1510]  eta: 0:01:34  lr: 0.000010  loss: 0.5144 (0.5957)  labels_encoder: 0.5144 (0.5957)  labels_encoder_unscaled: 0.5144 (0.5957)  time: 0.1285  data: 0.0002  max mem: 1206
Epoch: [2]  [ 850/1510]  eta: 0:01:27  lr: 0.000010  loss: 0.5992 (0.5940)  labels_encoder: 0.5992 (0.5940)  labels_encoder_unscaled: 0.5992 (0.5940)  time: 0.1252  data: 0.0002  max mem: 1206
Epoch: [2]  [ 900/1510]  eta: 0:01:20  lr: 0.000010  loss: 0.5304 (0.5919)  labels_encoder: 0.5304 (0.5919)  labels_encoder_unscaled: 0.5304 (0.5919)  time: 0.1338  data: 0.0002  max mem: 1206
Epoch: [2]  [ 950/1510]  eta: 0:01:14  lr: 0.000010  loss: 0.5031 (0.5896)  labels_encoder: 0.5031 (0.5896)  labels_encoder_unscaled: 0.5031 (0.5896)  time: 0.1280  data: 0.0002  max mem: 1206
Epoch: [2]  [1000/1510]  eta: 0:01:07  lr: 0.000010  loss: 0.5365 (0.5885)  labels_encoder: 0.5365 (0.5885)  labels_encoder_unscaled: 0.5365 (0.5885)  time: 0.1258  data: 0.0002  max mem: 1206
Epoch: [2]  [1050/1510]  eta: 0:01:00  lr: 0.000010  loss: 0.5473 (0.5873)  labels_encoder: 0.5473 (0.5873)  labels_encoder_unscaled: 0.5473 (0.5873)  time: 0.1234  data: 0.0002  max mem: 1206
Epoch: [2]  [1100/1510]  eta: 0:00:53  lr: 0.000010  loss: 0.5525 (0.5856)  labels_encoder: 0.5525 (0.5856)  labels_encoder_unscaled: 0.5525 (0.5856)  time: 0.1256  data: 0.0002  max mem: 1206
Epoch: [2]  [1150/1510]  eta: 0:00:47  lr: 0.000010  loss: 0.5315 (0.5843)  labels_encoder: 0.5315 (0.5843)  labels_encoder_unscaled: 0.5315 (0.5843)  time: 0.1364  data: 0.0002  max mem: 1206
Epoch: [2]  [1200/1510]  eta: 0:00:40  lr: 0.000010  loss: 0.5680 (0.5834)  labels_encoder: 0.5680 (0.5834)  labels_encoder_unscaled: 0.5680 (0.5834)  time: 0.1282  data: 0.0002  max mem: 1206
Epoch: [2]  [1250/1510]  eta: 0:00:34  lr: 0.000010  loss: 0.5311 (0.5817)  labels_encoder: 0.5311 (0.5817)  labels_encoder_unscaled: 0.5311 (0.5817)  time: 0.1355  data: 0.0002  max mem: 1206
Epoch: [2]  [1300/1510]  eta: 0:00:27  lr: 0.000010  loss: 0.5046 (0.5811)  labels_encoder: 0.5046 (0.5811)  labels_encoder_unscaled: 0.5046 (0.5811)  time: 0.1229  data: 0.0002  max mem: 1206
Epoch: [2]  [1350/1510]  eta: 0:00:20  lr: 0.000010  loss: 0.5436 (0.5805)  labels_encoder: 0.5436 (0.5805)  labels_encoder_unscaled: 0.5436 (0.5805)  time: 0.1375  data: 0.0002  max mem: 1206
Epoch: [2]  [1400/1510]  eta: 0:00:14  lr: 0.000010  loss: 0.4961 (0.5795)  labels_encoder: 0.4961 (0.5795)  labels_encoder_unscaled: 0.4961 (0.5795)  time: 0.1422  data: 0.0002  max mem: 1206
Epoch: [2]  [1450/1510]  eta: 0:00:07  lr: 0.000010  loss: 0.5897 (0.5790)  labels_encoder: 0.5897 (0.5790)  labels_encoder_unscaled: 0.5897 (0.5790)  time: 0.1376  data: 0.0002  max mem: 1206
Epoch: [2]  [1500/1510]  eta: 0:00:01  lr: 0.000010  loss: 0.4999 (0.5774)  labels_encoder: 0.4999 (0.5774)  labels_encoder_unscaled: 0.4999 (0.5774)  time: 0.1277  data: 0.0004  max mem: 1206
Epoch: [2]  [1509/1510]  eta: 0:00:00  lr: 0.000010  loss: 0.4999 (0.5769)  labels_encoder: 0.4999 (0.5769)  labels_encoder_unscaled: 0.4999 (0.5769)  time: 0.1203  data: 0.0003  max mem: 1206
Epoch: [2] Total time: 0:03:19 (0.1318 s / it)
Averaged stats: lr: 0.000010  loss: 0.4999 (0.5769)  labels_encoder: 0.4999 (0.5769)  labels_encoder_unscaled: 0.4999 (0.5769)
Test:  [  0/559]  eta: 0:19:45  loss: 0.6418 (0.6418)  labels_encoder: 0.6418 (0.6418)  labels_encoder_unscaled: 0.6418 (0.6418)  time: 2.1207  data: 2.0614  max mem: 1206
Test:  [ 50/559]  eta: 0:00:53  loss: 0.9877 (0.9660)  labels_encoder: 0.9877 (0.9660)  labels_encoder_unscaled: 0.9877 (0.9660)  time: 0.0673  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:39  loss: 0.9305 (0.9456)  labels_encoder: 0.9305 (0.9456)  labels_encoder_unscaled: 0.9305 (0.9456)  time: 0.0677  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:31  loss: 0.9203 (0.9687)  labels_encoder: 0.9203 (0.9687)  labels_encoder_unscaled: 0.9203 (0.9687)  time: 0.0578  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:26  loss: 0.8186 (0.9000)  labels_encoder: 0.8186 (0.9000)  labels_encoder_unscaled: 0.8186 (0.9000)  time: 0.0570  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:21  loss: 0.8070 (1.0002)  labels_encoder: 0.8070 (1.0002)  labels_encoder_unscaled: 0.8070 (1.0002)  time: 0.0632  data: 0.0184  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.0521 (1.0408)  labels_encoder: 1.0521 (1.0408)  labels_encoder_unscaled: 1.0521 (1.0408)  time: 0.0555  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7193 (1.0585)  labels_encoder: 0.7193 (1.0585)  labels_encoder_unscaled: 0.7193 (1.0585)  time: 0.0550  data: 0.0012  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.4134 (1.0346)  labels_encoder: 0.4134 (1.0346)  labels_encoder_unscaled: 0.4134 (1.0346)  time: 0.0530  data: 0.0001  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4467 (1.0052)  labels_encoder: 0.4467 (1.0052)  labels_encoder_unscaled: 0.4467 (1.0052)  time: 0.0620  data: 0.0165  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.7106 (0.9878)  labels_encoder: 0.7106 (0.9878)  labels_encoder_unscaled: 0.7106 (0.9878)  time: 0.0750  data: 0.0317  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.9250 (0.9604)  labels_encoder: 0.9250 (0.9604)  labels_encoder_unscaled: 0.9250 (0.9604)  time: 0.0627  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3781 (0.9534)  labels_encoder: 0.3781 (0.9534)  labels_encoder_unscaled: 0.3781 (0.9534)  time: 0.0539  data: 0.0001  max mem: 1206
Test: Total time: 0:00:35 (0.0644 s / it)
Averaged stats: loss: 0.3781 (0.9534)  labels_encoder: 0.3781 (0.9534)  labels_encoder_unscaled: 0.3781 (0.9534)
(21, 71496)
(21, 71496)
[Epoch-2] [IDU-tvseries_anet_features.pickle] mAP: 0.1144, mcAP: 0.8710

BaseballPitch: 0.0413
BasketballDunk: 0.1105
Billiards: 0.0040
CleanAndJerk: 0.3782
CliffDiving: 0.2825
CricketBowling: 0.0632
CricketShot: 0.0940
Diving: 0.0047
FrisbeeCatch: 0.1142
GolfSwing: 0.0623
HammerThrow: 0.0819
HighJump: 0.0341
JavelinThrow: 0.0978
LongJump: 0.2456
PoleVault: 0.1188
Shotput: 0.1161
SoccerPenalty: 0.0407
TennisSwing: 0.2173
ThrowDiscus: 0.0376
VolleyballSpiking: 0.1431
Epoch: [3]  [   0/1510]  eta: 0:46:47  lr: 0.000001  loss: 0.5146 (0.5146)  labels_encoder: 0.5146 (0.5146)  labels_encoder_unscaled: 0.5146 (0.5146)  time: 1.8591  data: 1.6963  max mem: 1206
Epoch: [3]  [  50/1510]  eta: 0:03:59  lr: 0.000001  loss: 0.5561 (0.5750)  labels_encoder: 0.5561 (0.5750)  labels_encoder_unscaled: 0.5561 (0.5750)  time: 0.1170  data: 0.0002  max mem: 1206
Epoch: [3]  [ 100/1510]  eta: 0:03:21  lr: 0.000001  loss: 0.5573 (0.5722)  labels_encoder: 0.5573 (0.5722)  labels_encoder_unscaled: 0.5573 (0.5722)  time: 0.1208  data: 0.0002  max mem: 1206
Epoch: [3]  [ 150/1510]  eta: 0:03:07  lr: 0.000001  loss: 0.5138 (0.5640)  labels_encoder: 0.5138 (0.5640)  labels_encoder_unscaled: 0.5138 (0.5640)  time: 0.1285  data: 0.0002  max mem: 1206
Epoch: [3]  [ 200/1510]  eta: 0:02:56  lr: 0.000001  loss: 0.4873 (0.5531)  labels_encoder: 0.4873 (0.5531)  labels_encoder_unscaled: 0.4873 (0.5531)  time: 0.1222  data: 0.0002  max mem: 1206
Epoch: [3]  [ 250/1510]  eta: 0:02:47  lr: 0.000001  loss: 0.5847 (0.5573)  labels_encoder: 0.5847 (0.5573)  labels_encoder_unscaled: 0.5847 (0.5573)  time: 0.1286  data: 0.0002  max mem: 1206
Epoch: [3]  [ 300/1510]  eta: 0:02:42  lr: 0.000001  loss: 0.5512 (0.5562)  labels_encoder: 0.5512 (0.5562)  labels_encoder_unscaled: 0.5512 (0.5562)  time: 0.1428  data: 0.0002  max mem: 1206
Epoch: [3]  [ 350/1510]  eta: 0:02:36  lr: 0.000001  loss: 0.5335 (0.5550)  labels_encoder: 0.5335 (0.5550)  labels_encoder_unscaled: 0.5335 (0.5550)  time: 0.1370  data: 0.0002  max mem: 1206
Epoch: [3]  [ 400/1510]  eta: 0:02:30  lr: 0.000001  loss: 0.5320 (0.5495)  labels_encoder: 0.5320 (0.5495)  labels_encoder_unscaled: 0.5320 (0.5495)  time: 0.1391  data: 0.0002  max mem: 1206
Epoch: [3]  [ 450/1510]  eta: 0:02:23  lr: 0.000001  loss: 0.5022 (0.5471)  labels_encoder: 0.5022 (0.5471)  labels_encoder_unscaled: 0.5022 (0.5471)  time: 0.1391  data: 0.0002  max mem: 1206
Epoch: [3]  [ 500/1510]  eta: 0:02:17  lr: 0.000001  loss: 0.5026 (0.5458)  labels_encoder: 0.5026 (0.5458)  labels_encoder_unscaled: 0.5026 (0.5458)  time: 0.1331  data: 0.0002  max mem: 1206
Epoch: [3]  [ 550/1510]  eta: 0:02:10  lr: 0.000001  loss: 0.5105 (0.5457)  labels_encoder: 0.5105 (0.5457)  labels_encoder_unscaled: 0.5105 (0.5457)  time: 0.1368  data: 0.0002  max mem: 1206
Epoch: [3]  [ 600/1510]  eta: 0:02:03  lr: 0.000001  loss: 0.5065 (0.5447)  labels_encoder: 0.5065 (0.5447)  labels_encoder_unscaled: 0.5065 (0.5447)  time: 0.1404  data: 0.0002  max mem: 1206
Epoch: [3]  [ 650/1510]  eta: 0:01:56  lr: 0.000001  loss: 0.5514 (0.5443)  labels_encoder: 0.5514 (0.5443)  labels_encoder_unscaled: 0.5514 (0.5443)  time: 0.1193  data: 0.0002  max mem: 1206
Epoch: [3]  [ 700/1510]  eta: 0:01:48  lr: 0.000001  loss: 0.5183 (0.5423)  labels_encoder: 0.5183 (0.5423)  labels_encoder_unscaled: 0.5183 (0.5423)  time: 0.1270  data: 0.0002  max mem: 1206
Epoch: [3]  [ 750/1510]  eta: 0:01:42  lr: 0.000001  loss: 0.5274 (0.5409)  labels_encoder: 0.5274 (0.5409)  labels_encoder_unscaled: 0.5274 (0.5409)  time: 0.1407  data: 0.0002  max mem: 1206
Epoch: [3]  [ 800/1510]  eta: 0:01:35  lr: 0.000001  loss: 0.5371 (0.5406)  labels_encoder: 0.5371 (0.5406)  labels_encoder_unscaled: 0.5371 (0.5406)  time: 0.1366  data: 0.0002  max mem: 1206
Epoch: [3]  [ 850/1510]  eta: 0:01:28  lr: 0.000001  loss: 0.5846 (0.5419)  labels_encoder: 0.5846 (0.5419)  labels_encoder_unscaled: 0.5846 (0.5419)  time: 0.1230  data: 0.0002  max mem: 1206
Epoch: [3]  [ 900/1510]  eta: 0:01:21  lr: 0.000001  loss: 0.5244 (0.5412)  labels_encoder: 0.5244 (0.5412)  labels_encoder_unscaled: 0.5244 (0.5412)  time: 0.1191  data: 0.0002  max mem: 1206
Epoch: [3]  [ 950/1510]  eta: 0:01:14  lr: 0.000001  loss: 0.5342 (0.5413)  labels_encoder: 0.5342 (0.5413)  labels_encoder_unscaled: 0.5342 (0.5413)  time: 0.1362  data: 0.0002  max mem: 1206
Epoch: [3]  [1000/1510]  eta: 0:01:07  lr: 0.000001  loss: 0.5224 (0.5415)  labels_encoder: 0.5224 (0.5415)  labels_encoder_unscaled: 0.5224 (0.5415)  time: 0.1366  data: 0.0002  max mem: 1206
Epoch: [3]  [1050/1510]  eta: 0:01:01  lr: 0.000001  loss: 0.5151 (0.5412)  labels_encoder: 0.5151 (0.5412)  labels_encoder_unscaled: 0.5151 (0.5412)  time: 0.1377  data: 0.0002  max mem: 1206
Epoch: [3]  [1100/1510]  eta: 0:00:54  lr: 0.000001  loss: 0.5210 (0.5410)  labels_encoder: 0.5210 (0.5410)  labels_encoder_unscaled: 0.5210 (0.5410)  time: 0.1365  data: 0.0002  max mem: 1206
Epoch: [3]  [1150/1510]  eta: 0:00:48  lr: 0.000001  loss: 0.4992 (0.5398)  labels_encoder: 0.4992 (0.5398)  labels_encoder_unscaled: 0.4992 (0.5398)  time: 0.1332  data: 0.0002  max mem: 1206
Epoch: [3]  [1200/1510]  eta: 0:00:41  lr: 0.000001  loss: 0.5088 (0.5393)  labels_encoder: 0.5088 (0.5393)  labels_encoder_unscaled: 0.5088 (0.5393)  time: 0.1418  data: 0.0002  max mem: 1206
Epoch: [3]  [1250/1510]  eta: 0:00:34  lr: 0.000001  loss: 0.5228 (0.5392)  labels_encoder: 0.5228 (0.5392)  labels_encoder_unscaled: 0.5228 (0.5392)  time: 0.1438  data: 0.0002  max mem: 1206
Epoch: [3]  [1300/1510]  eta: 0:00:28  lr: 0.000001  loss: 0.5104 (0.5392)  labels_encoder: 0.5104 (0.5392)  labels_encoder_unscaled: 0.5104 (0.5392)  time: 0.1388  data: 0.0002  max mem: 1206
Epoch: [3]  [1350/1510]  eta: 0:00:21  lr: 0.000001  loss: 0.5269 (0.5392)  labels_encoder: 0.5269 (0.5392)  labels_encoder_unscaled: 0.5269 (0.5392)  time: 0.1214  data: 0.0002  max mem: 1206
Epoch: [3]  [1400/1510]  eta: 0:00:14  lr: 0.000001  loss: 0.5388 (0.5387)  labels_encoder: 0.5388 (0.5387)  labels_encoder_unscaled: 0.5388 (0.5387)  time: 0.1276  data: 0.0002  max mem: 1206
Epoch: [3]  [1450/1510]  eta: 0:00:07  lr: 0.000001  loss: 0.5620 (0.5386)  labels_encoder: 0.5620 (0.5386)  labels_encoder_unscaled: 0.5620 (0.5386)  time: 0.1179  data: 0.0002  max mem: 1206
Epoch: [3]  [1500/1510]  eta: 0:00:01  lr: 0.000001  loss: 0.5114 (0.5381)  labels_encoder: 0.5114 (0.5381)  labels_encoder_unscaled: 0.5114 (0.5381)  time: 0.1218  data: 0.0003  max mem: 1206
Epoch: [3]  [1509/1510]  eta: 0:00:00  lr: 0.000001  loss: 0.5232 (0.5380)  labels_encoder: 0.5232 (0.5380)  labels_encoder_unscaled: 0.5232 (0.5380)  time: 0.1126  data: 0.0002  max mem: 1206
Epoch: [3] Total time: 0:03:20 (0.1326 s / it)
Averaged stats: lr: 0.000001  loss: 0.5232 (0.5380)  labels_encoder: 0.5232 (0.5380)  labels_encoder_unscaled: 0.5232 (0.5380)
Test:  [  0/559]  eta: 0:20:05  loss: 0.6515 (0.6515)  labels_encoder: 0.6515 (0.6515)  labels_encoder_unscaled: 0.6515 (0.6515)  time: 2.1557  data: 2.0943  max mem: 1206
Test:  [ 50/559]  eta: 0:00:50  loss: 0.9707 (0.9538)  labels_encoder: 0.9707 (0.9538)  labels_encoder_unscaled: 0.9707 (0.9538)  time: 0.0587  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:36  loss: 0.5174 (0.8674)  labels_encoder: 0.5174 (0.8674)  labels_encoder_unscaled: 0.5174 (0.8674)  time: 0.0596  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.4797 (0.8803)  labels_encoder: 0.4797 (0.8803)  labels_encoder_unscaled: 0.4797 (0.8803)  time: 0.0502  data: 0.0001  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.5326 (0.8195)  labels_encoder: 0.5326 (0.8195)  labels_encoder_unscaled: 0.5326 (0.8195)  time: 0.0483  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:19  loss: 0.7298 (0.9328)  labels_encoder: 0.7298 (0.9328)  labels_encoder_unscaled: 0.7298 (0.9328)  time: 0.0499  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:15  loss: 0.9950 (0.9817)  labels_encoder: 0.9950 (0.9817)  labels_encoder_unscaled: 0.9950 (0.9817)  time: 0.0549  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:12  loss: 0.7584 (1.0035)  labels_encoder: 0.7584 (1.0035)  labels_encoder_unscaled: 0.7584 (1.0035)  time: 0.0539  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3020 (0.9765)  labels_encoder: 0.3020 (0.9765)  labels_encoder_unscaled: 0.3020 (0.9765)  time: 0.0569  data: 0.0001  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4327 (0.9533)  labels_encoder: 0.4327 (0.9533)  labels_encoder_unscaled: 0.4327 (0.9533)  time: 0.0691  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5573 (0.9341)  labels_encoder: 0.5573 (0.9341)  labels_encoder_unscaled: 0.5573 (0.9341)  time: 0.0605  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.8459 (0.9079)  labels_encoder: 0.8459 (0.9079)  labels_encoder_unscaled: 0.8459 (0.9079)  time: 0.0518  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3681 (0.9016)  labels_encoder: 0.3681 (0.9016)  labels_encoder_unscaled: 0.3681 (0.9016)  time: 0.0506  data: 0.0001  max mem: 1206
Test: Total time: 0:00:33 (0.0603 s / it)
Averaged stats: loss: 0.3681 (0.9016)  labels_encoder: 0.3681 (0.9016)  labels_encoder_unscaled: 0.3681 (0.9016)
(21, 71496)
(21, 71496)
[Epoch-3] [IDU-tvseries_anet_features.pickle] mAP: 0.1311, mcAP: 0.8819

BaseballPitch: 0.0513
BasketballDunk: 0.1050
Billiards: 0.0043
CleanAndJerk: 0.4045
CliffDiving: 0.4091
CricketBowling: 0.0503
CricketShot: 0.0840
Diving: 0.0043
FrisbeeCatch: 0.1518
GolfSwing: 0.0682
HammerThrow: 0.1234
HighJump: 0.0525
JavelinThrow: 0.0884
LongJump: 0.3423
PoleVault: 0.1045
Shotput: 0.1385
SoccerPenalty: 0.0505
TennisSwing: 0.1882
ThrowDiscus: 0.0469
VolleyballSpiking: 0.1545
Epoch: [4]  [   0/1510]  eta: 0:49:43  lr: 0.000000  loss: 0.4922 (0.4922)  labels_encoder: 0.4922 (0.4922)  labels_encoder_unscaled: 0.4922 (0.4922)  time: 1.9755  data: 1.8358  max mem: 1206
Epoch: [4]  [  50/1510]  eta: 0:04:07  lr: 0.000000  loss: 0.5252 (0.5249)  labels_encoder: 0.5252 (0.5249)  labels_encoder_unscaled: 0.5252 (0.5249)  time: 0.1260  data: 0.0002  max mem: 1206
Epoch: [4]  [ 100/1510]  eta: 0:03:41  lr: 0.000000  loss: 0.5057 (0.5264)  labels_encoder: 0.5057 (0.5264)  labels_encoder_unscaled: 0.5057 (0.5264)  time: 0.1431  data: 0.0002  max mem: 1206
Epoch: [4]  [ 150/1510]  eta: 0:03:27  lr: 0.000000  loss: 0.5657 (0.5291)  labels_encoder: 0.5657 (0.5291)  labels_encoder_unscaled: 0.5657 (0.5291)  time: 0.1455  data: 0.0002  max mem: 1206
Epoch: [4]  [ 200/1510]  eta: 0:03:14  lr: 0.000000  loss: 0.5545 (0.5293)  labels_encoder: 0.5545 (0.5293)  labels_encoder_unscaled: 0.5545 (0.5293)  time: 0.1328  data: 0.0002  max mem: 1206
Epoch: [4]  [ 250/1510]  eta: 0:03:01  lr: 0.000000  loss: 0.5092 (0.5279)  labels_encoder: 0.5092 (0.5279)  labels_encoder_unscaled: 0.5092 (0.5279)  time: 0.1287  data: 0.0002  max mem: 1206
Epoch: [4]  [ 300/1510]  eta: 0:02:52  lr: 0.000000  loss: 0.4934 (0.5256)  labels_encoder: 0.4934 (0.5256)  labels_encoder_unscaled: 0.4934 (0.5256)  time: 0.1416  data: 0.0002  max mem: 1206
Epoch: [4]  [ 350/1510]  eta: 0:02:44  lr: 0.000000  loss: 0.5128 (0.5269)  labels_encoder: 0.5128 (0.5269)  labels_encoder_unscaled: 0.5128 (0.5269)  time: 0.1353  data: 0.0002  max mem: 1206
Epoch: [4]  [ 400/1510]  eta: 0:02:36  lr: 0.000000  loss: 0.5040 (0.5258)  labels_encoder: 0.5040 (0.5258)  labels_encoder_unscaled: 0.5040 (0.5258)  time: 0.1367  data: 0.0002  max mem: 1206
Epoch: [4]  [ 450/1510]  eta: 0:02:29  lr: 0.000000  loss: 0.5366 (0.5270)  labels_encoder: 0.5366 (0.5270)  labels_encoder_unscaled: 0.5366 (0.5270)  time: 0.1369  data: 0.0002  max mem: 1206
Epoch: [4]  [ 500/1510]  eta: 0:02:21  lr: 0.000000  loss: 0.5324 (0.5295)  labels_encoder: 0.5324 (0.5295)  labels_encoder_unscaled: 0.5324 (0.5295)  time: 0.1330  data: 0.0002  max mem: 1206
Epoch: [4]  [ 550/1510]  eta: 0:02:13  lr: 0.000000  loss: 0.5183 (0.5288)  labels_encoder: 0.5183 (0.5288)  labels_encoder_unscaled: 0.5183 (0.5288)  time: 0.1196  data: 0.0002  max mem: 1206
Epoch: [4]  [ 600/1510]  eta: 0:02:05  lr: 0.000000  loss: 0.4308 (0.5275)  labels_encoder: 0.4308 (0.5275)  labels_encoder_unscaled: 0.4308 (0.5275)  time: 0.1433  data: 0.0002  max mem: 1206
Epoch: [4]  [ 650/1510]  eta: 0:01:59  lr: 0.000000  loss: 0.4959 (0.5265)  labels_encoder: 0.4959 (0.5265)  labels_encoder_unscaled: 0.4959 (0.5265)  time: 0.1433  data: 0.0002  max mem: 1206
Epoch: [4]  [ 700/1510]  eta: 0:01:52  lr: 0.000000  loss: 0.5265 (0.5271)  labels_encoder: 0.5265 (0.5271)  labels_encoder_unscaled: 0.5265 (0.5271)  time: 0.1339  data: 0.0002  max mem: 1206
Epoch: [4]  [ 750/1510]  eta: 0:01:44  lr: 0.000000  loss: 0.5118 (0.5264)  labels_encoder: 0.5118 (0.5264)  labels_encoder_unscaled: 0.5118 (0.5264)  time: 0.1196  data: 0.0002  max mem: 1206
Epoch: [4]  [ 800/1510]  eta: 0:01:37  lr: 0.000000  loss: 0.5135 (0.5266)  labels_encoder: 0.5135 (0.5266)  labels_encoder_unscaled: 0.5135 (0.5266)  time: 0.1345  data: 0.0002  max mem: 1206
Epoch: [4]  [ 850/1510]  eta: 0:01:30  lr: 0.000000  loss: 0.5329 (0.5268)  labels_encoder: 0.5329 (0.5268)  labels_encoder_unscaled: 0.5329 (0.5268)  time: 0.1436  data: 0.0002  max mem: 1206
Epoch: [4]  [ 900/1510]  eta: 0:01:23  lr: 0.000000  loss: 0.5066 (0.5267)  labels_encoder: 0.5066 (0.5267)  labels_encoder_unscaled: 0.5066 (0.5267)  time: 0.1442  data: 0.0002  max mem: 1206
Epoch: [4]  [ 950/1510]  eta: 0:01:16  lr: 0.000000  loss: 0.5808 (0.5271)  labels_encoder: 0.5808 (0.5271)  labels_encoder_unscaled: 0.5808 (0.5271)  time: 0.1357  data: 0.0002  max mem: 1206
Epoch: [4]  [1000/1510]  eta: 0:01:09  lr: 0.000000  loss: 0.4916 (0.5271)  labels_encoder: 0.4916 (0.5271)  labels_encoder_unscaled: 0.4916 (0.5271)  time: 0.1232  data: 0.0002  max mem: 1206
Epoch: [4]  [1050/1510]  eta: 0:01:02  lr: 0.000000  loss: 0.5030 (0.5271)  labels_encoder: 0.5030 (0.5271)  labels_encoder_unscaled: 0.5030 (0.5271)  time: 0.1386  data: 0.0002  max mem: 1206
Epoch: [4]  [1100/1510]  eta: 0:00:56  lr: 0.000000  loss: 0.5422 (0.5282)  labels_encoder: 0.5422 (0.5282)  labels_encoder_unscaled: 0.5422 (0.5282)  time: 0.1432  data: 0.0002  max mem: 1206
Epoch: [4]  [1150/1510]  eta: 0:00:49  lr: 0.000000  loss: 0.4304 (0.5270)  labels_encoder: 0.4304 (0.5270)  labels_encoder_unscaled: 0.4304 (0.5270)  time: 0.1406  data: 0.0002  max mem: 1206
Epoch: [4]  [1200/1510]  eta: 0:00:42  lr: 0.000000  loss: 0.4466 (0.5266)  labels_encoder: 0.4466 (0.5266)  labels_encoder_unscaled: 0.4466 (0.5266)  time: 0.1261  data: 0.0002  max mem: 1206
Epoch: [4]  [1250/1510]  eta: 0:00:35  lr: 0.000000  loss: 0.5196 (0.5272)  labels_encoder: 0.5196 (0.5272)  labels_encoder_unscaled: 0.5196 (0.5272)  time: 0.1239  data: 0.0002  max mem: 1206
Epoch: [4]  [1300/1510]  eta: 0:00:28  lr: 0.000000  loss: 0.5210 (0.5274)  labels_encoder: 0.5210 (0.5274)  labels_encoder_unscaled: 0.5210 (0.5274)  time: 0.1329  data: 0.0002  max mem: 1206
Epoch: [4]  [1350/1510]  eta: 0:00:21  lr: 0.000000  loss: 0.4572 (0.5265)  labels_encoder: 0.4572 (0.5265)  labels_encoder_unscaled: 0.4572 (0.5265)  time: 0.1328  data: 0.0002  max mem: 1206
Epoch: [4]  [1400/1510]  eta: 0:00:14  lr: 0.000000  loss: 0.5045 (0.5264)  labels_encoder: 0.5045 (0.5264)  labels_encoder_unscaled: 0.5045 (0.5264)  time: 0.1374  data: 0.0002  max mem: 1206
Epoch: [4]  [1450/1510]  eta: 0:00:08  lr: 0.000000  loss: 0.5110 (0.5262)  labels_encoder: 0.5110 (0.5262)  labels_encoder_unscaled: 0.5110 (0.5262)  time: 0.1380  data: 0.0002  max mem: 1206
Epoch: [4]  [1500/1510]  eta: 0:00:01  lr: 0.000000  loss: 0.5034 (0.5262)  labels_encoder: 0.5034 (0.5262)  labels_encoder_unscaled: 0.5034 (0.5262)  time: 0.1315  data: 0.0004  max mem: 1206
Epoch: [4]  [1509/1510]  eta: 0:00:00  lr: 0.000000  loss: 0.5035 (0.5262)  labels_encoder: 0.5035 (0.5262)  labels_encoder_unscaled: 0.5035 (0.5262)  time: 0.1229  data: 0.0003  max mem: 1206
Epoch: [4] Total time: 0:03:25 (0.1362 s / it)
Averaged stats: lr: 0.000000  loss: 0.5035 (0.5262)  labels_encoder: 0.5035 (0.5262)  labels_encoder_unscaled: 0.5035 (0.5262)
Test:  [  0/559]  eta: 0:15:44  loss: 0.6650 (0.6650)  labels_encoder: 0.6650 (0.6650)  labels_encoder_unscaled: 0.6650 (0.6650)  time: 1.6903  data: 1.6163  max mem: 1206
Test:  [ 50/559]  eta: 0:00:46  loss: 0.9399 (0.9172)  labels_encoder: 0.9399 (0.9172)  labels_encoder_unscaled: 0.9399 (0.9172)  time: 0.0505  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:34  loss: 0.5143 (0.8609)  labels_encoder: 0.5143 (0.8609)  labels_encoder_unscaled: 0.5143 (0.8609)  time: 0.0627  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.4595 (0.8812)  labels_encoder: 0.4595 (0.8812)  labels_encoder_unscaled: 0.4595 (0.8812)  time: 0.0559  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.6409 (0.8171)  labels_encoder: 0.6409 (0.8171)  labels_encoder_unscaled: 0.6409 (0.8171)  time: 0.0570  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.6807 (0.9438)  labels_encoder: 0.6807 (0.9438)  labels_encoder_unscaled: 0.6807 (0.9438)  time: 0.0600  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.1032 (0.9924)  labels_encoder: 1.1032 (0.9924)  labels_encoder_unscaled: 1.1032 (0.9924)  time: 0.0575  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7602 (1.0148)  labels_encoder: 0.7602 (1.0148)  labels_encoder_unscaled: 0.7602 (1.0148)  time: 0.0565  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3303 (0.9879)  labels_encoder: 0.3303 (0.9879)  labels_encoder_unscaled: 0.3303 (0.9879)  time: 0.0560  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4264 (0.9623)  labels_encoder: 0.4264 (0.9623)  labels_encoder_unscaled: 0.4264 (0.9623)  time: 0.0545  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6194 (0.9429)  labels_encoder: 0.6194 (0.9429)  labels_encoder_unscaled: 0.6194 (0.9429)  time: 0.0571  data: 0.0010  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.8235 (0.9155)  labels_encoder: 0.8235 (0.9155)  labels_encoder_unscaled: 0.8235 (0.9155)  time: 0.0503  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3619 (0.9090)  labels_encoder: 0.3619 (0.9090)  labels_encoder_unscaled: 0.3619 (0.9090)  time: 0.0405  data: 0.0001  max mem: 1206
Test: Total time: 0:00:33 (0.0605 s / it)
Averaged stats: loss: 0.3619 (0.9090)  labels_encoder: 0.3619 (0.9090)  labels_encoder_unscaled: 0.3619 (0.9090)
(21, 71496)
(21, 71496)
[Epoch-4] [IDU-tvseries_anet_features.pickle] mAP: 0.1253, mcAP: 0.8766

BaseballPitch: 0.0572
BasketballDunk: 0.1212
Billiards: 0.0041
CleanAndJerk: 0.3928
CliffDiving: 0.4030
CricketBowling: 0.0558
CricketShot: 0.0929
Diving: 0.0047
FrisbeeCatch: 0.1325
GolfSwing: 0.0652
HammerThrow: 0.0816
HighJump: 0.0327
JavelinThrow: 0.0820
LongJump: 0.3041
PoleVault: 0.1064
Shotput: 0.1457
SoccerPenalty: 0.0539
TennisSwing: 0.1938
ThrowDiscus: 0.0412
VolleyballSpiking: 0.1360
Training time 0:15:43
