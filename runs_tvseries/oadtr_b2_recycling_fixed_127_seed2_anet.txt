Unable to compile CoConv C++ implementation. Falling back to Python version.
[Errno 2] No such file or directory: '/home/lh/.conda/envs/oadtr/lib/python3.8/site-packages/continual/conv.cpp'
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'conv_flops_counter_hook'
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'MODULES_MAPPING'
Not using distributed mode
lr:0.0001
batch_size:128
weight_decay:0.0001
epochs:5
resize_feature:False
lr_drop:1
clip_max_norm:1.0
dataparallel:False
removelog:False
version:v3
query_num:8
decoder_layers:5
decoder_embedding_dim:1024
decoder_embedding_dim_out:1024
decoder_attn_dropout_rate:0.1
decoder_num_heads:4
classification_pred_loss_coef:0.5
enc_layers:64
lr_backbone:0.0001
feature:tvseries_anet_features.pickle
dim_feature:4096
patch_dim:1
embedding_dim:1024
num_heads:8
num_layers:2
attn_dropout_rate:0.1
positional_encoding_type:recycling_fixed
num_embeddings:127
hidden_dim:1024
dropout_rate:0.1
numclass:31
classification_x_loss_coef:0.3
classification_h_loss_coef:1
similar_loss_coef:0.1
margin:1.0
dataset:tvseries
dataset_file:data/data_info_new.json
frozen_weights:None
thumos_data_path:/home/dancer/mycode/Temporal.Online.Detection/Online.TRN.Pytorch/preprocess/
thumos_anno_path:data/thumos_{}_anno.pickle
remove_difficult:False
device:cuda
output_dir:models
seed:2
resume:
start_epoch:1
eval:False
num_workers:8
world_size:1
dist_url:tcp://127.0.0.1:12342
train_session_set:['24_ep1', '24_ep2', '24_ep3', 'Breaking_Bad_ep1', 'Breaking_Bad_ep2', 'How_I_Met_Your_Mother_ep1', 'How_I_Met_Your_Mother_ep2', 'How_I_Met_Your_Mother_ep3', 'How_I_Met_Your_Mother_ep4', 'How_I_Met_Your_Mother_ep5', 'How_I_Met_Your_Mother_ep6', 'Mad_Men_ep1', 'Mad_Men_ep2', 'Modern_Family_ep1', 'Modern_Family_ep2', 'Modern_Family_ep3', 'Modern_Family_ep4', 'Modern_Family_ep6', 'Sons_of_Anarchy_ep1', 'Sons_of_Anarchy_ep2']
test_session_set:['24_ep4', 'Breaking_Bad_ep3', 'Mad_Men_ep3', 'How_I_Met_Your_Mother_ep7', 'How_I_Met_Your_Mother_ep8', 'Modern_Family_ep5', 'Sons_of_Anarchy_ep3']
class_index:['background', 'Pick something up', 'Point', 'Drink', 'Stand up', 'Run', 'Sit down', 'Read', 'Smoke', 'Drive car', 'Open door', 'Give something', 'Use computer', 'Write', 'Go down stairway', 'Close door', 'Throw something', 'Go up stairway', 'Get in/out of car', 'Hang up phone', 'Eat', 'Answer phone', 'Dress up', 'Clap', 'Undress', 'Kiss', 'Fall/trip', 'Wave', 'Pour', 'Punch', 'Fire weapon']
distributed:False
position encoding : recycling_fixed
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Sequential(
  16.814 M, 99.939% Params, 0.412 GMac, 100.000% MACs, 
  (0): Linear(4.195 M, 24.936% Params, 0.004 GMac, 1.018% MACs, in_features=4096, out_features=1024, bias=True, channel_dim=1)
  (1): RecyclingPositionalEncoding(
    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 
    (pe): CyclicPositionalEncoding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
  )
  (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
  (3): Sequential(
    12.587 M, 74.814% Params, 0.408 GMac, 98.974% MACs, 
    (0): Sequential(
      6.294 M, 37.407% Params, 0.205 GMac, 49.804% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, reduce=reduce_sum
        (0): RetroactiveUnity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 63)
        (1): CoReMultiheadAttention(
          4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(Sequential(
        2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
        (0): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
        (1): Residual(
          2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
          (fn): Sequential(
            2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
            (0): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
            (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
            (3): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          )
        )
        (2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
      ))
    )
    (1): Lambda(Sequential(
      6.294 M, 37.407% Params, 0.203 GMac, 49.170% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, reduce=sum_last_pairs
        (0): SelectOrDelay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): CoSiMultiheadAttention(
          4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
      (2): BroadcastReduce(
        2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, reduce=reduce_sum
        (0): Delay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): Sequential(
          2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, 
          (0): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
          (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          (3): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
        )
      )
      (3): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
    ), takes_time=True)
    (2): Lambda(unity, squeeze_last, squeeze_last, takes_time=True)
  )
  (4): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
  (5): Linear(0.032 M, 0.189% Params, 0.0 GMac, 0.008% MACs, in_features=1024, out_features=31, bias=True, channel_dim=1)
)
Model FLOPs: 411997217.0
Model params: 16824351
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| Active memory         |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   83968 KB |   83968 KB |   83968 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16445 KB |   16445 KB |  302954 KB |  286508 KB |
|---------------------------------------------------------------------------|
| Allocations           |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| Active allocs         |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |    1458    |    1448    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Memory state pre, max, post inference: 69143040 71518720 69143040
Loaded tvseries_anet_features.pickle
Loaded tvseries_anet_features.pickle
Start training
Epoch: [1]  [   0/1510]  eta: 1:06:33  lr: 0.000100  loss: 3.6220 (3.6220)  labels_encoder: 3.6220 (3.6220)  labels_encoder_unscaled: 3.6220 (3.6220)  time: 2.6448  data: 2.0343  max mem: 1013
Epoch: [1]  [  50/1510]  eta: 0:04:19  lr: 0.000100  loss: 0.9698 (1.1477)  labels_encoder: 0.9698 (1.1477)  labels_encoder_unscaled: 0.9698 (1.1477)  time: 0.1178  data: 0.0002  max mem: 1206
Epoch: [1]  [ 100/1510]  eta: 0:03:37  lr: 0.000100  loss: 1.0073 (1.0797)  labels_encoder: 1.0073 (1.0797)  labels_encoder_unscaled: 1.0073 (1.0797)  time: 0.1328  data: 0.0002  max mem: 1206
Epoch: [1]  [ 150/1510]  eta: 0:03:18  lr: 0.000100  loss: 0.8808 (1.0248)  labels_encoder: 0.8808 (1.0248)  labels_encoder_unscaled: 0.8808 (1.0248)  time: 0.1315  data: 0.0002  max mem: 1206
Epoch: [1]  [ 200/1510]  eta: 0:03:08  lr: 0.000100  loss: 0.8887 (0.9981)  labels_encoder: 0.8887 (0.9981)  labels_encoder_unscaled: 0.8887 (0.9981)  time: 0.1376  data: 0.0002  max mem: 1206
Epoch: [1]  [ 250/1510]  eta: 0:02:58  lr: 0.000100  loss: 0.8466 (0.9688)  labels_encoder: 0.8466 (0.9688)  labels_encoder_unscaled: 0.8466 (0.9688)  time: 0.1288  data: 0.0002  max mem: 1206
Epoch: [1]  [ 300/1510]  eta: 0:02:51  lr: 0.000100  loss: 0.8509 (0.9524)  labels_encoder: 0.8509 (0.9524)  labels_encoder_unscaled: 0.8509 (0.9524)  time: 0.1421  data: 0.0002  max mem: 1206
Epoch: [1]  [ 350/1510]  eta: 0:02:43  lr: 0.000100  loss: 0.8300 (0.9349)  labels_encoder: 0.8300 (0.9349)  labels_encoder_unscaled: 0.8300 (0.9349)  time: 0.1365  data: 0.0002  max mem: 1206
Epoch: [1]  [ 400/1510]  eta: 0:02:34  lr: 0.000100  loss: 0.7951 (0.9259)  labels_encoder: 0.7951 (0.9259)  labels_encoder_unscaled: 0.7951 (0.9259)  time: 0.1157  data: 0.0002  max mem: 1206
Epoch: [1]  [ 450/1510]  eta: 0:02:25  lr: 0.000100  loss: 0.7395 (0.9123)  labels_encoder: 0.7395 (0.9123)  labels_encoder_unscaled: 0.7395 (0.9123)  time: 0.1154  data: 0.0002  max mem: 1206
Epoch: [1]  [ 500/1510]  eta: 0:02:17  lr: 0.000100  loss: 0.7922 (0.9015)  labels_encoder: 0.7922 (0.9015)  labels_encoder_unscaled: 0.7922 (0.9015)  time: 0.1267  data: 0.0002  max mem: 1206
Epoch: [1]  [ 550/1510]  eta: 0:02:09  lr: 0.000100  loss: 0.7029 (0.8874)  labels_encoder: 0.7029 (0.8874)  labels_encoder_unscaled: 0.7029 (0.8874)  time: 0.1405  data: 0.0002  max mem: 1206
Epoch: [1]  [ 600/1510]  eta: 0:02:03  lr: 0.000100  loss: 0.7332 (0.8735)  labels_encoder: 0.7332 (0.8735)  labels_encoder_unscaled: 0.7332 (0.8735)  time: 0.1371  data: 0.0002  max mem: 1206
Epoch: [1]  [ 650/1510]  eta: 0:01:55  lr: 0.000100  loss: 0.7234 (0.8667)  labels_encoder: 0.7234 (0.8667)  labels_encoder_unscaled: 0.7234 (0.8667)  time: 0.1173  data: 0.0002  max mem: 1206
Epoch: [1]  [ 700/1510]  eta: 0:01:48  lr: 0.000100  loss: 0.7349 (0.8580)  labels_encoder: 0.7349 (0.8580)  labels_encoder_unscaled: 0.7349 (0.8580)  time: 0.1310  data: 0.0002  max mem: 1206
Epoch: [1]  [ 750/1510]  eta: 0:01:41  lr: 0.000100  loss: 0.7340 (0.8513)  labels_encoder: 0.7340 (0.8513)  labels_encoder_unscaled: 0.7340 (0.8513)  time: 0.1202  data: 0.0002  max mem: 1206
Epoch: [1]  [ 800/1510]  eta: 0:01:34  lr: 0.000100  loss: 0.7113 (0.8438)  labels_encoder: 0.7113 (0.8438)  labels_encoder_unscaled: 0.7113 (0.8438)  time: 0.1220  data: 0.0002  max mem: 1206
Epoch: [1]  [ 850/1510]  eta: 0:01:27  lr: 0.000100  loss: 0.7009 (0.8387)  labels_encoder: 0.7009 (0.8387)  labels_encoder_unscaled: 0.7009 (0.8387)  time: 0.1312  data: 0.0002  max mem: 1206
Epoch: [1]  [ 900/1510]  eta: 0:01:20  lr: 0.000100  loss: 0.7040 (0.8310)  labels_encoder: 0.7040 (0.8310)  labels_encoder_unscaled: 0.7040 (0.8310)  time: 0.1282  data: 0.0002  max mem: 1206
Epoch: [1]  [ 950/1510]  eta: 0:01:13  lr: 0.000100  loss: 0.6923 (0.8251)  labels_encoder: 0.6923 (0.8251)  labels_encoder_unscaled: 0.6923 (0.8251)  time: 0.1383  data: 0.0002  max mem: 1206
Epoch: [1]  [1000/1510]  eta: 0:01:07  lr: 0.000100  loss: 0.6982 (0.8186)  labels_encoder: 0.6982 (0.8186)  labels_encoder_unscaled: 0.6982 (0.8186)  time: 0.1373  data: 0.0002  max mem: 1206
Epoch: [1]  [1050/1510]  eta: 0:01:01  lr: 0.000100  loss: 0.7285 (0.8141)  labels_encoder: 0.7285 (0.8141)  labels_encoder_unscaled: 0.7285 (0.8141)  time: 0.1415  data: 0.0002  max mem: 1206
Epoch: [1]  [1100/1510]  eta: 0:00:54  lr: 0.000100  loss: 0.6815 (0.8092)  labels_encoder: 0.6815 (0.8092)  labels_encoder_unscaled: 0.6815 (0.8092)  time: 0.1422  data: 0.0002  max mem: 1206
Epoch: [1]  [1150/1510]  eta: 0:00:48  lr: 0.000100  loss: 0.7424 (0.8040)  labels_encoder: 0.7424 (0.8040)  labels_encoder_unscaled: 0.7424 (0.8040)  time: 0.1444  data: 0.0002  max mem: 1206
Epoch: [1]  [1200/1510]  eta: 0:00:41  lr: 0.000100  loss: 0.7153 (0.7986)  labels_encoder: 0.7153 (0.7986)  labels_encoder_unscaled: 0.7153 (0.7986)  time: 0.1364  data: 0.0002  max mem: 1206
Epoch: [1]  [1250/1510]  eta: 0:00:34  lr: 0.000100  loss: 0.7116 (0.7948)  labels_encoder: 0.7116 (0.7948)  labels_encoder_unscaled: 0.7116 (0.7948)  time: 0.1365  data: 0.0002  max mem: 1206
Epoch: [1]  [1300/1510]  eta: 0:00:28  lr: 0.000100  loss: 0.6378 (0.7901)  labels_encoder: 0.6378 (0.7901)  labels_encoder_unscaled: 0.6378 (0.7901)  time: 0.1236  data: 0.0002  max mem: 1206
Epoch: [1]  [1350/1510]  eta: 0:00:21  lr: 0.000100  loss: 0.6176 (0.7851)  labels_encoder: 0.6176 (0.7851)  labels_encoder_unscaled: 0.6176 (0.7851)  time: 0.1385  data: 0.0002  max mem: 1206
Epoch: [1]  [1400/1510]  eta: 0:00:14  lr: 0.000100  loss: 0.5595 (0.7800)  labels_encoder: 0.5595 (0.7800)  labels_encoder_unscaled: 0.5595 (0.7800)  time: 0.1424  data: 0.0002  max mem: 1206
Epoch: [1]  [1450/1510]  eta: 0:00:08  lr: 0.000100  loss: 0.6113 (0.7754)  labels_encoder: 0.6113 (0.7754)  labels_encoder_unscaled: 0.6113 (0.7754)  time: 0.1432  data: 0.0002  max mem: 1206
Epoch: [1]  [1500/1510]  eta: 0:00:01  lr: 0.000100  loss: 0.7077 (0.7722)  labels_encoder: 0.7077 (0.7722)  labels_encoder_unscaled: 0.7077 (0.7722)  time: 0.1297  data: 0.0004  max mem: 1206
Epoch: [1]  [1509/1510]  eta: 0:00:00  lr: 0.000100  loss: 0.6500 (0.7710)  labels_encoder: 0.6500 (0.7710)  labels_encoder_unscaled: 0.6500 (0.7710)  time: 0.1207  data: 0.0003  max mem: 1206
Epoch: [1] Total time: 0:03:22 (0.1343 s / it)
Averaged stats: lr: 0.000100  loss: 0.6500 (0.7710)  labels_encoder: 0.6500 (0.7710)  labels_encoder_unscaled: 0.6500 (0.7710)
Test:  [  0/559]  eta: 0:17:52  loss: 0.5949 (0.5949)  labels_encoder: 0.5949 (0.5949)  labels_encoder_unscaled: 0.5949 (0.5949)  time: 1.9185  data: 1.8562  max mem: 1206
Test:  [ 50/559]  eta: 0:00:49  loss: 1.0573 (1.0033)  labels_encoder: 1.0573 (1.0033)  labels_encoder_unscaled: 1.0573 (1.0033)  time: 0.0581  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:36  loss: 0.5586 (0.9578)  labels_encoder: 0.5586 (0.9578)  labels_encoder_unscaled: 0.5586 (0.9578)  time: 0.0607  data: 0.0263  max mem: 1206
Test:  [150/559]  eta: 0:00:30  loss: 0.5330 (0.9579)  labels_encoder: 0.5330 (0.9579)  labels_encoder_unscaled: 0.5330 (0.9579)  time: 0.0638  data: 0.0311  max mem: 1206
Test:  [200/559]  eta: 0:00:25  loss: 0.4487 (0.8857)  labels_encoder: 0.4487 (0.8857)  labels_encoder_unscaled: 0.4487 (0.8857)  time: 0.0616  data: 0.0257  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 1.0772 (0.9829)  labels_encoder: 1.0772 (0.9829)  labels_encoder_unscaled: 1.0772 (0.9829)  time: 0.0491  data: 0.0001  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.1306 (1.0233)  labels_encoder: 1.1306 (1.0233)  labels_encoder_unscaled: 1.1306 (1.0233)  time: 0.0524  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.6475 (1.0411)  labels_encoder: 0.6475 (1.0411)  labels_encoder_unscaled: 0.6475 (1.0411)  time: 0.0634  data: 0.0307  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.4022 (1.0190)  labels_encoder: 0.4022 (1.0190)  labels_encoder_unscaled: 0.4022 (1.0190)  time: 0.0673  data: 0.0314  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4414 (0.9958)  labels_encoder: 0.4414 (0.9958)  labels_encoder_unscaled: 0.4414 (0.9958)  time: 0.0583  data: 0.0192  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.8075 (0.9829)  labels_encoder: 0.8075 (0.9829)  labels_encoder_unscaled: 0.8075 (0.9829)  time: 0.0676  data: 0.0352  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.6375 (0.9524)  labels_encoder: 0.6375 (0.9524)  labels_encoder_unscaled: 0.6375 (0.9524)  time: 0.0633  data: 0.0300  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3803 (0.9458)  labels_encoder: 0.3803 (0.9458)  labels_encoder_unscaled: 0.3803 (0.9458)  time: 0.0594  data: 0.0226  max mem: 1206
Test: Total time: 0:00:36 (0.0646 s / it)
Averaged stats: loss: 0.3803 (0.9458)  labels_encoder: 0.3803 (0.9458)  labels_encoder_unscaled: 0.3803 (0.9458)
(21, 71496)
(21, 71496)
[Epoch-1] [IDU-tvseries_anet_features.pickle] mAP: 0.1276, mcAP: 0.8656

BaseballPitch: 0.0843
BasketballDunk: 0.1231
Billiards: 0.0048
CleanAndJerk: 0.3941
CliffDiving: 0.5429
CricketBowling: 0.0703
CricketShot: 0.0902
Diving: 0.0038
FrisbeeCatch: 0.0625
GolfSwing: 0.0345
HammerThrow: 0.0793
HighJump: 0.0204
JavelinThrow: 0.0598
LongJump: 0.3513
PoleVault: 0.0913
Shotput: 0.1388
SoccerPenalty: 0.0354
TennisSwing: 0.2000
ThrowDiscus: 0.0437
VolleyballSpiking: 0.1221
Epoch: [2]  [   0/1510]  eta: 0:51:50  lr: 0.000010  loss: 0.3690 (0.3690)  labels_encoder: 0.3690 (0.3690)  labels_encoder_unscaled: 0.3690 (0.3690)  time: 2.0599  data: 1.8861  max mem: 1206
Epoch: [2]  [  50/1510]  eta: 0:04:16  lr: 0.000010  loss: 0.5750 (0.6058)  labels_encoder: 0.5750 (0.6058)  labels_encoder_unscaled: 0.5750 (0.6058)  time: 0.1290  data: 0.0002  max mem: 1206
Epoch: [2]  [ 100/1510]  eta: 0:03:34  lr: 0.000010  loss: 0.5795 (0.6057)  labels_encoder: 0.5795 (0.6057)  labels_encoder_unscaled: 0.5795 (0.6057)  time: 0.1269  data: 0.0002  max mem: 1206
Epoch: [2]  [ 150/1510]  eta: 0:03:17  lr: 0.000010  loss: 0.5973 (0.6065)  labels_encoder: 0.5973 (0.6065)  labels_encoder_unscaled: 0.5973 (0.6065)  time: 0.1324  data: 0.0002  max mem: 1206
Epoch: [2]  [ 200/1510]  eta: 0:03:08  lr: 0.000010  loss: 0.5323 (0.6039)  labels_encoder: 0.5323 (0.6039)  labels_encoder_unscaled: 0.5323 (0.6039)  time: 0.1422  data: 0.0002  max mem: 1206
Epoch: [2]  [ 250/1510]  eta: 0:02:57  lr: 0.000010  loss: 0.5363 (0.6001)  labels_encoder: 0.5363 (0.6001)  labels_encoder_unscaled: 0.5363 (0.6001)  time: 0.1199  data: 0.0002  max mem: 1206
Epoch: [2]  [ 300/1510]  eta: 0:02:48  lr: 0.000010  loss: 0.5972 (0.5955)  labels_encoder: 0.5972 (0.5955)  labels_encoder_unscaled: 0.5972 (0.5955)  time: 0.1332  data: 0.0002  max mem: 1206
Epoch: [2]  [ 350/1510]  eta: 0:02:38  lr: 0.000010  loss: 0.5313 (0.5918)  labels_encoder: 0.5313 (0.5918)  labels_encoder_unscaled: 0.5313 (0.5918)  time: 0.1193  data: 0.0002  max mem: 1206
Epoch: [2]  [ 400/1510]  eta: 0:02:31  lr: 0.000010  loss: 0.5278 (0.5904)  labels_encoder: 0.5278 (0.5904)  labels_encoder_unscaled: 0.5278 (0.5904)  time: 0.1299  data: 0.0002  max mem: 1206
Epoch: [2]  [ 450/1510]  eta: 0:02:23  lr: 0.000010  loss: 0.5555 (0.5879)  labels_encoder: 0.5555 (0.5879)  labels_encoder_unscaled: 0.5555 (0.5879)  time: 0.1215  data: 0.0002  max mem: 1206
Epoch: [2]  [ 500/1510]  eta: 0:02:15  lr: 0.000010  loss: 0.5498 (0.5847)  labels_encoder: 0.5498 (0.5847)  labels_encoder_unscaled: 0.5498 (0.5847)  time: 0.1287  data: 0.0002  max mem: 1206
Epoch: [2]  [ 550/1510]  eta: 0:02:07  lr: 0.000010  loss: 0.6298 (0.5851)  labels_encoder: 0.6298 (0.5851)  labels_encoder_unscaled: 0.6298 (0.5851)  time: 0.1222  data: 0.0002  max mem: 1206
Epoch: [2]  [ 600/1510]  eta: 0:02:00  lr: 0.000010  loss: 0.5336 (0.5824)  labels_encoder: 0.5336 (0.5824)  labels_encoder_unscaled: 0.5336 (0.5824)  time: 0.1257  data: 0.0002  max mem: 1206
Epoch: [2]  [ 650/1510]  eta: 0:01:52  lr: 0.000010  loss: 0.5648 (0.5824)  labels_encoder: 0.5648 (0.5824)  labels_encoder_unscaled: 0.5648 (0.5824)  time: 0.1197  data: 0.0002  max mem: 1206
Epoch: [2]  [ 700/1510]  eta: 0:01:46  lr: 0.000010  loss: 0.5541 (0.5808)  labels_encoder: 0.5541 (0.5808)  labels_encoder_unscaled: 0.5541 (0.5808)  time: 0.1415  data: 0.0002  max mem: 1206
Epoch: [2]  [ 750/1510]  eta: 0:01:40  lr: 0.000010  loss: 0.5137 (0.5795)  labels_encoder: 0.5137 (0.5795)  labels_encoder_unscaled: 0.5137 (0.5795)  time: 0.1402  data: 0.0002  max mem: 1206
Epoch: [2]  [ 800/1510]  eta: 0:01:33  lr: 0.000010  loss: 0.4898 (0.5769)  labels_encoder: 0.4898 (0.5769)  labels_encoder_unscaled: 0.4898 (0.5769)  time: 0.1187  data: 0.0002  max mem: 1206
Epoch: [2]  [ 850/1510]  eta: 0:01:27  lr: 0.000010  loss: 0.5301 (0.5765)  labels_encoder: 0.5301 (0.5765)  labels_encoder_unscaled: 0.5301 (0.5765)  time: 0.1509  data: 0.0003  max mem: 1206
Epoch: [2]  [ 900/1510]  eta: 0:01:20  lr: 0.000010  loss: 0.5003 (0.5741)  labels_encoder: 0.5003 (0.5741)  labels_encoder_unscaled: 0.5003 (0.5741)  time: 0.1224  data: 0.0002  max mem: 1206
Epoch: [2]  [ 950/1510]  eta: 0:01:13  lr: 0.000010  loss: 0.5364 (0.5723)  labels_encoder: 0.5364 (0.5723)  labels_encoder_unscaled: 0.5364 (0.5723)  time: 0.1177  data: 0.0002  max mem: 1206
Epoch: [2]  [1000/1510]  eta: 0:01:07  lr: 0.000010  loss: 0.5463 (0.5716)  labels_encoder: 0.5463 (0.5716)  labels_encoder_unscaled: 0.5463 (0.5716)  time: 0.1210  data: 0.0002  max mem: 1206
Epoch: [2]  [1050/1510]  eta: 0:01:00  lr: 0.000010  loss: 0.5319 (0.5699)  labels_encoder: 0.5319 (0.5699)  labels_encoder_unscaled: 0.5319 (0.5699)  time: 0.1397  data: 0.0002  max mem: 1206
Epoch: [2]  [1100/1510]  eta: 0:00:53  lr: 0.000010  loss: 0.5542 (0.5679)  labels_encoder: 0.5542 (0.5679)  labels_encoder_unscaled: 0.5542 (0.5679)  time: 0.1153  data: 0.0002  max mem: 1206
Epoch: [2]  [1150/1510]  eta: 0:00:46  lr: 0.000010  loss: 0.5773 (0.5664)  labels_encoder: 0.5773 (0.5664)  labels_encoder_unscaled: 0.5773 (0.5664)  time: 0.1201  data: 0.0002  max mem: 1206
Epoch: [2]  [1200/1510]  eta: 0:00:40  lr: 0.000010  loss: 0.5483 (0.5654)  labels_encoder: 0.5483 (0.5654)  labels_encoder_unscaled: 0.5483 (0.5654)  time: 0.1109  data: 0.0002  max mem: 1206
Epoch: [2]  [1250/1510]  eta: 0:00:33  lr: 0.000010  loss: 0.5017 (0.5642)  labels_encoder: 0.5017 (0.5642)  labels_encoder_unscaled: 0.5017 (0.5642)  time: 0.1292  data: 0.0002  max mem: 1206
Epoch: [2]  [1300/1510]  eta: 0:00:27  lr: 0.000010  loss: 0.4839 (0.5619)  labels_encoder: 0.4839 (0.5619)  labels_encoder_unscaled: 0.4839 (0.5619)  time: 0.1153  data: 0.0002  max mem: 1206
Epoch: [2]  [1350/1510]  eta: 0:00:20  lr: 0.000010  loss: 0.5331 (0.5607)  labels_encoder: 0.5331 (0.5607)  labels_encoder_unscaled: 0.5331 (0.5607)  time: 0.1290  data: 0.0002  max mem: 1206
Epoch: [2]  [1400/1510]  eta: 0:00:14  lr: 0.000010  loss: 0.5025 (0.5600)  labels_encoder: 0.5025 (0.5600)  labels_encoder_unscaled: 0.5025 (0.5600)  time: 0.1319  data: 0.0002  max mem: 1206
Epoch: [2]  [1450/1510]  eta: 0:00:07  lr: 0.000010  loss: 0.4978 (0.5595)  labels_encoder: 0.4978 (0.5595)  labels_encoder_unscaled: 0.4978 (0.5595)  time: 0.1310  data: 0.0002  max mem: 1206
Epoch: [2]  [1500/1510]  eta: 0:00:01  lr: 0.000010  loss: 0.4813 (0.5581)  labels_encoder: 0.4813 (0.5581)  labels_encoder_unscaled: 0.4813 (0.5581)  time: 0.1194  data: 0.0003  max mem: 1206
Epoch: [2]  [1509/1510]  eta: 0:00:00  lr: 0.000010  loss: 0.4913 (0.5577)  labels_encoder: 0.4913 (0.5577)  labels_encoder_unscaled: 0.4913 (0.5577)  time: 0.1114  data: 0.0002  max mem: 1206
Epoch: [2] Total time: 0:03:14 (0.1291 s / it)
Averaged stats: lr: 0.000010  loss: 0.4913 (0.5577)  labels_encoder: 0.4913 (0.5577)  labels_encoder_unscaled: 0.4913 (0.5577)
Test:  [  0/559]  eta: 0:16:27  loss: 0.5607 (0.5607)  labels_encoder: 0.5607 (0.5607)  labels_encoder_unscaled: 0.5607 (0.5607)  time: 1.7665  data: 1.7027  max mem: 1206
Test:  [ 50/559]  eta: 0:00:49  loss: 0.9484 (0.8943)  labels_encoder: 0.9484 (0.8943)  labels_encoder_unscaled: 0.9484 (0.8943)  time: 0.0642  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:37  loss: 0.6417 (0.8587)  labels_encoder: 0.6417 (0.8587)  labels_encoder_unscaled: 0.6417 (0.8587)  time: 0.0651  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:31  loss: 0.4058 (0.8759)  labels_encoder: 0.4058 (0.8759)  labels_encoder_unscaled: 0.4058 (0.8759)  time: 0.0681  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:26  loss: 0.6576 (0.8236)  labels_encoder: 0.6576 (0.8236)  labels_encoder_unscaled: 0.6576 (0.8236)  time: 0.0584  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:21  loss: 0.7589 (0.9363)  labels_encoder: 0.7589 (0.9363)  labels_encoder_unscaled: 0.7589 (0.9363)  time: 0.0607  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.0834 (0.9837)  labels_encoder: 1.0834 (0.9837)  labels_encoder_unscaled: 1.0834 (0.9837)  time: 0.0541  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7874 (1.0032)  labels_encoder: 0.7874 (1.0032)  labels_encoder_unscaled: 0.7874 (1.0032)  time: 0.0605  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3460 (0.9809)  labels_encoder: 0.3460 (0.9809)  labels_encoder_unscaled: 0.3460 (0.9809)  time: 0.0498  data: 0.0010  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4028 (0.9573)  labels_encoder: 0.4028 (0.9573)  labels_encoder_unscaled: 0.4028 (0.9573)  time: 0.0637  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5914 (0.9385)  labels_encoder: 0.5914 (0.9385)  labels_encoder_unscaled: 0.5914 (0.9385)  time: 0.0563  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7486 (0.9164)  labels_encoder: 0.7486 (0.9164)  labels_encoder_unscaled: 0.7486 (0.9164)  time: 0.0560  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3687 (0.9094)  labels_encoder: 0.3687 (0.9094)  labels_encoder_unscaled: 0.3687 (0.9094)  time: 0.0444  data: 0.0001  max mem: 1206
Test: Total time: 0:00:35 (0.0629 s / it)
Averaged stats: loss: 0.3687 (0.9094)  labels_encoder: 0.3687 (0.9094)  labels_encoder_unscaled: 0.3687 (0.9094)
(21, 71496)
(21, 71496)
[Epoch-2] [IDU-tvseries_anet_features.pickle] mAP: 0.1336, mcAP: 0.8781

BaseballPitch: 0.0711
BasketballDunk: 0.1105
Billiards: 0.0044
CleanAndJerk: 0.3998
CliffDiving: 0.4509
CricketBowling: 0.0702
CricketShot: 0.1088
Diving: 0.0028
FrisbeeCatch: 0.1405
GolfSwing: 0.0721
HammerThrow: 0.1139
HighJump: 0.0366
JavelinThrow: 0.0418
LongJump: 0.3793
PoleVault: 0.1038
Shotput: 0.1459
SoccerPenalty: 0.0519
TennisSwing: 0.1712
ThrowDiscus: 0.0545
VolleyballSpiking: 0.1424
Epoch: [3]  [   0/1510]  eta: 0:51:42  lr: 0.000001  loss: 0.5257 (0.5257)  labels_encoder: 0.5257 (0.5257)  labels_encoder_unscaled: 0.5257 (0.5257)  time: 2.0545  data: 1.8808  max mem: 1206
Epoch: [3]  [  50/1510]  eta: 0:04:05  lr: 0.000001  loss: 0.5166 (0.5289)  labels_encoder: 0.5166 (0.5289)  labels_encoder_unscaled: 0.5166 (0.5289)  time: 0.1226  data: 0.0002  max mem: 1206
Epoch: [3]  [ 100/1510]  eta: 0:03:26  lr: 0.000001  loss: 0.5263 (0.5292)  labels_encoder: 0.5263 (0.5292)  labels_encoder_unscaled: 0.5263 (0.5292)  time: 0.1255  data: 0.0002  max mem: 1206
Epoch: [3]  [ 150/1510]  eta: 0:03:08  lr: 0.000001  loss: 0.5240 (0.5304)  labels_encoder: 0.5240 (0.5304)  labels_encoder_unscaled: 0.5240 (0.5304)  time: 0.1228  data: 0.0002  max mem: 1206
Epoch: [3]  [ 200/1510]  eta: 0:02:57  lr: 0.000001  loss: 0.4721 (0.5285)  labels_encoder: 0.4721 (0.5285)  labels_encoder_unscaled: 0.4721 (0.5285)  time: 0.1265  data: 0.0002  max mem: 1206
Epoch: [3]  [ 250/1510]  eta: 0:02:50  lr: 0.000001  loss: 0.4658 (0.5263)  labels_encoder: 0.4658 (0.5263)  labels_encoder_unscaled: 0.4658 (0.5263)  time: 0.1477  data: 0.0003  max mem: 1206
Epoch: [3]  [ 300/1510]  eta: 0:02:42  lr: 0.000001  loss: 0.4806 (0.5225)  labels_encoder: 0.4806 (0.5225)  labels_encoder_unscaled: 0.4806 (0.5225)  time: 0.1322  data: 0.0002  max mem: 1206
Epoch: [3]  [ 350/1510]  eta: 0:02:34  lr: 0.000001  loss: 0.5187 (0.5225)  labels_encoder: 0.5187 (0.5225)  labels_encoder_unscaled: 0.5187 (0.5225)  time: 0.1203  data: 0.0002  max mem: 1206
Epoch: [3]  [ 400/1510]  eta: 0:02:26  lr: 0.000001  loss: 0.5102 (0.5215)  labels_encoder: 0.5102 (0.5215)  labels_encoder_unscaled: 0.5102 (0.5215)  time: 0.1292  data: 0.0002  max mem: 1206
Epoch: [3]  [ 450/1510]  eta: 0:02:19  lr: 0.000001  loss: 0.4536 (0.5183)  labels_encoder: 0.4536 (0.5183)  labels_encoder_unscaled: 0.4536 (0.5183)  time: 0.1253  data: 0.0002  max mem: 1206
Epoch: [3]  [ 500/1510]  eta: 0:02:12  lr: 0.000001  loss: 0.4966 (0.5181)  labels_encoder: 0.4966 (0.5181)  labels_encoder_unscaled: 0.4966 (0.5181)  time: 0.1165  data: 0.0002  max mem: 1206
Epoch: [3]  [ 550/1510]  eta: 0:02:05  lr: 0.000001  loss: 0.4692 (0.5167)  labels_encoder: 0.4692 (0.5167)  labels_encoder_unscaled: 0.4692 (0.5167)  time: 0.1274  data: 0.0002  max mem: 1206
Epoch: [3]  [ 600/1510]  eta: 0:01:58  lr: 0.000001  loss: 0.4902 (0.5153)  labels_encoder: 0.4902 (0.5153)  labels_encoder_unscaled: 0.4902 (0.5153)  time: 0.1300  data: 0.0002  max mem: 1206
Epoch: [3]  [ 650/1510]  eta: 0:01:51  lr: 0.000001  loss: 0.5406 (0.5163)  labels_encoder: 0.5406 (0.5163)  labels_encoder_unscaled: 0.5406 (0.5163)  time: 0.1194  data: 0.0002  max mem: 1206
Epoch: [3]  [ 700/1510]  eta: 0:01:45  lr: 0.000001  loss: 0.4846 (0.5151)  labels_encoder: 0.4846 (0.5151)  labels_encoder_unscaled: 0.4846 (0.5151)  time: 0.1310  data: 0.0002  max mem: 1206
Epoch: [3]  [ 750/1510]  eta: 0:01:38  lr: 0.000001  loss: 0.5050 (0.5147)  labels_encoder: 0.5050 (0.5147)  labels_encoder_unscaled: 0.5050 (0.5147)  time: 0.1294  data: 0.0002  max mem: 1206
Epoch: [3]  [ 800/1510]  eta: 0:01:31  lr: 0.000001  loss: 0.4960 (0.5150)  labels_encoder: 0.4960 (0.5150)  labels_encoder_unscaled: 0.4960 (0.5150)  time: 0.1191  data: 0.0002  max mem: 1206
Epoch: [3]  [ 850/1510]  eta: 0:01:24  lr: 0.000001  loss: 0.5338 (0.5146)  labels_encoder: 0.5338 (0.5146)  labels_encoder_unscaled: 0.5338 (0.5146)  time: 0.1208  data: 0.0002  max mem: 1206
Epoch: [3]  [ 900/1510]  eta: 0:01:18  lr: 0.000001  loss: 0.4853 (0.5144)  labels_encoder: 0.4853 (0.5144)  labels_encoder_unscaled: 0.4853 (0.5144)  time: 0.1255  data: 0.0002  max mem: 1206
Epoch: [3]  [ 950/1510]  eta: 0:01:11  lr: 0.000001  loss: 0.4771 (0.5133)  labels_encoder: 0.4771 (0.5133)  labels_encoder_unscaled: 0.4771 (0.5133)  time: 0.1381  data: 0.0004  max mem: 1206
Epoch: [3]  [1000/1510]  eta: 0:01:05  lr: 0.000001  loss: 0.5266 (0.5133)  labels_encoder: 0.5266 (0.5133)  labels_encoder_unscaled: 0.5266 (0.5133)  time: 0.1323  data: 0.0002  max mem: 1206
Epoch: [3]  [1050/1510]  eta: 0:00:59  lr: 0.000001  loss: 0.5393 (0.5130)  labels_encoder: 0.5393 (0.5130)  labels_encoder_unscaled: 0.5393 (0.5130)  time: 0.1122  data: 0.0001  max mem: 1206
Epoch: [3]  [1100/1510]  eta: 0:00:52  lr: 0.000001  loss: 0.5234 (0.5130)  labels_encoder: 0.5234 (0.5130)  labels_encoder_unscaled: 0.5234 (0.5130)  time: 0.1103  data: 0.0001  max mem: 1206
Epoch: [3]  [1150/1510]  eta: 0:00:45  lr: 0.000001  loss: 0.4751 (0.5127)  labels_encoder: 0.4751 (0.5127)  labels_encoder_unscaled: 0.4751 (0.5127)  time: 0.1119  data: 0.0002  max mem: 1206
Epoch: [3]  [1200/1510]  eta: 0:00:39  lr: 0.000001  loss: 0.4691 (0.5126)  labels_encoder: 0.4691 (0.5126)  labels_encoder_unscaled: 0.4691 (0.5126)  time: 0.1219  data: 0.0002  max mem: 1206
Epoch: [3]  [1250/1510]  eta: 0:00:32  lr: 0.000001  loss: 0.5204 (0.5118)  labels_encoder: 0.5204 (0.5118)  labels_encoder_unscaled: 0.5204 (0.5118)  time: 0.1301  data: 0.0002  max mem: 1206
Epoch: [3]  [1300/1510]  eta: 0:00:26  lr: 0.000001  loss: 0.5385 (0.5127)  labels_encoder: 0.5385 (0.5127)  labels_encoder_unscaled: 0.5385 (0.5127)  time: 0.1326  data: 0.0002  max mem: 1206
Epoch: [3]  [1350/1510]  eta: 0:00:20  lr: 0.000001  loss: 0.4762 (0.5128)  labels_encoder: 0.4762 (0.5128)  labels_encoder_unscaled: 0.4762 (0.5128)  time: 0.1199  data: 0.0002  max mem: 1206
Epoch: [3]  [1400/1510]  eta: 0:00:13  lr: 0.000001  loss: 0.5013 (0.5129)  labels_encoder: 0.5013 (0.5129)  labels_encoder_unscaled: 0.5013 (0.5129)  time: 0.1187  data: 0.0002  max mem: 1206
Epoch: [3]  [1450/1510]  eta: 0:00:07  lr: 0.000001  loss: 0.5029 (0.5131)  labels_encoder: 0.5029 (0.5131)  labels_encoder_unscaled: 0.5029 (0.5131)  time: 0.1259  data: 0.0002  max mem: 1206
Epoch: [3]  [1500/1510]  eta: 0:00:01  lr: 0.000001  loss: 0.5456 (0.5128)  labels_encoder: 0.5456 (0.5128)  labels_encoder_unscaled: 0.5456 (0.5128)  time: 0.1215  data: 0.0003  max mem: 1206
Epoch: [3]  [1509/1510]  eta: 0:00:00  lr: 0.000001  loss: 0.5004 (0.5125)  labels_encoder: 0.5004 (0.5125)  labels_encoder_unscaled: 0.5004 (0.5125)  time: 0.1124  data: 0.0002  max mem: 1206
Epoch: [3] Total time: 0:03:11 (0.1266 s / it)
Averaged stats: lr: 0.000001  loss: 0.5004 (0.5125)  labels_encoder: 0.5004 (0.5125)  labels_encoder_unscaled: 0.5004 (0.5125)
Test:  [  0/559]  eta: 0:16:47  loss: 0.6174 (0.6174)  labels_encoder: 0.6174 (0.6174)  labels_encoder_unscaled: 0.6174 (0.6174)  time: 1.8021  data: 1.7157  max mem: 1206
Test:  [ 50/559]  eta: 0:00:42  loss: 0.9621 (0.8818)  labels_encoder: 0.9621 (0.8818)  labels_encoder_unscaled: 0.9621 (0.8818)  time: 0.0476  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:30  loss: 0.4651 (0.8453)  labels_encoder: 0.4651 (0.8453)  labels_encoder_unscaled: 0.4651 (0.8453)  time: 0.0484  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:24  loss: 0.4317 (0.8703)  labels_encoder: 0.4317 (0.8703)  labels_encoder_unscaled: 0.4317 (0.8703)  time: 0.0492  data: 0.0009  max mem: 1206
Test:  [200/559]  eta: 0:00:20  loss: 0.6164 (0.8148)  labels_encoder: 0.6164 (0.8148)  labels_encoder_unscaled: 0.6164 (0.8148)  time: 0.0465  data: 0.0001  max mem: 1206
Test:  [250/559]  eta: 0:00:17  loss: 0.7700 (0.9248)  labels_encoder: 0.7700 (0.9248)  labels_encoder_unscaled: 0.7700 (0.9248)  time: 0.0512  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:14  loss: 0.9915 (0.9798)  labels_encoder: 0.9915 (0.9798)  labels_encoder_unscaled: 0.9915 (0.9798)  time: 0.0473  data: 0.0001  max mem: 1206
Test:  [350/559]  eta: 0:00:11  loss: 0.8135 (1.0051)  labels_encoder: 0.8135 (1.0051)  labels_encoder_unscaled: 0.8135 (1.0051)  time: 0.0518  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:08  loss: 0.3433 (0.9803)  labels_encoder: 0.3433 (0.9803)  labels_encoder_unscaled: 0.3433 (0.9803)  time: 0.0468  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:05  loss: 0.4037 (0.9555)  labels_encoder: 0.4037 (0.9555)  labels_encoder_unscaled: 0.4037 (0.9555)  time: 0.0475  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6137 (0.9374)  labels_encoder: 0.6137 (0.9374)  labels_encoder_unscaled: 0.6137 (0.9374)  time: 0.0471  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7165 (0.9130)  labels_encoder: 0.7165 (0.9130)  labels_encoder_unscaled: 0.7165 (0.9130)  time: 0.0441  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3472 (0.9064)  labels_encoder: 0.3472 (0.9064)  labels_encoder_unscaled: 0.3472 (0.9064)  time: 0.0371  data: 0.0001  max mem: 1206
Test: Total time: 0:00:28 (0.0518 s / it)
Averaged stats: loss: 0.3472 (0.9064)  labels_encoder: 0.3472 (0.9064)  labels_encoder_unscaled: 0.3472 (0.9064)
(21, 71496)
(21, 71496)
[Epoch-3] [IDU-tvseries_anet_features.pickle] mAP: 0.1289, mcAP: 0.8796

BaseballPitch: 0.0289
BasketballDunk: 0.1217
Billiards: 0.0044
CleanAndJerk: 0.3873
CliffDiving: 0.3919
CricketBowling: 0.0576
CricketShot: 0.0959
Diving: 0.0055
FrisbeeCatch: 0.1362
GolfSwing: 0.0575
HammerThrow: 0.1065
HighJump: 0.0563
JavelinThrow: 0.0574
LongJump: 0.3811
PoleVault: 0.1063
Shotput: 0.1530
SoccerPenalty: 0.0499
TennisSwing: 0.1747
ThrowDiscus: 0.0558
VolleyballSpiking: 0.1509
Epoch: [4]  [   0/1510]  eta: 0:49:29  lr: 0.000000  loss: 0.6371 (0.6371)  labels_encoder: 0.6371 (0.6371)  labels_encoder_unscaled: 0.6371 (0.6371)  time: 1.9666  data: 1.7948  max mem: 1206
Epoch: [4]  [  50/1510]  eta: 0:04:10  lr: 0.000000  loss: 0.5274 (0.5041)  labels_encoder: 0.5274 (0.5041)  labels_encoder_unscaled: 0.5274 (0.5041)  time: 0.1303  data: 0.0002  max mem: 1206
Epoch: [4]  [ 100/1510]  eta: 0:03:33  lr: 0.000000  loss: 0.5461 (0.5102)  labels_encoder: 0.5461 (0.5102)  labels_encoder_unscaled: 0.5461 (0.5102)  time: 0.1308  data: 0.0002  max mem: 1206
Epoch: [4]  [ 150/1510]  eta: 0:03:16  lr: 0.000000  loss: 0.5025 (0.5106)  labels_encoder: 0.5025 (0.5106)  labels_encoder_unscaled: 0.5025 (0.5106)  time: 0.1300  data: 0.0002  max mem: 1206
Epoch: [4]  [ 200/1510]  eta: 0:03:04  lr: 0.000000  loss: 0.4454 (0.5094)  labels_encoder: 0.4454 (0.5094)  labels_encoder_unscaled: 0.4454 (0.5094)  time: 0.1325  data: 0.0002  max mem: 1206
Epoch: [4]  [ 250/1510]  eta: 0:02:55  lr: 0.000000  loss: 0.5015 (0.5139)  labels_encoder: 0.5015 (0.5139)  labels_encoder_unscaled: 0.5015 (0.5139)  time: 0.1301  data: 0.0002  max mem: 1206
Epoch: [4]  [ 300/1510]  eta: 0:02:46  lr: 0.000000  loss: 0.4578 (0.5054)  labels_encoder: 0.4578 (0.5054)  labels_encoder_unscaled: 0.4578 (0.5054)  time: 0.1342  data: 0.0002  max mem: 1206
Epoch: [4]  [ 350/1510]  eta: 0:02:40  lr: 0.000000  loss: 0.4583 (0.5042)  labels_encoder: 0.4583 (0.5042)  labels_encoder_unscaled: 0.4583 (0.5042)  time: 0.1410  data: 0.0002  max mem: 1206
Epoch: [4]  [ 400/1510]  eta: 0:02:34  lr: 0.000000  loss: 0.4821 (0.5036)  labels_encoder: 0.4821 (0.5036)  labels_encoder_unscaled: 0.4821 (0.5036)  time: 0.1422  data: 0.0002  max mem: 1206
Epoch: [4]  [ 450/1510]  eta: 0:02:27  lr: 0.000000  loss: 0.4718 (0.5043)  labels_encoder: 0.4718 (0.5043)  labels_encoder_unscaled: 0.4718 (0.5043)  time: 0.1399  data: 0.0002  max mem: 1206
Epoch: [4]  [ 500/1510]  eta: 0:02:20  lr: 0.000000  loss: 0.5207 (0.5043)  labels_encoder: 0.5207 (0.5043)  labels_encoder_unscaled: 0.5207 (0.5043)  time: 0.1358  data: 0.0002  max mem: 1206
Epoch: [4]  [ 550/1510]  eta: 0:02:13  lr: 0.000000  loss: 0.5136 (0.5034)  labels_encoder: 0.5136 (0.5034)  labels_encoder_unscaled: 0.5136 (0.5034)  time: 0.1322  data: 0.0002  max mem: 1206
Epoch: [4]  [ 600/1510]  eta: 0:02:06  lr: 0.000000  loss: 0.4560 (0.5034)  labels_encoder: 0.4560 (0.5034)  labels_encoder_unscaled: 0.4560 (0.5034)  time: 0.1429  data: 0.0002  max mem: 1206
Epoch: [4]  [ 650/1510]  eta: 0:01:58  lr: 0.000000  loss: 0.4610 (0.5032)  labels_encoder: 0.4610 (0.5032)  labels_encoder_unscaled: 0.4610 (0.5032)  time: 0.1212  data: 0.0002  max mem: 1206
Epoch: [4]  [ 700/1510]  eta: 0:01:51  lr: 0.000000  loss: 0.4897 (0.5045)  labels_encoder: 0.4897 (0.5045)  labels_encoder_unscaled: 0.4897 (0.5045)  time: 0.1240  data: 0.0002  max mem: 1206
Epoch: [4]  [ 750/1510]  eta: 0:01:43  lr: 0.000000  loss: 0.5135 (0.5035)  labels_encoder: 0.5135 (0.5035)  labels_encoder_unscaled: 0.5135 (0.5035)  time: 0.1212  data: 0.0002  max mem: 1206
Epoch: [4]  [ 800/1510]  eta: 0:01:36  lr: 0.000000  loss: 0.4963 (0.5048)  labels_encoder: 0.4963 (0.5048)  labels_encoder_unscaled: 0.4963 (0.5048)  time: 0.1288  data: 0.0001  max mem: 1206
Epoch: [4]  [ 850/1510]  eta: 0:01:29  lr: 0.000000  loss: 0.5038 (0.5053)  labels_encoder: 0.5038 (0.5053)  labels_encoder_unscaled: 0.5038 (0.5053)  time: 0.1209  data: 0.0002  max mem: 1206
Epoch: [4]  [ 900/1510]  eta: 0:01:21  lr: 0.000000  loss: 0.5433 (0.5060)  labels_encoder: 0.5433 (0.5060)  labels_encoder_unscaled: 0.5433 (0.5060)  time: 0.1171  data: 0.0002  max mem: 1206
Epoch: [4]  [ 950/1510]  eta: 0:01:14  lr: 0.000000  loss: 0.4626 (0.5054)  labels_encoder: 0.4626 (0.5054)  labels_encoder_unscaled: 0.4626 (0.5054)  time: 0.1187  data: 0.0002  max mem: 1206
Epoch: [4]  [1000/1510]  eta: 0:01:07  lr: 0.000000  loss: 0.5153 (0.5069)  labels_encoder: 0.5153 (0.5069)  labels_encoder_unscaled: 0.5153 (0.5069)  time: 0.1201  data: 0.0002  max mem: 1206
Epoch: [4]  [1050/1510]  eta: 0:01:00  lr: 0.000000  loss: 0.4859 (0.5074)  labels_encoder: 0.4859 (0.5074)  labels_encoder_unscaled: 0.4859 (0.5074)  time: 0.1212  data: 0.0002  max mem: 1206
Epoch: [4]  [1100/1510]  eta: 0:00:54  lr: 0.000000  loss: 0.5050 (0.5079)  labels_encoder: 0.5050 (0.5079)  labels_encoder_unscaled: 0.5050 (0.5079)  time: 0.1317  data: 0.0002  max mem: 1206
Epoch: [4]  [1150/1510]  eta: 0:00:47  lr: 0.000000  loss: 0.4723 (0.5073)  labels_encoder: 0.4723 (0.5073)  labels_encoder_unscaled: 0.4723 (0.5073)  time: 0.1271  data: 0.0002  max mem: 1206
Epoch: [4]  [1200/1510]  eta: 0:00:40  lr: 0.000000  loss: 0.5281 (0.5068)  labels_encoder: 0.5281 (0.5068)  labels_encoder_unscaled: 0.5281 (0.5068)  time: 0.1288  data: 0.0002  max mem: 1206
Epoch: [4]  [1250/1510]  eta: 0:00:34  lr: 0.000000  loss: 0.5171 (0.5066)  labels_encoder: 0.5171 (0.5066)  labels_encoder_unscaled: 0.5171 (0.5066)  time: 0.1305  data: 0.0001  max mem: 1206
Epoch: [4]  [1300/1510]  eta: 0:00:27  lr: 0.000000  loss: 0.5218 (0.5061)  labels_encoder: 0.5218 (0.5061)  labels_encoder_unscaled: 0.5218 (0.5061)  time: 0.1280  data: 0.0002  max mem: 1206
Epoch: [4]  [1350/1510]  eta: 0:00:20  lr: 0.000000  loss: 0.5252 (0.5064)  labels_encoder: 0.5252 (0.5064)  labels_encoder_unscaled: 0.5252 (0.5064)  time: 0.1260  data: 0.0002  max mem: 1206
Epoch: [4]  [1400/1510]  eta: 0:00:14  lr: 0.000000  loss: 0.4851 (0.5068)  labels_encoder: 0.4851 (0.5068)  labels_encoder_unscaled: 0.4851 (0.5068)  time: 0.1246  data: 0.0001  max mem: 1206
Epoch: [4]  [1450/1510]  eta: 0:00:07  lr: 0.000000  loss: 0.5393 (0.5077)  labels_encoder: 0.5393 (0.5077)  labels_encoder_unscaled: 0.5393 (0.5077)  time: 0.1303  data: 0.0001  max mem: 1206
Epoch: [4]  [1500/1510]  eta: 0:00:01  lr: 0.000000  loss: 0.5124 (0.5072)  labels_encoder: 0.5124 (0.5072)  labels_encoder_unscaled: 0.5124 (0.5072)  time: 0.1265  data: 0.0003  max mem: 1206
Epoch: [4]  [1509/1510]  eta: 0:00:00  lr: 0.000000  loss: 0.5108 (0.5074)  labels_encoder: 0.5108 (0.5074)  labels_encoder_unscaled: 0.5108 (0.5074)  time: 0.1122  data: 0.0002  max mem: 1206
Epoch: [4] Total time: 0:03:17 (0.1311 s / it)
Averaged stats: lr: 0.000000  loss: 0.5108 (0.5074)  labels_encoder: 0.5108 (0.5074)  labels_encoder_unscaled: 0.5108 (0.5074)
Test:  [  0/559]  eta: 0:17:20  loss: 0.6285 (0.6285)  labels_encoder: 0.6285 (0.6285)  labels_encoder_unscaled: 0.6285 (0.6285)  time: 1.8617  data: 1.8109  max mem: 1206
Test:  [ 50/559]  eta: 0:00:46  loss: 1.0547 (0.8965)  labels_encoder: 1.0547 (0.8965)  labels_encoder_unscaled: 1.0547 (0.8965)  time: 0.0534  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:32  loss: 0.5643 (0.8594)  labels_encoder: 0.5643 (0.8594)  labels_encoder_unscaled: 0.5643 (0.8594)  time: 0.0474  data: 0.0001  max mem: 1206
Test:  [150/559]  eta: 0:00:25  loss: 0.4888 (0.8815)  labels_encoder: 0.4888 (0.8815)  labels_encoder_unscaled: 0.4888 (0.8815)  time: 0.0475  data: 0.0001  max mem: 1206
Test:  [200/559]  eta: 0:00:21  loss: 0.7664 (0.8223)  labels_encoder: 0.7664 (0.8223)  labels_encoder_unscaled: 0.7664 (0.8223)  time: 0.0561  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:18  loss: 0.7639 (0.9396)  labels_encoder: 0.7639 (0.9396)  labels_encoder_unscaled: 0.7639 (0.9396)  time: 0.0568  data: 0.0148  max mem: 1206
Test:  [300/559]  eta: 0:00:15  loss: 1.0969 (0.9944)  labels_encoder: 1.0969 (0.9944)  labels_encoder_unscaled: 1.0969 (0.9944)  time: 0.0475  data: 0.0001  max mem: 1206
Test:  [350/559]  eta: 0:00:11  loss: 0.7912 (1.0154)  labels_encoder: 0.7912 (1.0154)  labels_encoder_unscaled: 0.7912 (1.0154)  time: 0.0486  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:08  loss: 0.3461 (0.9892)  labels_encoder: 0.3461 (0.9892)  labels_encoder_unscaled: 0.3461 (0.9892)  time: 0.0475  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:05  loss: 0.4110 (0.9636)  labels_encoder: 0.4110 (0.9636)  labels_encoder_unscaled: 0.4110 (0.9636)  time: 0.0475  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6190 (0.9436)  labels_encoder: 0.6190 (0.9436)  labels_encoder_unscaled: 0.6190 (0.9436)  time: 0.0485  data: 0.0001  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7249 (0.9189)  labels_encoder: 0.7249 (0.9189)  labels_encoder_unscaled: 0.7249 (0.9189)  time: 0.0454  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3644 (0.9122)  labels_encoder: 0.3644 (0.9122)  labels_encoder_unscaled: 0.3644 (0.9122)  time: 0.0426  data: 0.0001  max mem: 1206
Test: Total time: 0:00:29 (0.0534 s / it)
Averaged stats: loss: 0.3644 (0.9122)  labels_encoder: 0.3644 (0.9122)  labels_encoder_unscaled: 0.3644 (0.9122)
(21, 71496)
(21, 71496)
[Epoch-4] [IDU-tvseries_anet_features.pickle] mAP: 0.1247, mcAP: 0.8767

BaseballPitch: 0.0347
BasketballDunk: 0.1374
Billiards: 0.0044
CleanAndJerk: 0.4013
CliffDiving: 0.4209
CricketBowling: 0.0578
CricketShot: 0.0812
Diving: 0.0061
FrisbeeCatch: 0.1208
GolfSwing: 0.0567
HammerThrow: 0.0837
HighJump: 0.0330
JavelinThrow: 0.0525
LongJump: 0.3488
PoleVault: 0.1193
Shotput: 0.1544
SoccerPenalty: 0.0483
TennisSwing: 0.1394
ThrowDiscus: 0.0481
VolleyballSpiking: 0.1462
Training time 0:15:27
