Unable to compile CoConv C++ implementation. Falling back to Python version.
[Errno 2] No such file or directory: '/home/lh/.conda/envs/oadtr/lib/python3.8/site-packages/continual/conv.cpp'
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'conv_flops_counter_hook'
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'MODULES_MAPPING'
Not using distributed mode
lr:0.0001
batch_size:128
weight_decay:0.0001
epochs:5
resize_feature:False
lr_drop:1
clip_max_norm:1.0
dataparallel:False
removelog:False
version:v3
query_num:8
decoder_layers:5
decoder_embedding_dim:1024
decoder_embedding_dim_out:1024
decoder_attn_dropout_rate:0.1
decoder_num_heads:4
classification_pred_loss_coef:0.5
enc_layers:64
lr_backbone:0.0001
feature:tvseries_anet_features.pickle
dim_feature:4096
patch_dim:1
embedding_dim:1024
num_heads:8
num_layers:2
attn_dropout_rate:0.1
positional_encoding_type:recycling_fixed
num_embeddings:127
hidden_dim:1024
dropout_rate:0.1
numclass:31
classification_x_loss_coef:0.3
classification_h_loss_coef:1
similar_loss_coef:0.1
margin:1.0
dataset:tvseries
dataset_file:data/data_info_new.json
frozen_weights:None
thumos_data_path:/home/dancer/mycode/Temporal.Online.Detection/Online.TRN.Pytorch/preprocess/
thumos_anno_path:data/thumos_{}_anno.pickle
remove_difficult:False
device:cuda
output_dir:models
seed:4
resume:
start_epoch:1
eval:False
num_workers:8
world_size:1
dist_url:tcp://127.0.0.1:12342
train_session_set:['24_ep1', '24_ep2', '24_ep3', 'Breaking_Bad_ep1', 'Breaking_Bad_ep2', 'How_I_Met_Your_Mother_ep1', 'How_I_Met_Your_Mother_ep2', 'How_I_Met_Your_Mother_ep3', 'How_I_Met_Your_Mother_ep4', 'How_I_Met_Your_Mother_ep5', 'How_I_Met_Your_Mother_ep6', 'Mad_Men_ep1', 'Mad_Men_ep2', 'Modern_Family_ep1', 'Modern_Family_ep2', 'Modern_Family_ep3', 'Modern_Family_ep4', 'Modern_Family_ep6', 'Sons_of_Anarchy_ep1', 'Sons_of_Anarchy_ep2']
test_session_set:['24_ep4', 'Breaking_Bad_ep3', 'Mad_Men_ep3', 'How_I_Met_Your_Mother_ep7', 'How_I_Met_Your_Mother_ep8', 'Modern_Family_ep5', 'Sons_of_Anarchy_ep3']
class_index:['background', 'Pick something up', 'Point', 'Drink', 'Stand up', 'Run', 'Sit down', 'Read', 'Smoke', 'Drive car', 'Open door', 'Give something', 'Use computer', 'Write', 'Go down stairway', 'Close door', 'Throw something', 'Go up stairway', 'Get in/out of car', 'Hang up phone', 'Eat', 'Answer phone', 'Dress up', 'Clap', 'Undress', 'Kiss', 'Fall/trip', 'Wave', 'Pour', 'Punch', 'Fire weapon']
distributed:False
position encoding : recycling_fixed
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Sequential(
  16.814 M, 99.939% Params, 0.412 GMac, 100.000% MACs, 
  (0): Linear(4.195 M, 24.936% Params, 0.004 GMac, 1.018% MACs, in_features=4096, out_features=1024, bias=True, channel_dim=1)
  (1): RecyclingPositionalEncoding(
    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 
    (pe): CyclicPositionalEncoding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
  )
  (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
  (3): Sequential(
    12.587 M, 74.814% Params, 0.408 GMac, 98.974% MACs, 
    (0): Sequential(
      6.294 M, 37.407% Params, 0.205 GMac, 49.804% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, reduce=reduce_sum
        (0): RetroactiveUnity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 63)
        (1): CoReMultiheadAttention(
          4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(Sequential(
        2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
        (0): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
        (1): Residual(
          2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
          (fn): Sequential(
            2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
            (0): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
            (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
            (3): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          )
        )
        (2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
      ))
    )
    (1): Lambda(Sequential(
      6.294 M, 37.407% Params, 0.203 GMac, 49.170% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, reduce=sum_last_pairs
        (0): SelectOrDelay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): CoSiMultiheadAttention(
          4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
      (2): BroadcastReduce(
        2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, reduce=reduce_sum
        (0): Delay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): Sequential(
          2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, 
          (0): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
          (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          (3): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
        )
      )
      (3): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
    ), takes_time=True)
    (2): Lambda(unity, squeeze_last, squeeze_last, takes_time=True)
  )
  (4): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
  (5): Linear(0.032 M, 0.189% Params, 0.0 GMac, 0.008% MACs, in_features=1024, out_features=31, bias=True, channel_dim=1)
)
Model FLOPs: 411997217.0
Model params: 16824351
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| Active memory         |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   83968 KB |   83968 KB |   83968 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16445 KB |   16445 KB |  309356 KB |  292910 KB |
|---------------------------------------------------------------------------|
| Allocations           |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| Active allocs         |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      13    |    1416    |    1406    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Memory state pre, max, post inference: 69143040 71518720 69143040
Loaded tvseries_anet_features.pickle
Loaded tvseries_anet_features.pickle
Start training
Epoch: [1]  [   0/1509]  eta: 1:20:49  lr: 0.000100  loss: 3.3572 (3.3572)  labels_encoder: 3.3572 (3.3572)  labels_encoder_unscaled: 3.3572 (3.3572)  time: 3.2140  data: 2.5725  max mem: 1013
Epoch: [1]  [  50/1509]  eta: 0:04:35  lr: 0.000100  loss: 1.0381 (1.1102)  labels_encoder: 1.0381 (1.1102)  labels_encoder_unscaled: 1.0381 (1.1102)  time: 0.1205  data: 0.0002  max mem: 1206
Epoch: [1]  [ 100/1509]  eta: 0:03:42  lr: 0.000100  loss: 0.9421 (1.0418)  labels_encoder: 0.9421 (1.0418)  labels_encoder_unscaled: 0.9421 (1.0418)  time: 0.1310  data: 0.0002  max mem: 1206
Epoch: [1]  [ 150/1509]  eta: 0:03:22  lr: 0.000100  loss: 0.9250 (1.0223)  labels_encoder: 0.9250 (1.0223)  labels_encoder_unscaled: 0.9250 (1.0223)  time: 0.1326  data: 0.0002  max mem: 1206
Epoch: [1]  [ 200/1509]  eta: 0:03:08  lr: 0.000100  loss: 0.9959 (1.0004)  labels_encoder: 0.9959 (1.0004)  labels_encoder_unscaled: 0.9959 (1.0004)  time: 0.1221  data: 0.0002  max mem: 1206
Epoch: [1]  [ 250/1509]  eta: 0:02:53  lr: 0.000100  loss: 0.9227 (0.9866)  labels_encoder: 0.9227 (0.9866)  labels_encoder_unscaled: 0.9227 (0.9866)  time: 0.1155  data: 0.0002  max mem: 1206
Epoch: [1]  [ 300/1509]  eta: 0:02:42  lr: 0.000100  loss: 0.7871 (0.9620)  labels_encoder: 0.7871 (0.9620)  labels_encoder_unscaled: 0.7871 (0.9620)  time: 0.1188  data: 0.0002  max mem: 1206
Epoch: [1]  [ 350/1509]  eta: 0:02:34  lr: 0.000100  loss: 0.8594 (0.9424)  labels_encoder: 0.8594 (0.9424)  labels_encoder_unscaled: 0.8594 (0.9424)  time: 0.1289  data: 0.0002  max mem: 1206
Epoch: [1]  [ 400/1509]  eta: 0:02:27  lr: 0.000100  loss: 0.7864 (0.9261)  labels_encoder: 0.7864 (0.9261)  labels_encoder_unscaled: 0.7864 (0.9261)  time: 0.1287  data: 0.0002  max mem: 1206
Epoch: [1]  [ 450/1509]  eta: 0:02:19  lr: 0.000100  loss: 0.8328 (0.9155)  labels_encoder: 0.8328 (0.9155)  labels_encoder_unscaled: 0.8328 (0.9155)  time: 0.1213  data: 0.0002  max mem: 1206
Epoch: [1]  [ 500/1509]  eta: 0:02:11  lr: 0.000100  loss: 0.7497 (0.8991)  labels_encoder: 0.7497 (0.8991)  labels_encoder_unscaled: 0.7497 (0.8991)  time: 0.1297  data: 0.0002  max mem: 1206
Epoch: [1]  [ 550/1509]  eta: 0:02:06  lr: 0.000100  loss: 0.7437 (0.8879)  labels_encoder: 0.7437 (0.8879)  labels_encoder_unscaled: 0.7437 (0.8879)  time: 0.1399  data: 0.0002  max mem: 1206
Epoch: [1]  [ 600/1509]  eta: 0:02:00  lr: 0.000100  loss: 0.8043 (0.8769)  labels_encoder: 0.8043 (0.8769)  labels_encoder_unscaled: 0.8043 (0.8769)  time: 0.1373  data: 0.0002  max mem: 1206
Epoch: [1]  [ 650/1509]  eta: 0:01:52  lr: 0.000100  loss: 0.7771 (0.8702)  labels_encoder: 0.7771 (0.8702)  labels_encoder_unscaled: 0.7771 (0.8702)  time: 0.1253  data: 0.0002  max mem: 1206
Epoch: [1]  [ 700/1509]  eta: 0:01:45  lr: 0.000100  loss: 0.7494 (0.8617)  labels_encoder: 0.7494 (0.8617)  labels_encoder_unscaled: 0.7494 (0.8617)  time: 0.1143  data: 0.0002  max mem: 1206
Epoch: [1]  [ 750/1509]  eta: 0:01:38  lr: 0.000100  loss: 0.6969 (0.8523)  labels_encoder: 0.6969 (0.8523)  labels_encoder_unscaled: 0.6969 (0.8523)  time: 0.1269  data: 0.0002  max mem: 1206
Epoch: [1]  [ 800/1509]  eta: 0:01:31  lr: 0.000100  loss: 0.6849 (0.8440)  labels_encoder: 0.6849 (0.8440)  labels_encoder_unscaled: 0.6849 (0.8440)  time: 0.1252  data: 0.0002  max mem: 1206
Epoch: [1]  [ 850/1509]  eta: 0:01:25  lr: 0.000100  loss: 0.7781 (0.8374)  labels_encoder: 0.7781 (0.8374)  labels_encoder_unscaled: 0.7781 (0.8374)  time: 0.1227  data: 0.0002  max mem: 1206
Epoch: [1]  [ 900/1509]  eta: 0:01:18  lr: 0.000100  loss: 0.7472 (0.8320)  labels_encoder: 0.7472 (0.8320)  labels_encoder_unscaled: 0.7472 (0.8320)  time: 0.1213  data: 0.0002  max mem: 1206
Epoch: [1]  [ 950/1509]  eta: 0:01:11  lr: 0.000100  loss: 0.6763 (0.8262)  labels_encoder: 0.6763 (0.8262)  labels_encoder_unscaled: 0.6763 (0.8262)  time: 0.1229  data: 0.0002  max mem: 1206
Epoch: [1]  [1000/1509]  eta: 0:01:05  lr: 0.000100  loss: 0.7092 (0.8198)  labels_encoder: 0.7092 (0.8198)  labels_encoder_unscaled: 0.7092 (0.8198)  time: 0.1148  data: 0.0002  max mem: 1206
Epoch: [1]  [1050/1509]  eta: 0:00:58  lr: 0.000100  loss: 0.6702 (0.8156)  labels_encoder: 0.6702 (0.8156)  labels_encoder_unscaled: 0.6702 (0.8156)  time: 0.1169  data: 0.0002  max mem: 1206
Epoch: [1]  [1100/1509]  eta: 0:00:52  lr: 0.000100  loss: 0.6770 (0.8098)  labels_encoder: 0.6770 (0.8098)  labels_encoder_unscaled: 0.6770 (0.8098)  time: 0.1213  data: 0.0002  max mem: 1206
Epoch: [1]  [1150/1509]  eta: 0:00:45  lr: 0.000100  loss: 0.7028 (0.8062)  labels_encoder: 0.7028 (0.8062)  labels_encoder_unscaled: 0.7028 (0.8062)  time: 0.1340  data: 0.0002  max mem: 1206
Epoch: [1]  [1200/1509]  eta: 0:00:39  lr: 0.000100  loss: 0.6764 (0.8008)  labels_encoder: 0.6764 (0.8008)  labels_encoder_unscaled: 0.6764 (0.8008)  time: 0.1401  data: 0.0002  max mem: 1206
Epoch: [1]  [1250/1509]  eta: 0:00:33  lr: 0.000100  loss: 0.6731 (0.7968)  labels_encoder: 0.6731 (0.7968)  labels_encoder_unscaled: 0.6731 (0.7968)  time: 0.1351  data: 0.0002  max mem: 1206
Epoch: [1]  [1300/1509]  eta: 0:00:26  lr: 0.000100  loss: 0.6222 (0.7938)  labels_encoder: 0.6222 (0.7938)  labels_encoder_unscaled: 0.6222 (0.7938)  time: 0.1365  data: 0.0002  max mem: 1206
Epoch: [1]  [1350/1509]  eta: 0:00:20  lr: 0.000100  loss: 0.6487 (0.7891)  labels_encoder: 0.6487 (0.7891)  labels_encoder_unscaled: 0.6487 (0.7891)  time: 0.1330  data: 0.0002  max mem: 1206
Epoch: [1]  [1400/1509]  eta: 0:00:14  lr: 0.000100  loss: 0.6431 (0.7845)  labels_encoder: 0.6431 (0.7845)  labels_encoder_unscaled: 0.6431 (0.7845)  time: 0.1307  data: 0.0002  max mem: 1206
Epoch: [1]  [1450/1509]  eta: 0:00:07  lr: 0.000100  loss: 0.6110 (0.7805)  labels_encoder: 0.6110 (0.7805)  labels_encoder_unscaled: 0.6110 (0.7805)  time: 0.1319  data: 0.0002  max mem: 1206
Epoch: [1]  [1500/1509]  eta: 0:00:01  lr: 0.000100  loss: 0.6468 (0.7763)  labels_encoder: 0.6468 (0.7763)  labels_encoder_unscaled: 0.6468 (0.7763)  time: 0.1293  data: 0.0004  max mem: 1206
Epoch: [1]  [1508/1509]  eta: 0:00:00  lr: 0.000100  loss: 0.6159 (0.7755)  labels_encoder: 0.6159 (0.7755)  labels_encoder_unscaled: 0.6159 (0.7755)  time: 0.1238  data: 0.0003  max mem: 1206
Epoch: [1] Total time: 0:03:14 (0.1291 s / it)
Averaged stats: lr: 0.000100  loss: 0.6159 (0.7755)  labels_encoder: 0.6159 (0.7755)  labels_encoder_unscaled: 0.6159 (0.7755)
Test:  [  0/559]  eta: 0:15:32  loss: 0.7461 (0.7461)  labels_encoder: 0.7461 (0.7461)  labels_encoder_unscaled: 0.7461 (0.7461)  time: 1.6676  data: 1.6062  max mem: 1206
Test:  [ 50/559]  eta: 0:00:46  loss: 0.8195 (0.9191)  labels_encoder: 0.8195 (0.9191)  labels_encoder_unscaled: 0.8195 (0.9191)  time: 0.0543  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:34  loss: 0.5049 (0.8610)  labels_encoder: 0.5049 (0.8610)  labels_encoder_unscaled: 0.5049 (0.8610)  time: 0.0633  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.8811 (0.9192)  labels_encoder: 0.8811 (0.9192)  labels_encoder_unscaled: 0.8811 (0.9192)  time: 0.0555  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.6409 (0.8729)  labels_encoder: 0.6409 (0.8729)  labels_encoder_unscaled: 0.6409 (0.8729)  time: 0.0569  data: 0.0010  max mem: 1206
Test:  [250/559]  eta: 0:00:19  loss: 0.8544 (0.9909)  labels_encoder: 0.8544 (0.9909)  labels_encoder_unscaled: 0.8544 (0.9909)  time: 0.0477  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:15  loss: 1.2911 (1.0549)  labels_encoder: 1.2911 (1.0549)  labels_encoder_unscaled: 1.2911 (1.0549)  time: 0.0510  data: 0.0001  max mem: 1206
Test:  [350/559]  eta: 0:00:12  loss: 0.8288 (1.0748)  labels_encoder: 0.8288 (1.0748)  labels_encoder_unscaled: 0.8288 (1.0748)  time: 0.0476  data: 0.0001  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.4134 (1.0500)  labels_encoder: 0.4134 (1.0500)  labels_encoder_unscaled: 0.4134 (1.0500)  time: 0.0474  data: 0.0001  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4696 (1.0169)  labels_encoder: 0.4696 (1.0169)  labels_encoder_unscaled: 0.4696 (1.0169)  time: 0.0469  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5897 (0.9923)  labels_encoder: 0.5897 (0.9923)  labels_encoder_unscaled: 0.5897 (0.9923)  time: 0.0575  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7806 (0.9640)  labels_encoder: 0.7806 (0.9640)  labels_encoder_unscaled: 0.7806 (0.9640)  time: 0.0438  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3933 (0.9570)  labels_encoder: 0.3933 (0.9570)  labels_encoder_unscaled: 0.3933 (0.9570)  time: 0.0358  data: 0.0001  max mem: 1206
Test: Total time: 0:00:31 (0.0560 s / it)
Averaged stats: loss: 0.3933 (0.9570)  labels_encoder: 0.3933 (0.9570)  labels_encoder_unscaled: 0.3933 (0.9570)
(21, 71496)
(21, 71496)
[Epoch-1] [IDU-tvseries_anet_features.pickle] mAP: 0.1121, mcAP: 0.8690

BaseballPitch: 0.0463
BasketballDunk: 0.0775
Billiards: 0.0043
CleanAndJerk: 0.3588
CliffDiving: 0.4014
CricketBowling: 0.0806
CricketShot: 0.0765
Diving: 0.0038
FrisbeeCatch: 0.1225
GolfSwing: 0.0763
HammerThrow: 0.0773
HighJump: 0.0382
JavelinThrow: 0.0853
LongJump: 0.1882
PoleVault: 0.0957
Shotput: 0.0982
SoccerPenalty: 0.0483
TennisSwing: 0.1852
ThrowDiscus: 0.0189
VolleyballSpiking: 0.1584
Epoch: [2]  [   0/1509]  eta: 0:54:20  lr: 0.000010  loss: 0.8423 (0.8423)  labels_encoder: 0.8423 (0.8423)  labels_encoder_unscaled: 0.8423 (0.8423)  time: 2.1604  data: 1.9794  max mem: 1206
Epoch: [2]  [  50/1509]  eta: 0:04:23  lr: 0.000010  loss: 0.6103 (0.6387)  labels_encoder: 0.6103 (0.6387)  labels_encoder_unscaled: 0.6103 (0.6387)  time: 0.1382  data: 0.0002  max mem: 1206
Epoch: [2]  [ 100/1509]  eta: 0:03:44  lr: 0.000010  loss: 0.6060 (0.6241)  labels_encoder: 0.6060 (0.6241)  labels_encoder_unscaled: 0.6060 (0.6241)  time: 0.1398  data: 0.0002  max mem: 1206
Epoch: [2]  [ 150/1509]  eta: 0:03:28  lr: 0.000010  loss: 0.6251 (0.6260)  labels_encoder: 0.6251 (0.6260)  labels_encoder_unscaled: 0.6251 (0.6260)  time: 0.1393  data: 0.0002  max mem: 1206
Epoch: [2]  [ 200/1509]  eta: 0:03:16  lr: 0.000010  loss: 0.5792 (0.6196)  labels_encoder: 0.5792 (0.6196)  labels_encoder_unscaled: 0.5792 (0.6196)  time: 0.1389  data: 0.0002  max mem: 1206
Epoch: [2]  [ 250/1509]  eta: 0:03:06  lr: 0.000010  loss: 0.5727 (0.6149)  labels_encoder: 0.5727 (0.6149)  labels_encoder_unscaled: 0.5727 (0.6149)  time: 0.1431  data: 0.0002  max mem: 1206
Epoch: [2]  [ 300/1509]  eta: 0:02:57  lr: 0.000010  loss: 0.6318 (0.6108)  labels_encoder: 0.6318 (0.6108)  labels_encoder_unscaled: 0.6318 (0.6108)  time: 0.1405  data: 0.0002  max mem: 1206
Epoch: [2]  [ 350/1509]  eta: 0:02:49  lr: 0.000010  loss: 0.5739 (0.6119)  labels_encoder: 0.5739 (0.6119)  labels_encoder_unscaled: 0.5739 (0.6119)  time: 0.1406  data: 0.0002  max mem: 1206
Epoch: [2]  [ 400/1509]  eta: 0:02:40  lr: 0.000010  loss: 0.5576 (0.6066)  labels_encoder: 0.5576 (0.6066)  labels_encoder_unscaled: 0.5576 (0.6066)  time: 0.1378  data: 0.0002  max mem: 1206
Epoch: [2]  [ 450/1509]  eta: 0:02:32  lr: 0.000010  loss: 0.5644 (0.6048)  labels_encoder: 0.5644 (0.6048)  labels_encoder_unscaled: 0.5644 (0.6048)  time: 0.1389  data: 0.0002  max mem: 1206
Epoch: [2]  [ 500/1509]  eta: 0:02:25  lr: 0.000010  loss: 0.5947 (0.6047)  labels_encoder: 0.5947 (0.6047)  labels_encoder_unscaled: 0.5947 (0.6047)  time: 0.1377  data: 0.0002  max mem: 1206
Epoch: [2]  [ 550/1509]  eta: 0:02:17  lr: 0.000010  loss: 0.5731 (0.6044)  labels_encoder: 0.5731 (0.6044)  labels_encoder_unscaled: 0.5731 (0.6044)  time: 0.1450  data: 0.0002  max mem: 1206
Epoch: [2]  [ 600/1509]  eta: 0:02:10  lr: 0.000010  loss: 0.5501 (0.6025)  labels_encoder: 0.5501 (0.6025)  labels_encoder_unscaled: 0.5501 (0.6025)  time: 0.1409  data: 0.0002  max mem: 1206
Epoch: [2]  [ 650/1509]  eta: 0:02:02  lr: 0.000010  loss: 0.5952 (0.6016)  labels_encoder: 0.5952 (0.6016)  labels_encoder_unscaled: 0.5952 (0.6016)  time: 0.1418  data: 0.0002  max mem: 1206
Epoch: [2]  [ 700/1509]  eta: 0:01:55  lr: 0.000010  loss: 0.5310 (0.5994)  labels_encoder: 0.5310 (0.5994)  labels_encoder_unscaled: 0.5310 (0.5994)  time: 0.1420  data: 0.0002  max mem: 1206
Epoch: [2]  [ 750/1509]  eta: 0:01:48  lr: 0.000010  loss: 0.5769 (0.5985)  labels_encoder: 0.5769 (0.5985)  labels_encoder_unscaled: 0.5769 (0.5985)  time: 0.1336  data: 0.0002  max mem: 1206
Epoch: [2]  [ 800/1509]  eta: 0:01:40  lr: 0.000010  loss: 0.5707 (0.5967)  labels_encoder: 0.5707 (0.5967)  labels_encoder_unscaled: 0.5707 (0.5967)  time: 0.1357  data: 0.0002  max mem: 1206
Epoch: [2]  [ 850/1509]  eta: 0:01:33  lr: 0.000010  loss: 0.5653 (0.5959)  labels_encoder: 0.5653 (0.5959)  labels_encoder_unscaled: 0.5653 (0.5959)  time: 0.1361  data: 0.0002  max mem: 1206
Epoch: [2]  [ 900/1509]  eta: 0:01:26  lr: 0.000010  loss: 0.5491 (0.5955)  labels_encoder: 0.5491 (0.5955)  labels_encoder_unscaled: 0.5491 (0.5955)  time: 0.1406  data: 0.0002  max mem: 1206
Epoch: [2]  [ 950/1509]  eta: 0:01:19  lr: 0.000010  loss: 0.5705 (0.5926)  labels_encoder: 0.5705 (0.5926)  labels_encoder_unscaled: 0.5705 (0.5926)  time: 0.1431  data: 0.0002  max mem: 1206
Epoch: [2]  [1000/1509]  eta: 0:01:12  lr: 0.000010  loss: 0.5064 (0.5904)  labels_encoder: 0.5064 (0.5904)  labels_encoder_unscaled: 0.5064 (0.5904)  time: 0.1410  data: 0.0002  max mem: 1206
Epoch: [2]  [1050/1509]  eta: 0:01:05  lr: 0.000010  loss: 0.5668 (0.5894)  labels_encoder: 0.5668 (0.5894)  labels_encoder_unscaled: 0.5668 (0.5894)  time: 0.1380  data: 0.0002  max mem: 1206
Epoch: [2]  [1100/1509]  eta: 0:00:57  lr: 0.000010  loss: 0.5587 (0.5870)  labels_encoder: 0.5587 (0.5870)  labels_encoder_unscaled: 0.5587 (0.5870)  time: 0.1384  data: 0.0002  max mem: 1206
Epoch: [2]  [1150/1509]  eta: 0:00:50  lr: 0.000010  loss: 0.5016 (0.5860)  labels_encoder: 0.5016 (0.5860)  labels_encoder_unscaled: 0.5016 (0.5860)  time: 0.1356  data: 0.0002  max mem: 1206
Epoch: [2]  [1200/1509]  eta: 0:00:43  lr: 0.000010  loss: 0.4984 (0.5841)  labels_encoder: 0.4984 (0.5841)  labels_encoder_unscaled: 0.4984 (0.5841)  time: 0.1412  data: 0.0002  max mem: 1206
Epoch: [2]  [1250/1509]  eta: 0:00:36  lr: 0.000010  loss: 0.5520 (0.5837)  labels_encoder: 0.5520 (0.5837)  labels_encoder_unscaled: 0.5520 (0.5837)  time: 0.1438  data: 0.0002  max mem: 1206
Epoch: [2]  [1300/1509]  eta: 0:00:29  lr: 0.000010  loss: 0.5820 (0.5839)  labels_encoder: 0.5820 (0.5839)  labels_encoder_unscaled: 0.5820 (0.5839)  time: 0.1416  data: 0.0002  max mem: 1206
Epoch: [2]  [1350/1509]  eta: 0:00:22  lr: 0.000010  loss: 0.5976 (0.5829)  labels_encoder: 0.5976 (0.5829)  labels_encoder_unscaled: 0.5976 (0.5829)  time: 0.1267  data: 0.0002  max mem: 1206
Epoch: [2]  [1400/1509]  eta: 0:00:15  lr: 0.000010  loss: 0.5377 (0.5820)  labels_encoder: 0.5377 (0.5820)  labels_encoder_unscaled: 0.5377 (0.5820)  time: 0.1239  data: 0.0002  max mem: 1206
Epoch: [2]  [1450/1509]  eta: 0:00:08  lr: 0.000010  loss: 0.5076 (0.5811)  labels_encoder: 0.5076 (0.5811)  labels_encoder_unscaled: 0.5076 (0.5811)  time: 0.1269  data: 0.0002  max mem: 1206
Epoch: [2]  [1500/1509]  eta: 0:00:01  lr: 0.000010  loss: 0.5435 (0.5806)  labels_encoder: 0.5435 (0.5806)  labels_encoder_unscaled: 0.5435 (0.5806)  time: 0.1189  data: 0.0002  max mem: 1206
Epoch: [2]  [1508/1509]  eta: 0:00:00  lr: 0.000010  loss: 0.5715 (0.5806)  labels_encoder: 0.5715 (0.5806)  labels_encoder_unscaled: 0.5715 (0.5806)  time: 0.1121  data: 0.0002  max mem: 1206
Epoch: [2] Total time: 0:03:30 (0.1395 s / it)
Averaged stats: lr: 0.000010  loss: 0.5715 (0.5806)  labels_encoder: 0.5715 (0.5806)  labels_encoder_unscaled: 0.5715 (0.5806)
Test:  [  0/559]  eta: 0:16:21  loss: 0.6659 (0.6659)  labels_encoder: 0.6659 (0.6659)  labels_encoder_unscaled: 0.6659 (0.6659)  time: 1.7561  data: 1.6819  max mem: 1206
Test:  [ 50/559]  eta: 0:00:47  loss: 0.9233 (0.9672)  labels_encoder: 0.9233 (0.9672)  labels_encoder_unscaled: 0.9233 (0.9672)  time: 0.0621  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:35  loss: 0.7322 (0.9223)  labels_encoder: 0.7322 (0.9223)  labels_encoder_unscaled: 0.7322 (0.9223)  time: 0.0600  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:29  loss: 0.7750 (0.9444)  labels_encoder: 0.7750 (0.9444)  labels_encoder_unscaled: 0.7750 (0.9444)  time: 0.0643  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.4905 (0.8815)  labels_encoder: 0.4905 (0.8815)  labels_encoder_unscaled: 0.4905 (0.8815)  time: 0.0504  data: 0.0001  max mem: 1206
Test:  [250/559]  eta: 0:00:19  loss: 0.7214 (0.9745)  labels_encoder: 0.7214 (0.9745)  labels_encoder_unscaled: 0.7214 (0.9745)  time: 0.0486  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:15  loss: 1.0352 (1.0124)  labels_encoder: 1.0352 (1.0124)  labels_encoder_unscaled: 1.0352 (1.0124)  time: 0.0469  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:12  loss: 0.7994 (1.0339)  labels_encoder: 0.7994 (1.0339)  labels_encoder_unscaled: 0.7994 (1.0339)  time: 0.0484  data: 0.0001  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.4493 (1.0084)  labels_encoder: 0.4493 (1.0084)  labels_encoder_unscaled: 0.4493 (1.0084)  time: 0.0476  data: 0.0001  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4617 (0.9818)  labels_encoder: 0.4617 (0.9818)  labels_encoder_unscaled: 0.4617 (0.9818)  time: 0.0488  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6959 (0.9619)  labels_encoder: 0.6959 (0.9619)  labels_encoder_unscaled: 0.6959 (0.9619)  time: 0.0468  data: 0.0001  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7590 (0.9346)  labels_encoder: 0.7590 (0.9346)  labels_encoder_unscaled: 0.7590 (0.9346)  time: 0.0579  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3971 (0.9277)  labels_encoder: 0.3971 (0.9277)  labels_encoder_unscaled: 0.3971 (0.9277)  time: 0.0432  data: 0.0001  max mem: 1206
Test: Total time: 0:00:31 (0.0561 s / it)
Averaged stats: loss: 0.3971 (0.9277)  labels_encoder: 0.3971 (0.9277)  labels_encoder_unscaled: 0.3971 (0.9277)
(21, 71496)
(21, 71496)
[Epoch-2] [IDU-tvseries_anet_features.pickle] mAP: 0.1175, mcAP: 0.8690

BaseballPitch: 0.0487
BasketballDunk: 0.1199
Billiards: 0.0039
CleanAndJerk: 0.3567
CliffDiving: 0.3336
CricketBowling: 0.0605
CricketShot: 0.0894
Diving: 0.0035
FrisbeeCatch: 0.1235
GolfSwing: 0.0721
HammerThrow: 0.1367
HighJump: 0.0353
JavelinThrow: 0.0890
LongJump: 0.2177
PoleVault: 0.1131
Shotput: 0.1257
SoccerPenalty: 0.0425
TennisSwing: 0.1850
ThrowDiscus: 0.0730
VolleyballSpiking: 0.1209
Epoch: [3]  [   0/1509]  eta: 0:50:11  lr: 0.000001  loss: 0.6723 (0.6723)  labels_encoder: 0.6723 (0.6723)  labels_encoder_unscaled: 0.6723 (0.6723)  time: 1.9954  data: 1.7919  max mem: 1206
Epoch: [3]  [  50/1509]  eta: 0:03:41  lr: 0.000001  loss: 0.5240 (0.5592)  labels_encoder: 0.5240 (0.5592)  labels_encoder_unscaled: 0.5240 (0.5592)  time: 0.1187  data: 0.0002  max mem: 1206
Epoch: [3]  [ 100/1509]  eta: 0:03:15  lr: 0.000001  loss: 0.5739 (0.5565)  labels_encoder: 0.5739 (0.5565)  labels_encoder_unscaled: 0.5739 (0.5565)  time: 0.1318  data: 0.0002  max mem: 1206
Epoch: [3]  [ 150/1509]  eta: 0:03:03  lr: 0.000001  loss: 0.5851 (0.5601)  labels_encoder: 0.5851 (0.5601)  labels_encoder_unscaled: 0.5851 (0.5601)  time: 0.1239  data: 0.0002  max mem: 1206
Epoch: [3]  [ 200/1509]  eta: 0:02:54  lr: 0.000001  loss: 0.5306 (0.5571)  labels_encoder: 0.5306 (0.5571)  labels_encoder_unscaled: 0.5306 (0.5571)  time: 0.1303  data: 0.0002  max mem: 1206
Epoch: [3]  [ 250/1509]  eta: 0:02:47  lr: 0.000001  loss: 0.5968 (0.5570)  labels_encoder: 0.5968 (0.5570)  labels_encoder_unscaled: 0.5968 (0.5570)  time: 0.1297  data: 0.0002  max mem: 1206
Epoch: [3]  [ 300/1509]  eta: 0:02:39  lr: 0.000001  loss: 0.5147 (0.5525)  labels_encoder: 0.5147 (0.5525)  labels_encoder_unscaled: 0.5147 (0.5525)  time: 0.1316  data: 0.0002  max mem: 1206
Epoch: [3]  [ 350/1509]  eta: 0:02:33  lr: 0.000001  loss: 0.5203 (0.5499)  labels_encoder: 0.5203 (0.5499)  labels_encoder_unscaled: 0.5203 (0.5499)  time: 0.1330  data: 0.0002  max mem: 1206
Epoch: [3]  [ 400/1509]  eta: 0:02:26  lr: 0.000001  loss: 0.5169 (0.5501)  labels_encoder: 0.5169 (0.5501)  labels_encoder_unscaled: 0.5169 (0.5501)  time: 0.1322  data: 0.0002  max mem: 1206
Epoch: [3]  [ 450/1509]  eta: 0:02:20  lr: 0.000001  loss: 0.5286 (0.5507)  labels_encoder: 0.5286 (0.5507)  labels_encoder_unscaled: 0.5286 (0.5507)  time: 0.1328  data: 0.0002  max mem: 1206
Epoch: [3]  [ 500/1509]  eta: 0:02:13  lr: 0.000001  loss: 0.5420 (0.5502)  labels_encoder: 0.5420 (0.5502)  labels_encoder_unscaled: 0.5420 (0.5502)  time: 0.1343  data: 0.0002  max mem: 1206
Epoch: [3]  [ 550/1509]  eta: 0:02:07  lr: 0.000001  loss: 0.5500 (0.5486)  labels_encoder: 0.5500 (0.5486)  labels_encoder_unscaled: 0.5500 (0.5486)  time: 0.1321  data: 0.0001  max mem: 1206
Epoch: [3]  [ 600/1509]  eta: 0:02:00  lr: 0.000001  loss: 0.5704 (0.5487)  labels_encoder: 0.5704 (0.5487)  labels_encoder_unscaled: 0.5704 (0.5487)  time: 0.1293  data: 0.0002  max mem: 1206
Epoch: [3]  [ 650/1509]  eta: 0:01:53  lr: 0.000001  loss: 0.5166 (0.5473)  labels_encoder: 0.5166 (0.5473)  labels_encoder_unscaled: 0.5166 (0.5473)  time: 0.1318  data: 0.0002  max mem: 1206
Epoch: [3]  [ 700/1509]  eta: 0:01:46  lr: 0.000001  loss: 0.5284 (0.5460)  labels_encoder: 0.5284 (0.5460)  labels_encoder_unscaled: 0.5284 (0.5460)  time: 0.1272  data: 0.0002  max mem: 1206
Epoch: [3]  [ 750/1509]  eta: 0:01:39  lr: 0.000001  loss: 0.5428 (0.5449)  labels_encoder: 0.5428 (0.5449)  labels_encoder_unscaled: 0.5428 (0.5449)  time: 0.1309  data: 0.0001  max mem: 1206
Epoch: [3]  [ 800/1509]  eta: 0:01:33  lr: 0.000001  loss: 0.4801 (0.5442)  labels_encoder: 0.4801 (0.5442)  labels_encoder_unscaled: 0.4801 (0.5442)  time: 0.1315  data: 0.0002  max mem: 1206
Epoch: [3]  [ 850/1509]  eta: 0:01:26  lr: 0.000001  loss: 0.5196 (0.5450)  labels_encoder: 0.5196 (0.5450)  labels_encoder_unscaled: 0.5196 (0.5450)  time: 0.1322  data: 0.0001  max mem: 1206
Epoch: [3]  [ 900/1509]  eta: 0:01:20  lr: 0.000001  loss: 0.5282 (0.5449)  labels_encoder: 0.5282 (0.5449)  labels_encoder_unscaled: 0.5282 (0.5449)  time: 0.1343  data: 0.0002  max mem: 1206
Epoch: [3]  [ 950/1509]  eta: 0:01:13  lr: 0.000001  loss: 0.4770 (0.5442)  labels_encoder: 0.4770 (0.5442)  labels_encoder_unscaled: 0.4770 (0.5442)  time: 0.1304  data: 0.0001  max mem: 1206
Epoch: [3]  [1000/1509]  eta: 0:01:07  lr: 0.000001  loss: 0.5390 (0.5438)  labels_encoder: 0.5390 (0.5438)  labels_encoder_unscaled: 0.5390 (0.5438)  time: 0.1310  data: 0.0001  max mem: 1206
Epoch: [3]  [1050/1509]  eta: 0:01:00  lr: 0.000001  loss: 0.5595 (0.5437)  labels_encoder: 0.5595 (0.5437)  labels_encoder_unscaled: 0.5595 (0.5437)  time: 0.1314  data: 0.0002  max mem: 1206
Epoch: [3]  [1100/1509]  eta: 0:00:53  lr: 0.000001  loss: 0.5480 (0.5434)  labels_encoder: 0.5480 (0.5434)  labels_encoder_unscaled: 0.5480 (0.5434)  time: 0.1311  data: 0.0001  max mem: 1206
Epoch: [3]  [1150/1509]  eta: 0:00:47  lr: 0.000001  loss: 0.5093 (0.5439)  labels_encoder: 0.5093 (0.5439)  labels_encoder_unscaled: 0.5093 (0.5439)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [3]  [1200/1509]  eta: 0:00:40  lr: 0.000001  loss: 0.5157 (0.5433)  labels_encoder: 0.5157 (0.5433)  labels_encoder_unscaled: 0.5157 (0.5433)  time: 0.1234  data: 0.0002  max mem: 1206
Epoch: [3]  [1250/1509]  eta: 0:00:33  lr: 0.000001  loss: 0.4986 (0.5424)  labels_encoder: 0.4986 (0.5424)  labels_encoder_unscaled: 0.4986 (0.5424)  time: 0.1181  data: 0.0001  max mem: 1206
Epoch: [3]  [1300/1509]  eta: 0:00:27  lr: 0.000001  loss: 0.5181 (0.5426)  labels_encoder: 0.5181 (0.5426)  labels_encoder_unscaled: 0.5181 (0.5426)  time: 0.1190  data: 0.0001  max mem: 1206
Epoch: [3]  [1350/1509]  eta: 0:00:20  lr: 0.000001  loss: 0.5150 (0.5421)  labels_encoder: 0.5150 (0.5421)  labels_encoder_unscaled: 0.5150 (0.5421)  time: 0.1201  data: 0.0001  max mem: 1206
Epoch: [3]  [1400/1509]  eta: 0:00:14  lr: 0.000001  loss: 0.5429 (0.5423)  labels_encoder: 0.5429 (0.5423)  labels_encoder_unscaled: 0.5429 (0.5423)  time: 0.1193  data: 0.0001  max mem: 1206
Epoch: [3]  [1450/1509]  eta: 0:00:07  lr: 0.000001  loss: 0.5501 (0.5420)  labels_encoder: 0.5501 (0.5420)  labels_encoder_unscaled: 0.5501 (0.5420)  time: 0.1361  data: 0.0002  max mem: 1206
Epoch: [3]  [1500/1509]  eta: 0:00:01  lr: 0.000001  loss: 0.4995 (0.5412)  labels_encoder: 0.4995 (0.5412)  labels_encoder_unscaled: 0.4995 (0.5412)  time: 0.1264  data: 0.0004  max mem: 1206
Epoch: [3]  [1508/1509]  eta: 0:00:00  lr: 0.000001  loss: 0.4995 (0.5412)  labels_encoder: 0.4995 (0.5412)  labels_encoder_unscaled: 0.4995 (0.5412)  time: 0.1200  data: 0.0003  max mem: 1206
Epoch: [3] Total time: 0:03:15 (0.1298 s / it)
Averaged stats: lr: 0.000001  loss: 0.4995 (0.5412)  labels_encoder: 0.4995 (0.5412)  labels_encoder_unscaled: 0.4995 (0.5412)
Test:  [  0/559]  eta: 0:15:43  loss: 0.6089 (0.6089)  labels_encoder: 0.6089 (0.6089)  labels_encoder_unscaled: 0.6089 (0.6089)  time: 1.6884  data: 1.5915  max mem: 1206
Test:  [ 50/559]  eta: 0:00:49  loss: 0.7752 (0.9009)  labels_encoder: 0.7752 (0.9009)  labels_encoder_unscaled: 0.7752 (0.9009)  time: 0.0629  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:36  loss: 0.6151 (0.8589)  labels_encoder: 0.6151 (0.8589)  labels_encoder_unscaled: 0.6151 (0.8589)  time: 0.0600  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:29  loss: 0.4374 (0.8819)  labels_encoder: 0.4374 (0.8819)  labels_encoder_unscaled: 0.4374 (0.8819)  time: 0.0596  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.5616 (0.8225)  labels_encoder: 0.5616 (0.8225)  labels_encoder_unscaled: 0.5616 (0.8225)  time: 0.0492  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:19  loss: 0.7835 (0.9288)  labels_encoder: 0.7835 (0.9288)  labels_encoder_unscaled: 0.7835 (0.9288)  time: 0.0501  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.1166 (0.9799)  labels_encoder: 1.1166 (0.9799)  labels_encoder_unscaled: 1.1166 (0.9799)  time: 0.0628  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:12  loss: 0.7527 (1.0002)  labels_encoder: 0.7527 (1.0002)  labels_encoder_unscaled: 0.7527 (1.0002)  time: 0.0564  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3262 (0.9740)  labels_encoder: 0.3262 (0.9740)  labels_encoder_unscaled: 0.3262 (0.9740)  time: 0.0546  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4436 (0.9496)  labels_encoder: 0.4436 (0.9496)  labels_encoder_unscaled: 0.4436 (0.9496)  time: 0.0651  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6229 (0.9304)  labels_encoder: 0.6229 (0.9304)  labels_encoder_unscaled: 0.6229 (0.9304)  time: 0.0548  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7003 (0.9047)  labels_encoder: 0.7003 (0.9047)  labels_encoder_unscaled: 0.7003 (0.9047)  time: 0.0514  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.4025 (0.8981)  labels_encoder: 0.4025 (0.8981)  labels_encoder_unscaled: 0.4025 (0.8981)  time: 0.0436  data: 0.0001  max mem: 1206
Test: Total time: 0:00:33 (0.0604 s / it)
Averaged stats: loss: 0.4025 (0.8981)  labels_encoder: 0.4025 (0.8981)  labels_encoder_unscaled: 0.4025 (0.8981)
(21, 71496)
(21, 71496)
[Epoch-3] [IDU-tvseries_anet_features.pickle] mAP: 0.1279, mcAP: 0.8803

BaseballPitch: 0.0746
BasketballDunk: 0.0939
Billiards: 0.0045
CleanAndJerk: 0.3879
CliffDiving: 0.4011
CricketBowling: 0.0587
CricketShot: 0.0876
Diving: 0.0058
FrisbeeCatch: 0.1330
GolfSwing: 0.0551
HammerThrow: 0.1459
HighJump: 0.0523
JavelinThrow: 0.0501
LongJump: 0.3306
PoleVault: 0.1157
Shotput: 0.1282
SoccerPenalty: 0.0511
TennisSwing: 0.1720
ThrowDiscus: 0.0677
VolleyballSpiking: 0.1427
Epoch: [4]  [   0/1509]  eta: 0:44:53  lr: 0.000000  loss: 0.6042 (0.6042)  labels_encoder: 0.6042 (0.6042)  labels_encoder_unscaled: 0.6042 (0.6042)  time: 1.7852  data: 1.5617  max mem: 1206
Epoch: [4]  [  50/1509]  eta: 0:04:07  lr: 0.000000  loss: 0.4301 (0.5076)  labels_encoder: 0.4301 (0.5076)  labels_encoder_unscaled: 0.4301 (0.5076)  time: 0.1303  data: 0.0002  max mem: 1206
Epoch: [4]  [ 100/1509]  eta: 0:03:28  lr: 0.000000  loss: 0.5439 (0.5306)  labels_encoder: 0.5439 (0.5306)  labels_encoder_unscaled: 0.5439 (0.5306)  time: 0.1245  data: 0.0002  max mem: 1206
Epoch: [4]  [ 150/1509]  eta: 0:03:06  lr: 0.000000  loss: 0.5409 (0.5294)  labels_encoder: 0.5409 (0.5294)  labels_encoder_unscaled: 0.5409 (0.5294)  time: 0.1195  data: 0.0002  max mem: 1206
Epoch: [4]  [ 200/1509]  eta: 0:02:55  lr: 0.000000  loss: 0.4770 (0.5281)  labels_encoder: 0.4770 (0.5281)  labels_encoder_unscaled: 0.4770 (0.5281)  time: 0.1257  data: 0.0002  max mem: 1206
Epoch: [4]  [ 250/1509]  eta: 0:02:45  lr: 0.000000  loss: 0.5027 (0.5272)  labels_encoder: 0.5027 (0.5272)  labels_encoder_unscaled: 0.5027 (0.5272)  time: 0.1191  data: 0.0002  max mem: 1206
Epoch: [4]  [ 300/1509]  eta: 0:02:37  lr: 0.000000  loss: 0.5294 (0.5271)  labels_encoder: 0.5294 (0.5271)  labels_encoder_unscaled: 0.5294 (0.5271)  time: 0.1306  data: 0.0002  max mem: 1206
Epoch: [4]  [ 350/1509]  eta: 0:02:32  lr: 0.000000  loss: 0.5109 (0.5279)  labels_encoder: 0.5109 (0.5279)  labels_encoder_unscaled: 0.5109 (0.5279)  time: 0.1322  data: 0.0002  max mem: 1206
Epoch: [4]  [ 400/1509]  eta: 0:02:26  lr: 0.000000  loss: 0.5185 (0.5281)  labels_encoder: 0.5185 (0.5281)  labels_encoder_unscaled: 0.5185 (0.5281)  time: 0.1424  data: 0.0002  max mem: 1206
Epoch: [4]  [ 450/1509]  eta: 0:02:19  lr: 0.000000  loss: 0.5138 (0.5322)  labels_encoder: 0.5138 (0.5322)  labels_encoder_unscaled: 0.5138 (0.5322)  time: 0.1261  data: 0.0002  max mem: 1206
Epoch: [4]  [ 500/1509]  eta: 0:02:12  lr: 0.000000  loss: 0.5482 (0.5332)  labels_encoder: 0.5482 (0.5332)  labels_encoder_unscaled: 0.5482 (0.5332)  time: 0.1245  data: 0.0002  max mem: 1206
Epoch: [4]  [ 550/1509]  eta: 0:02:06  lr: 0.000000  loss: 0.4974 (0.5316)  labels_encoder: 0.4974 (0.5316)  labels_encoder_unscaled: 0.4974 (0.5316)  time: 0.1356  data: 0.0002  max mem: 1206
Epoch: [4]  [ 600/1509]  eta: 0:02:00  lr: 0.000000  loss: 0.5019 (0.5319)  labels_encoder: 0.5019 (0.5319)  labels_encoder_unscaled: 0.5019 (0.5319)  time: 0.1406  data: 0.0002  max mem: 1206
Epoch: [4]  [ 650/1509]  eta: 0:01:53  lr: 0.000000  loss: 0.5173 (0.5330)  labels_encoder: 0.5173 (0.5330)  labels_encoder_unscaled: 0.5173 (0.5330)  time: 0.1301  data: 0.0002  max mem: 1206
Epoch: [4]  [ 700/1509]  eta: 0:01:46  lr: 0.000000  loss: 0.5069 (0.5313)  labels_encoder: 0.5069 (0.5313)  labels_encoder_unscaled: 0.5069 (0.5313)  time: 0.1291  data: 0.0001  max mem: 1206
Epoch: [4]  [ 750/1509]  eta: 0:01:39  lr: 0.000000  loss: 0.5481 (0.5320)  labels_encoder: 0.5481 (0.5320)  labels_encoder_unscaled: 0.5481 (0.5320)  time: 0.1302  data: 0.0001  max mem: 1206
Epoch: [4]  [ 800/1509]  eta: 0:01:33  lr: 0.000000  loss: 0.5469 (0.5318)  labels_encoder: 0.5469 (0.5318)  labels_encoder_unscaled: 0.5469 (0.5318)  time: 0.1410  data: 0.0002  max mem: 1206
Epoch: [4]  [ 850/1509]  eta: 0:01:27  lr: 0.000000  loss: 0.5088 (0.5303)  labels_encoder: 0.5088 (0.5303)  labels_encoder_unscaled: 0.5088 (0.5303)  time: 0.1326  data: 0.0002  max mem: 1206
Epoch: [4]  [ 900/1509]  eta: 0:01:21  lr: 0.000000  loss: 0.5359 (0.5301)  labels_encoder: 0.5359 (0.5301)  labels_encoder_unscaled: 0.5359 (0.5301)  time: 0.1421  data: 0.0002  max mem: 1206
Epoch: [4]  [ 950/1509]  eta: 0:01:14  lr: 0.000000  loss: 0.5091 (0.5296)  labels_encoder: 0.5091 (0.5296)  labels_encoder_unscaled: 0.5091 (0.5296)  time: 0.1427  data: 0.0002  max mem: 1206
Epoch: [4]  [1000/1509]  eta: 0:01:08  lr: 0.000000  loss: 0.4645 (0.5289)  labels_encoder: 0.4645 (0.5289)  labels_encoder_unscaled: 0.4645 (0.5289)  time: 0.1435  data: 0.0002  max mem: 1206
Epoch: [4]  [1050/1509]  eta: 0:01:01  lr: 0.000000  loss: 0.5065 (0.5288)  labels_encoder: 0.5065 (0.5288)  labels_encoder_unscaled: 0.5065 (0.5288)  time: 0.1408  data: 0.0002  max mem: 1206
Epoch: [4]  [1100/1509]  eta: 0:00:55  lr: 0.000000  loss: 0.5160 (0.5282)  labels_encoder: 0.5160 (0.5282)  labels_encoder_unscaled: 0.5160 (0.5282)  time: 0.1414  data: 0.0002  max mem: 1206
Epoch: [4]  [1150/1509]  eta: 0:00:48  lr: 0.000000  loss: 0.5415 (0.5281)  labels_encoder: 0.5415 (0.5281)  labels_encoder_unscaled: 0.5415 (0.5281)  time: 0.1417  data: 0.0002  max mem: 1206
Epoch: [4]  [1200/1509]  eta: 0:00:41  lr: 0.000000  loss: 0.4924 (0.5279)  labels_encoder: 0.4924 (0.5279)  labels_encoder_unscaled: 0.4924 (0.5279)  time: 0.1372  data: 0.0002  max mem: 1206
Epoch: [4]  [1250/1509]  eta: 0:00:35  lr: 0.000000  loss: 0.5057 (0.5283)  labels_encoder: 0.5057 (0.5283)  labels_encoder_unscaled: 0.5057 (0.5283)  time: 0.1420  data: 0.0002  max mem: 1206
Epoch: [4]  [1300/1509]  eta: 0:00:28  lr: 0.000000  loss: 0.5483 (0.5281)  labels_encoder: 0.5483 (0.5281)  labels_encoder_unscaled: 0.5483 (0.5281)  time: 0.1311  data: 0.0002  max mem: 1206
Epoch: [4]  [1350/1509]  eta: 0:00:21  lr: 0.000000  loss: 0.5495 (0.5294)  labels_encoder: 0.5495 (0.5294)  labels_encoder_unscaled: 0.5495 (0.5294)  time: 0.1364  data: 0.0002  max mem: 1206
Epoch: [4]  [1400/1509]  eta: 0:00:14  lr: 0.000000  loss: 0.4844 (0.5290)  labels_encoder: 0.4844 (0.5290)  labels_encoder_unscaled: 0.4844 (0.5290)  time: 0.1430  data: 0.0002  max mem: 1206
Epoch: [4]  [1450/1509]  eta: 0:00:07  lr: 0.000000  loss: 0.4813 (0.5291)  labels_encoder: 0.4813 (0.5291)  labels_encoder_unscaled: 0.4813 (0.5291)  time: 0.1307  data: 0.0001  max mem: 1206
Epoch: [4]  [1500/1509]  eta: 0:00:01  lr: 0.000000  loss: 0.5138 (0.5291)  labels_encoder: 0.5138 (0.5291)  labels_encoder_unscaled: 0.5138 (0.5291)  time: 0.1217  data: 0.0002  max mem: 1206
Epoch: [4]  [1508/1509]  eta: 0:00:00  lr: 0.000000  loss: 0.5172 (0.5292)  labels_encoder: 0.5172 (0.5292)  labels_encoder_unscaled: 0.5172 (0.5292)  time: 0.1115  data: 0.0002  max mem: 1206
Epoch: [4] Total time: 0:03:24 (0.1353 s / it)
Averaged stats: lr: 0.000000  loss: 0.5172 (0.5292)  labels_encoder: 0.5172 (0.5292)  labels_encoder_unscaled: 0.5172 (0.5292)
Test:  [  0/559]  eta: 0:15:45  loss: 0.6215 (0.6215)  labels_encoder: 0.6215 (0.6215)  labels_encoder_unscaled: 0.6215 (0.6215)  time: 1.6919  data: 1.6317  max mem: 1206
Test:  [ 50/559]  eta: 0:00:45  loss: 0.7707 (0.8839)  labels_encoder: 0.7707 (0.8839)  labels_encoder_unscaled: 0.7707 (0.8839)  time: 0.0642  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:33  loss: 0.6327 (0.8528)  labels_encoder: 0.6327 (0.8528)  labels_encoder_unscaled: 0.6327 (0.8528)  time: 0.0546  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:27  loss: 0.5467 (0.8782)  labels_encoder: 0.5467 (0.8782)  labels_encoder_unscaled: 0.5467 (0.8782)  time: 0.0552  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:22  loss: 0.5649 (0.8191)  labels_encoder: 0.5649 (0.8191)  labels_encoder_unscaled: 0.5649 (0.8191)  time: 0.0481  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:18  loss: 0.7143 (0.9279)  labels_encoder: 0.7143 (0.9279)  labels_encoder_unscaled: 0.7143 (0.9279)  time: 0.0507  data: 0.0001  max mem: 1206
Test:  [300/559]  eta: 0:00:15  loss: 1.2215 (0.9811)  labels_encoder: 1.2215 (0.9811)  labels_encoder_unscaled: 1.2215 (0.9811)  time: 0.0571  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:12  loss: 0.7772 (1.0019)  labels_encoder: 0.7772 (1.0019)  labels_encoder_unscaled: 0.7772 (1.0019)  time: 0.0563  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3380 (0.9766)  labels_encoder: 0.3380 (0.9766)  labels_encoder_unscaled: 0.3380 (0.9766)  time: 0.0513  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4197 (0.9517)  labels_encoder: 0.4197 (0.9517)  labels_encoder_unscaled: 0.4197 (0.9517)  time: 0.0490  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5984 (0.9329)  labels_encoder: 0.5984 (0.9329)  labels_encoder_unscaled: 0.5984 (0.9329)  time: 0.0500  data: 0.0001  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7204 (0.9056)  labels_encoder: 0.7204 (0.9056)  labels_encoder_unscaled: 0.7204 (0.9056)  time: 0.0509  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3590 (0.8989)  labels_encoder: 0.3590 (0.8989)  labels_encoder_unscaled: 0.3590 (0.8989)  time: 0.0399  data: 0.0001  max mem: 1206
Test: Total time: 0:00:31 (0.0562 s / it)
Averaged stats: loss: 0.3590 (0.8989)  labels_encoder: 0.3590 (0.8989)  labels_encoder_unscaled: 0.3590 (0.8989)
(21, 71496)
(21, 71496)
[Epoch-4] [IDU-tvseries_anet_features.pickle] mAP: 0.1243, mcAP: 0.8795

BaseballPitch: 0.0764
BasketballDunk: 0.1031
Billiards: 0.0044
CleanAndJerk: 0.3784
CliffDiving: 0.3742
CricketBowling: 0.0634
CricketShot: 0.1028
Diving: 0.0070
FrisbeeCatch: 0.1290
GolfSwing: 0.0604
HammerThrow: 0.1416
HighJump: 0.0373
JavelinThrow: 0.0519
LongJump: 0.3036
PoleVault: 0.1082
Shotput: 0.1366
SoccerPenalty: 0.0492
TennisSwing: 0.1774
ThrowDiscus: 0.0412
VolleyballSpiking: 0.1402
Training time 0:15:44
