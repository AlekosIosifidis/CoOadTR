Unable to compile CoConv C++ implementation. Falling back to Python version.
[Errno 2] No such file or directory: '/home/lh/.conda/envs/oadtr/lib/python3.8/site-packages/continual/conv.cpp'
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'conv_flops_counter_hook'
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'MODULES_MAPPING'
Not using distributed mode
lr:0.0001
batch_size:128
weight_decay:0.0001
epochs:5
resize_feature:False
lr_drop:1
clip_max_norm:1.0
dataparallel:False
removelog:False
version:v3
query_num:8
decoder_layers:5
decoder_embedding_dim:1024
decoder_embedding_dim_out:1024
decoder_attn_dropout_rate:0.1
decoder_num_heads:4
classification_pred_loss_coef:0.5
enc_layers:64
lr_backbone:0.0001
feature:tvseries_anet_features.pickle
dim_feature:4096
patch_dim:1
embedding_dim:1024
num_heads:8
num_layers:2
attn_dropout_rate:0.1
positional_encoding_type:recycling_fixed
num_embeddings:127
hidden_dim:1024
dropout_rate:0.1
numclass:31
classification_x_loss_coef:0.3
classification_h_loss_coef:1
similar_loss_coef:0.1
margin:1.0
dataset:tvseries
dataset_file:data/data_info_new.json
frozen_weights:None
thumos_data_path:/home/dancer/mycode/Temporal.Online.Detection/Online.TRN.Pytorch/preprocess/
thumos_anno_path:data/thumos_{}_anno.pickle
remove_difficult:False
device:cuda
output_dir:models
seed:3
resume:
start_epoch:1
eval:False
num_workers:8
world_size:1
dist_url:tcp://127.0.0.1:12342
train_session_set:['24_ep1', '24_ep2', '24_ep3', 'Breaking_Bad_ep1', 'Breaking_Bad_ep2', 'How_I_Met_Your_Mother_ep1', 'How_I_Met_Your_Mother_ep2', 'How_I_Met_Your_Mother_ep3', 'How_I_Met_Your_Mother_ep4', 'How_I_Met_Your_Mother_ep5', 'How_I_Met_Your_Mother_ep6', 'Mad_Men_ep1', 'Mad_Men_ep2', 'Modern_Family_ep1', 'Modern_Family_ep2', 'Modern_Family_ep3', 'Modern_Family_ep4', 'Modern_Family_ep6', 'Sons_of_Anarchy_ep1', 'Sons_of_Anarchy_ep2']
test_session_set:['24_ep4', 'Breaking_Bad_ep3', 'Mad_Men_ep3', 'How_I_Met_Your_Mother_ep7', 'How_I_Met_Your_Mother_ep8', 'Modern_Family_ep5', 'Sons_of_Anarchy_ep3']
class_index:['background', 'Pick something up', 'Point', 'Drink', 'Stand up', 'Run', 'Sit down', 'Read', 'Smoke', 'Drive car', 'Open door', 'Give something', 'Use computer', 'Write', 'Go down stairway', 'Close door', 'Throw something', 'Go up stairway', 'Get in/out of car', 'Hang up phone', 'Eat', 'Answer phone', 'Dress up', 'Clap', 'Undress', 'Kiss', 'Fall/trip', 'Wave', 'Pour', 'Punch', 'Fire weapon']
distributed:False
position encoding : recycling_fixed
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Sequential(
  16.814 M, 99.939% Params, 0.412 GMac, 100.000% MACs, 
  (0): Linear(4.195 M, 24.936% Params, 0.004 GMac, 1.018% MACs, in_features=4096, out_features=1024, bias=True, channel_dim=1)
  (1): RecyclingPositionalEncoding(
    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 
    (pe): CyclicPositionalEncoding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
  )
  (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
  (3): Sequential(
    12.587 M, 74.814% Params, 0.408 GMac, 98.974% MACs, 
    (0): Sequential(
      6.294 M, 37.407% Params, 0.205 GMac, 49.804% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, reduce=reduce_sum
        (0): RetroactiveUnity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 63)
        (1): CoReMultiheadAttention(
          4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(Sequential(
        2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
        (0): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
        (1): Residual(
          2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
          (fn): Sequential(
            2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
            (0): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
            (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
            (3): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          )
        )
        (2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
      ))
    )
    (1): Lambda(Sequential(
      6.294 M, 37.407% Params, 0.203 GMac, 49.170% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, reduce=sum_last_pairs
        (0): SelectOrDelay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): CoSiMultiheadAttention(
          4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
      (2): BroadcastReduce(
        2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, reduce=reduce_sum
        (0): Delay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): Sequential(
          2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, 
          (0): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
          (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          (3): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
        )
      )
      (3): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
    ), takes_time=True)
    (2): Lambda(unity, squeeze_last, squeeze_last, takes_time=True)
  )
  (4): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
  (5): Linear(0.032 M, 0.189% Params, 0.0 GMac, 0.008% MACs, in_features=1024, out_features=31, bias=True, channel_dim=1)
)
Model FLOPs: 411997217.0
Model params: 16824351
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| Active memory         |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   83968 KB |   83968 KB |   83968 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16445 KB |   16445 KB |  302954 KB |  286508 KB |
|---------------------------------------------------------------------------|
| Allocations           |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| Active allocs         |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |    1458    |    1448    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Memory state pre, max, post inference: 69143040 71518720 69143040
Loaded tvseries_anet_features.pickle
Loaded tvseries_anet_features.pickle
Start training
Epoch: [1]  [   0/1510]  eta: 1:08:36  lr: 0.000100  loss: 2.9984 (2.9984)  labels_encoder: 2.9984 (2.9984)  labels_encoder_unscaled: 2.9984 (2.9984)  time: 2.7261  data: 2.1420  max mem: 1013
Epoch: [1]  [  50/1510]  eta: 0:04:27  lr: 0.000100  loss: 1.0425 (1.1292)  labels_encoder: 1.0425 (1.1292)  labels_encoder_unscaled: 1.0425 (1.1292)  time: 0.1277  data: 0.0002  max mem: 1206
Epoch: [1]  [ 100/1510]  eta: 0:03:35  lr: 0.000100  loss: 1.0273 (1.0593)  labels_encoder: 1.0273 (1.0593)  labels_encoder_unscaled: 1.0273 (1.0593)  time: 0.1142  data: 0.0002  max mem: 1206
Epoch: [1]  [ 150/1510]  eta: 0:03:11  lr: 0.000100  loss: 0.9479 (1.0245)  labels_encoder: 0.9479 (1.0245)  labels_encoder_unscaled: 0.9479 (1.0245)  time: 0.1210  data: 0.0002  max mem: 1206
Epoch: [1]  [ 200/1510]  eta: 0:02:56  lr: 0.000100  loss: 0.9474 (0.9981)  labels_encoder: 0.9474 (0.9981)  labels_encoder_unscaled: 0.9474 (0.9981)  time: 0.1203  data: 0.0002  max mem: 1206
Epoch: [1]  [ 250/1510]  eta: 0:02:46  lr: 0.000100  loss: 0.7578 (0.9589)  labels_encoder: 0.7578 (0.9589)  labels_encoder_unscaled: 0.7578 (0.9589)  time: 0.1218  data: 0.0002  max mem: 1206
Epoch: [1]  [ 300/1510]  eta: 0:02:37  lr: 0.000100  loss: 0.8508 (0.9422)  labels_encoder: 0.8508 (0.9422)  labels_encoder_unscaled: 0.8508 (0.9422)  time: 0.1202  data: 0.0002  max mem: 1206
Epoch: [1]  [ 350/1510]  eta: 0:02:29  lr: 0.000100  loss: 0.8084 (0.9301)  labels_encoder: 0.8084 (0.9301)  labels_encoder_unscaled: 0.8084 (0.9301)  time: 0.1201  data: 0.0002  max mem: 1206
Epoch: [1]  [ 400/1510]  eta: 0:02:22  lr: 0.000100  loss: 0.8197 (0.9165)  labels_encoder: 0.8197 (0.9165)  labels_encoder_unscaled: 0.8197 (0.9165)  time: 0.1228  data: 0.0002  max mem: 1206
Epoch: [1]  [ 450/1510]  eta: 0:02:15  lr: 0.000100  loss: 0.7672 (0.9027)  labels_encoder: 0.7672 (0.9027)  labels_encoder_unscaled: 0.7672 (0.9027)  time: 0.1325  data: 0.0002  max mem: 1206
Epoch: [1]  [ 500/1510]  eta: 0:02:09  lr: 0.000100  loss: 0.7583 (0.8908)  labels_encoder: 0.7583 (0.8908)  labels_encoder_unscaled: 0.7583 (0.8908)  time: 0.1282  data: 0.0002  max mem: 1206
Epoch: [1]  [ 550/1510]  eta: 0:02:03  lr: 0.000100  loss: 0.7999 (0.8817)  labels_encoder: 0.7999 (0.8817)  labels_encoder_unscaled: 0.7999 (0.8817)  time: 0.1308  data: 0.0002  max mem: 1206
Epoch: [1]  [ 600/1510]  eta: 0:01:57  lr: 0.000100  loss: 0.7371 (0.8734)  labels_encoder: 0.7371 (0.8734)  labels_encoder_unscaled: 0.7371 (0.8734)  time: 0.1313  data: 0.0002  max mem: 1206
Epoch: [1]  [ 650/1510]  eta: 0:01:50  lr: 0.000100  loss: 0.7463 (0.8640)  labels_encoder: 0.7463 (0.8640)  labels_encoder_unscaled: 0.7463 (0.8640)  time: 0.1304  data: 0.0002  max mem: 1206
Epoch: [1]  [ 700/1510]  eta: 0:01:43  lr: 0.000100  loss: 0.7441 (0.8564)  labels_encoder: 0.7441 (0.8564)  labels_encoder_unscaled: 0.7441 (0.8564)  time: 0.1219  data: 0.0002  max mem: 1206
Epoch: [1]  [ 750/1510]  eta: 0:01:36  lr: 0.000100  loss: 0.7258 (0.8508)  labels_encoder: 0.7258 (0.8508)  labels_encoder_unscaled: 0.7258 (0.8508)  time: 0.1197  data: 0.0002  max mem: 1206
Epoch: [1]  [ 800/1510]  eta: 0:01:30  lr: 0.000100  loss: 0.7445 (0.8431)  labels_encoder: 0.7445 (0.8431)  labels_encoder_unscaled: 0.7445 (0.8431)  time: 0.1223  data: 0.0002  max mem: 1206
Epoch: [1]  [ 850/1510]  eta: 0:01:23  lr: 0.000100  loss: 0.6812 (0.8363)  labels_encoder: 0.6812 (0.8363)  labels_encoder_unscaled: 0.6812 (0.8363)  time: 0.1152  data: 0.0002  max mem: 1206
Epoch: [1]  [ 900/1510]  eta: 0:01:17  lr: 0.000100  loss: 0.7065 (0.8294)  labels_encoder: 0.7065 (0.8294)  labels_encoder_unscaled: 0.7065 (0.8294)  time: 0.1232  data: 0.0002  max mem: 1206
Epoch: [1]  [ 950/1510]  eta: 0:01:10  lr: 0.000100  loss: 0.7520 (0.8243)  labels_encoder: 0.7520 (0.8243)  labels_encoder_unscaled: 0.7520 (0.8243)  time: 0.1246  data: 0.0002  max mem: 1206
Epoch: [1]  [1000/1510]  eta: 0:01:04  lr: 0.000100  loss: 0.7400 (0.8194)  labels_encoder: 0.7400 (0.8194)  labels_encoder_unscaled: 0.7400 (0.8194)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [1]  [1050/1510]  eta: 0:00:58  lr: 0.000100  loss: 0.7257 (0.8148)  labels_encoder: 0.7257 (0.8148)  labels_encoder_unscaled: 0.7257 (0.8148)  time: 0.1428  data: 0.0002  max mem: 1206
Epoch: [1]  [1100/1510]  eta: 0:00:52  lr: 0.000100  loss: 0.6915 (0.8111)  labels_encoder: 0.6915 (0.8111)  labels_encoder_unscaled: 0.6915 (0.8111)  time: 0.1273  data: 0.0002  max mem: 1206
Epoch: [1]  [1150/1510]  eta: 0:00:45  lr: 0.000100  loss: 0.6721 (0.8069)  labels_encoder: 0.6721 (0.8069)  labels_encoder_unscaled: 0.6721 (0.8069)  time: 0.1367  data: 0.0002  max mem: 1206
Epoch: [1]  [1200/1510]  eta: 0:00:39  lr: 0.000100  loss: 0.6250 (0.8010)  labels_encoder: 0.6250 (0.8010)  labels_encoder_unscaled: 0.6250 (0.8010)  time: 0.1426  data: 0.0002  max mem: 1206
Epoch: [1]  [1250/1510]  eta: 0:00:33  lr: 0.000100  loss: 0.6490 (0.7979)  labels_encoder: 0.6490 (0.7979)  labels_encoder_unscaled: 0.6490 (0.7979)  time: 0.1444  data: 0.0002  max mem: 1206
Epoch: [1]  [1300/1510]  eta: 0:00:27  lr: 0.000100  loss: 0.6254 (0.7930)  labels_encoder: 0.6254 (0.7930)  labels_encoder_unscaled: 0.6254 (0.7930)  time: 0.1296  data: 0.0002  max mem: 1206
Epoch: [1]  [1350/1510]  eta: 0:00:20  lr: 0.000100  loss: 0.6955 (0.7890)  labels_encoder: 0.6955 (0.7890)  labels_encoder_unscaled: 0.6955 (0.7890)  time: 0.1306  data: 0.0002  max mem: 1206
Epoch: [1]  [1400/1510]  eta: 0:00:14  lr: 0.000100  loss: 0.6372 (0.7842)  labels_encoder: 0.6372 (0.7842)  labels_encoder_unscaled: 0.6372 (0.7842)  time: 0.1418  data: 0.0002  max mem: 1206
Epoch: [1]  [1450/1510]  eta: 0:00:07  lr: 0.000100  loss: 0.6526 (0.7808)  labels_encoder: 0.6526 (0.7808)  labels_encoder_unscaled: 0.6526 (0.7808)  time: 0.1418  data: 0.0002  max mem: 1206
Epoch: [1]  [1500/1510]  eta: 0:00:01  lr: 0.000100  loss: 0.6407 (0.7773)  labels_encoder: 0.6407 (0.7773)  labels_encoder_unscaled: 0.6407 (0.7773)  time: 0.1260  data: 0.0003  max mem: 1206
Epoch: [1]  [1509/1510]  eta: 0:00:00  lr: 0.000100  loss: 0.6426 (0.7767)  labels_encoder: 0.6426 (0.7767)  labels_encoder_unscaled: 0.6426 (0.7767)  time: 0.1209  data: 0.0003  max mem: 1206
Epoch: [1] Total time: 0:03:16 (0.1300 s / it)
Averaged stats: lr: 0.000100  loss: 0.6426 (0.7767)  labels_encoder: 0.6426 (0.7767)  labels_encoder_unscaled: 0.6426 (0.7767)
Test:  [  0/559]  eta: 0:15:22  loss: 0.6843 (0.6843)  labels_encoder: 0.6843 (0.6843)  labels_encoder_unscaled: 0.6843 (0.6843)  time: 1.6504  data: 1.6084  max mem: 1206
Test:  [ 50/559]  eta: 0:00:48  loss: 1.5196 (1.1159)  labels_encoder: 1.5196 (1.1159)  labels_encoder_unscaled: 1.5196 (1.1159)  time: 0.0613  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:35  loss: 0.7063 (1.0235)  labels_encoder: 0.7063 (1.0235)  labels_encoder_unscaled: 0.7063 (1.0235)  time: 0.0569  data: 0.0010  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.5588 (1.0031)  labels_encoder: 0.5588 (1.0031)  labels_encoder_unscaled: 0.5588 (1.0031)  time: 0.0554  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.7045 (0.9147)  labels_encoder: 0.7045 (0.9147)  labels_encoder_unscaled: 0.7045 (0.9147)  time: 0.0535  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:19  loss: 0.9434 (1.0322)  labels_encoder: 0.9434 (1.0322)  labels_encoder_unscaled: 0.9434 (1.0322)  time: 0.0510  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:15  loss: 1.4810 (1.0829)  labels_encoder: 1.4810 (1.0829)  labels_encoder_unscaled: 1.4810 (1.0829)  time: 0.0477  data: 0.0001  max mem: 1206
Test:  [350/559]  eta: 0:00:12  loss: 0.8281 (1.0972)  labels_encoder: 0.8281 (1.0972)  labels_encoder_unscaled: 0.8281 (1.0972)  time: 0.0482  data: 0.0001  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.4464 (1.0671)  labels_encoder: 0.4464 (1.0671)  labels_encoder_unscaled: 0.4464 (1.0671)  time: 0.0476  data: 0.0001  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.3969 (1.0327)  labels_encoder: 0.3969 (1.0327)  labels_encoder_unscaled: 0.3969 (1.0327)  time: 0.0475  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6305 (1.0103)  labels_encoder: 0.6305 (1.0103)  labels_encoder_unscaled: 0.6305 (1.0103)  time: 0.0532  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.8409 (0.9849)  labels_encoder: 0.8409 (0.9849)  labels_encoder_unscaled: 0.8409 (0.9849)  time: 0.0455  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.7236 (0.9791)  labels_encoder: 0.7236 (0.9791)  labels_encoder_unscaled: 0.7236 (0.9791)  time: 0.0373  data: 0.0001  max mem: 1206
Test: Total time: 0:00:30 (0.0553 s / it)
Averaged stats: loss: 0.7236 (0.9791)  labels_encoder: 0.7236 (0.9791)  labels_encoder_unscaled: 0.7236 (0.9791)
(21, 71496)
(21, 71496)
[Epoch-1] [IDU-tvseries_anet_features.pickle] mAP: 0.1234, mcAP: 0.8796

BaseballPitch: 0.0470
BasketballDunk: 0.0743
Billiards: 0.0049
CleanAndJerk: 0.3771
CliffDiving: 0.5437
CricketBowling: 0.0581
CricketShot: 0.0828
Diving: 0.0258
FrisbeeCatch: 0.0799
GolfSwing: 0.0259
HammerThrow: 0.0975
HighJump: 0.0482
JavelinThrow: 0.0685
LongJump: 0.3026
PoleVault: 0.0862
Shotput: 0.1217
SoccerPenalty: 0.0535
TennisSwing: 0.1354
ThrowDiscus: 0.1072
VolleyballSpiking: 0.1278
Epoch: [2]  [   0/1510]  eta: 0:44:38  lr: 0.000010  loss: 0.6370 (0.6370)  labels_encoder: 0.6370 (0.6370)  labels_encoder_unscaled: 0.6370 (0.6370)  time: 1.7742  data: 1.6094  max mem: 1206
Epoch: [2]  [  50/1510]  eta: 0:04:02  lr: 0.000010  loss: 0.6400 (0.6332)  labels_encoder: 0.6400 (0.6332)  labels_encoder_unscaled: 0.6400 (0.6332)  time: 0.1302  data: 0.0002  max mem: 1206
Epoch: [2]  [ 100/1510]  eta: 0:03:29  lr: 0.000010  loss: 0.6379 (0.6343)  labels_encoder: 0.6379 (0.6343)  labels_encoder_unscaled: 0.6379 (0.6343)  time: 0.1300  data: 0.0002  max mem: 1206
Epoch: [2]  [ 150/1510]  eta: 0:03:13  lr: 0.000010  loss: 0.5805 (0.6215)  labels_encoder: 0.5805 (0.6215)  labels_encoder_unscaled: 0.5805 (0.6215)  time: 0.1329  data: 0.0002  max mem: 1206
Epoch: [2]  [ 200/1510]  eta: 0:03:02  lr: 0.000010  loss: 0.5431 (0.6240)  labels_encoder: 0.5431 (0.6240)  labels_encoder_unscaled: 0.5431 (0.6240)  time: 0.1277  data: 0.0002  max mem: 1206
Epoch: [2]  [ 250/1510]  eta: 0:02:51  lr: 0.000010  loss: 0.6195 (0.6174)  labels_encoder: 0.6195 (0.6174)  labels_encoder_unscaled: 0.6195 (0.6174)  time: 0.1315  data: 0.0002  max mem: 1206
Epoch: [2]  [ 300/1510]  eta: 0:02:41  lr: 0.000010  loss: 0.5625 (0.6099)  labels_encoder: 0.5625 (0.6099)  labels_encoder_unscaled: 0.5625 (0.6099)  time: 0.1168  data: 0.0002  max mem: 1206
Epoch: [2]  [ 350/1510]  eta: 0:02:33  lr: 0.000010  loss: 0.6173 (0.6092)  labels_encoder: 0.6173 (0.6092)  labels_encoder_unscaled: 0.6173 (0.6092)  time: 0.1222  data: 0.0002  max mem: 1206
Epoch: [2]  [ 400/1510]  eta: 0:02:25  lr: 0.000010  loss: 0.5324 (0.6042)  labels_encoder: 0.5324 (0.6042)  labels_encoder_unscaled: 0.5324 (0.6042)  time: 0.1294  data: 0.0002  max mem: 1206
Epoch: [2]  [ 450/1510]  eta: 0:02:18  lr: 0.000010  loss: 0.5926 (0.6022)  labels_encoder: 0.5926 (0.6022)  labels_encoder_unscaled: 0.5926 (0.6022)  time: 0.1253  data: 0.0002  max mem: 1206
Epoch: [2]  [ 500/1510]  eta: 0:02:11  lr: 0.000010  loss: 0.5903 (0.5994)  labels_encoder: 0.5903 (0.5994)  labels_encoder_unscaled: 0.5903 (0.5994)  time: 0.1257  data: 0.0002  max mem: 1206
Epoch: [2]  [ 550/1510]  eta: 0:02:05  lr: 0.000010  loss: 0.6002 (0.5979)  labels_encoder: 0.6002 (0.5979)  labels_encoder_unscaled: 0.6002 (0.5979)  time: 0.1306  data: 0.0002  max mem: 1206
Epoch: [2]  [ 600/1510]  eta: 0:01:58  lr: 0.000010  loss: 0.5510 (0.5935)  labels_encoder: 0.5510 (0.5935)  labels_encoder_unscaled: 0.5510 (0.5935)  time: 0.1260  data: 0.0002  max mem: 1206
Epoch: [2]  [ 650/1510]  eta: 0:01:52  lr: 0.000010  loss: 0.5568 (0.5916)  labels_encoder: 0.5568 (0.5916)  labels_encoder_unscaled: 0.5568 (0.5916)  time: 0.1442  data: 0.0002  max mem: 1206
Epoch: [2]  [ 700/1510]  eta: 0:01:46  lr: 0.000010  loss: 0.5507 (0.5911)  labels_encoder: 0.5507 (0.5911)  labels_encoder_unscaled: 0.5507 (0.5911)  time: 0.1445  data: 0.0002  max mem: 1206
Epoch: [2]  [ 750/1510]  eta: 0:01:40  lr: 0.000010  loss: 0.5546 (0.5887)  labels_encoder: 0.5546 (0.5887)  labels_encoder_unscaled: 0.5546 (0.5887)  time: 0.1418  data: 0.0002  max mem: 1206
Epoch: [2]  [ 800/1510]  eta: 0:01:34  lr: 0.000010  loss: 0.5760 (0.5880)  labels_encoder: 0.5760 (0.5880)  labels_encoder_unscaled: 0.5760 (0.5880)  time: 0.1414  data: 0.0002  max mem: 1206
Epoch: [2]  [ 850/1510]  eta: 0:01:27  lr: 0.000010  loss: 0.6130 (0.5875)  labels_encoder: 0.6130 (0.5875)  labels_encoder_unscaled: 0.6130 (0.5875)  time: 0.1332  data: 0.0002  max mem: 1206
Epoch: [2]  [ 900/1510]  eta: 0:01:21  lr: 0.000010  loss: 0.5019 (0.5843)  labels_encoder: 0.5019 (0.5843)  labels_encoder_unscaled: 0.5019 (0.5843)  time: 0.1286  data: 0.0002  max mem: 1206
Epoch: [2]  [ 950/1510]  eta: 0:01:14  lr: 0.000010  loss: 0.5924 (0.5835)  labels_encoder: 0.5924 (0.5835)  labels_encoder_unscaled: 0.5924 (0.5835)  time: 0.1340  data: 0.0002  max mem: 1206
Epoch: [2]  [1000/1510]  eta: 0:01:07  lr: 0.000010  loss: 0.5417 (0.5820)  labels_encoder: 0.5417 (0.5820)  labels_encoder_unscaled: 0.5417 (0.5820)  time: 0.1347  data: 0.0002  max mem: 1206
Epoch: [2]  [1050/1510]  eta: 0:01:01  lr: 0.000010  loss: 0.5455 (0.5800)  labels_encoder: 0.5455 (0.5800)  labels_encoder_unscaled: 0.5455 (0.5800)  time: 0.1329  data: 0.0002  max mem: 1206
Epoch: [2]  [1100/1510]  eta: 0:00:54  lr: 0.000010  loss: 0.5617 (0.5789)  labels_encoder: 0.5617 (0.5789)  labels_encoder_unscaled: 0.5617 (0.5789)  time: 0.1447  data: 0.0002  max mem: 1206
Epoch: [2]  [1150/1510]  eta: 0:00:48  lr: 0.000010  loss: 0.5195 (0.5782)  labels_encoder: 0.5195 (0.5782)  labels_encoder_unscaled: 0.5195 (0.5782)  time: 0.1413  data: 0.0002  max mem: 1206
Epoch: [2]  [1200/1510]  eta: 0:00:41  lr: 0.000010  loss: 0.5393 (0.5768)  labels_encoder: 0.5393 (0.5768)  labels_encoder_unscaled: 0.5393 (0.5768)  time: 0.1413  data: 0.0002  max mem: 1206
Epoch: [2]  [1250/1510]  eta: 0:00:35  lr: 0.000010  loss: 0.5710 (0.5761)  labels_encoder: 0.5710 (0.5761)  labels_encoder_unscaled: 0.5710 (0.5761)  time: 0.1421  data: 0.0002  max mem: 1206
Epoch: [2]  [1300/1510]  eta: 0:00:28  lr: 0.000010  loss: 0.5299 (0.5760)  labels_encoder: 0.5299 (0.5760)  labels_encoder_unscaled: 0.5299 (0.5760)  time: 0.1413  data: 0.0002  max mem: 1206
Epoch: [2]  [1350/1510]  eta: 0:00:21  lr: 0.000010  loss: 0.5399 (0.5750)  labels_encoder: 0.5399 (0.5750)  labels_encoder_unscaled: 0.5399 (0.5750)  time: 0.1417  data: 0.0002  max mem: 1206
Epoch: [2]  [1400/1510]  eta: 0:00:14  lr: 0.000010  loss: 0.5576 (0.5741)  labels_encoder: 0.5576 (0.5741)  labels_encoder_unscaled: 0.5576 (0.5741)  time: 0.1379  data: 0.0002  max mem: 1206
Epoch: [2]  [1450/1510]  eta: 0:00:08  lr: 0.000010  loss: 0.5437 (0.5734)  labels_encoder: 0.5437 (0.5734)  labels_encoder_unscaled: 0.5437 (0.5734)  time: 0.1192  data: 0.0002  max mem: 1206
Epoch: [2]  [1500/1510]  eta: 0:00:01  lr: 0.000010  loss: 0.5017 (0.5715)  labels_encoder: 0.5017 (0.5715)  labels_encoder_unscaled: 0.5017 (0.5715)  time: 0.1180  data: 0.0002  max mem: 1206
Epoch: [2]  [1509/1510]  eta: 0:00:00  lr: 0.000010  loss: 0.5017 (0.5715)  labels_encoder: 0.5017 (0.5715)  labels_encoder_unscaled: 0.5017 (0.5715)  time: 0.1103  data: 0.0002  max mem: 1206
Epoch: [2] Total time: 0:03:23 (0.1345 s / it)
Averaged stats: lr: 0.000010  loss: 0.5017 (0.5715)  labels_encoder: 0.5017 (0.5715)  labels_encoder_unscaled: 0.5017 (0.5715)
Test:  [  0/559]  eta: 0:15:39  loss: 0.6026 (0.6026)  labels_encoder: 0.6026 (0.6026)  labels_encoder_unscaled: 0.6026 (0.6026)  time: 1.6798  data: 1.6365  max mem: 1206
Test:  [ 50/559]  eta: 0:00:49  loss: 0.8337 (0.9124)  labels_encoder: 0.8337 (0.9124)  labels_encoder_unscaled: 0.8337 (0.9124)  time: 0.0630  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:37  loss: 0.4618 (0.8523)  labels_encoder: 0.4618 (0.8523)  labels_encoder_unscaled: 0.4618 (0.8523)  time: 0.0640  data: 0.0016  max mem: 1206
Test:  [150/559]  eta: 0:00:30  loss: 0.5285 (0.8688)  labels_encoder: 0.5285 (0.8688)  labels_encoder_unscaled: 0.5285 (0.8688)  time: 0.0642  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:25  loss: 0.6524 (0.8148)  labels_encoder: 0.6524 (0.8148)  labels_encoder_unscaled: 0.6524 (0.8148)  time: 0.0596  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.8599 (0.9270)  labels_encoder: 0.8599 (0.9270)  labels_encoder_unscaled: 0.8599 (0.9270)  time: 0.0488  data: 0.0001  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.1313 (0.9753)  labels_encoder: 1.1313 (0.9753)  labels_encoder_unscaled: 1.1313 (0.9753)  time: 0.0525  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.8040 (0.9969)  labels_encoder: 0.8040 (0.9969)  labels_encoder_unscaled: 0.8040 (0.9969)  time: 0.0521  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3034 (0.9743)  labels_encoder: 0.3034 (0.9743)  labels_encoder_unscaled: 0.3034 (0.9743)  time: 0.0550  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4110 (0.9509)  labels_encoder: 0.4110 (0.9509)  labels_encoder_unscaled: 0.4110 (0.9509)  time: 0.0517  data: 0.0001  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5938 (0.9350)  labels_encoder: 0.5938 (0.9350)  labels_encoder_unscaled: 0.5938 (0.9350)  time: 0.0488  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7113 (0.9101)  labels_encoder: 0.7113 (0.9101)  labels_encoder_unscaled: 0.7113 (0.9101)  time: 0.0492  data: 0.0024  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3554 (0.9036)  labels_encoder: 0.3554 (0.9036)  labels_encoder_unscaled: 0.3554 (0.9036)  time: 0.0451  data: 0.0001  max mem: 1206
Test: Total time: 0:00:33 (0.0599 s / it)
Averaged stats: loss: 0.3554 (0.9036)  labels_encoder: 0.3554 (0.9036)  labels_encoder_unscaled: 0.3554 (0.9036)
(21, 71496)
(21, 71496)
[Epoch-2] [IDU-tvseries_anet_features.pickle] mAP: 0.1259, mcAP: 0.8815

BaseballPitch: 0.0470
BasketballDunk: 0.1031
Billiards: 0.0045
CleanAndJerk: 0.3768
CliffDiving: 0.4737
CricketBowling: 0.0547
CricketShot: 0.1051
Diving: 0.0116
FrisbeeCatch: 0.1213
GolfSwing: 0.0562
HammerThrow: 0.1002
HighJump: 0.0457
JavelinThrow: 0.0562
LongJump: 0.2941
PoleVault: 0.1102
Shotput: 0.1332
SoccerPenalty: 0.0514
TennisSwing: 0.1737
ThrowDiscus: 0.0511
VolleyballSpiking: 0.1488
Epoch: [3]  [   0/1510]  eta: 0:44:19  lr: 0.000001  loss: 0.4269 (0.4269)  labels_encoder: 0.4269 (0.4269)  labels_encoder_unscaled: 0.4269 (0.4269)  time: 1.7612  data: 1.5898  max mem: 1206
Epoch: [3]  [  50/1510]  eta: 0:03:51  lr: 0.000001  loss: 0.5151 (0.5476)  labels_encoder: 0.5151 (0.5476)  labels_encoder_unscaled: 0.5151 (0.5476)  time: 0.1213  data: 0.0002  max mem: 1206
Epoch: [3]  [ 100/1510]  eta: 0:03:27  lr: 0.000001  loss: 0.5187 (0.5377)  labels_encoder: 0.5187 (0.5377)  labels_encoder_unscaled: 0.5187 (0.5377)  time: 0.1383  data: 0.0002  max mem: 1206
Epoch: [3]  [ 150/1510]  eta: 0:03:15  lr: 0.000001  loss: 0.5292 (0.5304)  labels_encoder: 0.5292 (0.5304)  labels_encoder_unscaled: 0.5292 (0.5304)  time: 0.1353  data: 0.0002  max mem: 1206
Epoch: [3]  [ 200/1510]  eta: 0:03:06  lr: 0.000001  loss: 0.4915 (0.5361)  labels_encoder: 0.4915 (0.5361)  labels_encoder_unscaled: 0.4915 (0.5361)  time: 0.1350  data: 0.0002  max mem: 1206
Epoch: [3]  [ 250/1510]  eta: 0:02:58  lr: 0.000001  loss: 0.5025 (0.5341)  labels_encoder: 0.5025 (0.5341)  labels_encoder_unscaled: 0.5025 (0.5341)  time: 0.1379  data: 0.0002  max mem: 1206
Epoch: [3]  [ 300/1510]  eta: 0:02:51  lr: 0.000001  loss: 0.5138 (0.5343)  labels_encoder: 0.5138 (0.5343)  labels_encoder_unscaled: 0.5138 (0.5343)  time: 0.1380  data: 0.0002  max mem: 1206
Epoch: [3]  [ 350/1510]  eta: 0:02:43  lr: 0.000001  loss: 0.4883 (0.5317)  labels_encoder: 0.4883 (0.5317)  labels_encoder_unscaled: 0.4883 (0.5317)  time: 0.1407  data: 0.0002  max mem: 1206
Epoch: [3]  [ 400/1510]  eta: 0:02:36  lr: 0.000001  loss: 0.4894 (0.5311)  labels_encoder: 0.4894 (0.5311)  labels_encoder_unscaled: 0.4894 (0.5311)  time: 0.1382  data: 0.0002  max mem: 1206
Epoch: [3]  [ 450/1510]  eta: 0:02:29  lr: 0.000001  loss: 0.5456 (0.5304)  labels_encoder: 0.5456 (0.5304)  labels_encoder_unscaled: 0.5456 (0.5304)  time: 0.1398  data: 0.0002  max mem: 1206
Epoch: [3]  [ 500/1510]  eta: 0:02:22  lr: 0.000001  loss: 0.5380 (0.5298)  labels_encoder: 0.5380 (0.5298)  labels_encoder_unscaled: 0.5380 (0.5298)  time: 0.1404  data: 0.0002  max mem: 1206
Epoch: [3]  [ 550/1510]  eta: 0:02:15  lr: 0.000001  loss: 0.5741 (0.5302)  labels_encoder: 0.5741 (0.5302)  labels_encoder_unscaled: 0.5741 (0.5302)  time: 0.1406  data: 0.0003  max mem: 1206
Epoch: [3]  [ 600/1510]  eta: 0:02:08  lr: 0.000001  loss: 0.5532 (0.5310)  labels_encoder: 0.5532 (0.5310)  labels_encoder_unscaled: 0.5532 (0.5310)  time: 0.1411  data: 0.0002  max mem: 1206
Epoch: [3]  [ 650/1510]  eta: 0:02:01  lr: 0.000001  loss: 0.5033 (0.5303)  labels_encoder: 0.5033 (0.5303)  labels_encoder_unscaled: 0.5033 (0.5303)  time: 0.1429  data: 0.0002  max mem: 1206
Epoch: [3]  [ 700/1510]  eta: 0:01:54  lr: 0.000001  loss: 0.5352 (0.5311)  labels_encoder: 0.5352 (0.5311)  labels_encoder_unscaled: 0.5352 (0.5311)  time: 0.1416  data: 0.0002  max mem: 1206
Epoch: [3]  [ 750/1510]  eta: 0:01:47  lr: 0.000001  loss: 0.5170 (0.5302)  labels_encoder: 0.5170 (0.5302)  labels_encoder_unscaled: 0.5170 (0.5302)  time: 0.1366  data: 0.0002  max mem: 1206
Epoch: [3]  [ 800/1510]  eta: 0:01:40  lr: 0.000001  loss: 0.5177 (0.5297)  labels_encoder: 0.5177 (0.5297)  labels_encoder_unscaled: 0.5177 (0.5297)  time: 0.1425  data: 0.0002  max mem: 1206
Epoch: [3]  [ 850/1510]  eta: 0:01:33  lr: 0.000001  loss: 0.5345 (0.5291)  labels_encoder: 0.5345 (0.5291)  labels_encoder_unscaled: 0.5345 (0.5291)  time: 0.1417  data: 0.0002  max mem: 1206
Epoch: [3]  [ 900/1510]  eta: 0:01:26  lr: 0.000001  loss: 0.4751 (0.5284)  labels_encoder: 0.4751 (0.5284)  labels_encoder_unscaled: 0.4751 (0.5284)  time: 0.1396  data: 0.0002  max mem: 1206
Epoch: [3]  [ 950/1510]  eta: 0:01:18  lr: 0.000001  loss: 0.5243 (0.5286)  labels_encoder: 0.5243 (0.5286)  labels_encoder_unscaled: 0.5243 (0.5286)  time: 0.1388  data: 0.0002  max mem: 1206
Epoch: [3]  [1000/1510]  eta: 0:01:11  lr: 0.000001  loss: 0.4919 (0.5283)  labels_encoder: 0.4919 (0.5283)  labels_encoder_unscaled: 0.4919 (0.5283)  time: 0.1412  data: 0.0002  max mem: 1206
Epoch: [3]  [1050/1510]  eta: 0:01:04  lr: 0.000001  loss: 0.5152 (0.5277)  labels_encoder: 0.5152 (0.5277)  labels_encoder_unscaled: 0.5152 (0.5277)  time: 0.1335  data: 0.0002  max mem: 1206
Epoch: [3]  [1100/1510]  eta: 0:00:57  lr: 0.000001  loss: 0.5549 (0.5276)  labels_encoder: 0.5549 (0.5276)  labels_encoder_unscaled: 0.5549 (0.5276)  time: 0.1323  data: 0.0002  max mem: 1206
Epoch: [3]  [1150/1510]  eta: 0:00:50  lr: 0.000001  loss: 0.5301 (0.5269)  labels_encoder: 0.5301 (0.5269)  labels_encoder_unscaled: 0.5301 (0.5269)  time: 0.1319  data: 0.0002  max mem: 1206
Epoch: [3]  [1200/1510]  eta: 0:00:43  lr: 0.000001  loss: 0.5164 (0.5270)  labels_encoder: 0.5164 (0.5270)  labels_encoder_unscaled: 0.5164 (0.5270)  time: 0.1308  data: 0.0002  max mem: 1206
Epoch: [3]  [1250/1510]  eta: 0:00:36  lr: 0.000001  loss: 0.5598 (0.5283)  labels_encoder: 0.5598 (0.5283)  labels_encoder_unscaled: 0.5598 (0.5283)  time: 0.1322  data: 0.0002  max mem: 1206
Epoch: [3]  [1300/1510]  eta: 0:00:29  lr: 0.000001  loss: 0.5083 (0.5279)  labels_encoder: 0.5083 (0.5279)  labels_encoder_unscaled: 0.5083 (0.5279)  time: 0.1315  data: 0.0002  max mem: 1206
Epoch: [3]  [1350/1510]  eta: 0:00:22  lr: 0.000001  loss: 0.5040 (0.5275)  labels_encoder: 0.5040 (0.5275)  labels_encoder_unscaled: 0.5040 (0.5275)  time: 0.1435  data: 0.0002  max mem: 1206
Epoch: [3]  [1400/1510]  eta: 0:00:15  lr: 0.000001  loss: 0.4826 (0.5276)  labels_encoder: 0.4826 (0.5276)  labels_encoder_unscaled: 0.4826 (0.5276)  time: 0.1452  data: 0.0002  max mem: 1206
Epoch: [3]  [1450/1510]  eta: 0:00:08  lr: 0.000001  loss: 0.4805 (0.5274)  labels_encoder: 0.4805 (0.5274)  labels_encoder_unscaled: 0.4805 (0.5274)  time: 0.1445  data: 0.0002  max mem: 1206
Epoch: [3]  [1500/1510]  eta: 0:00:01  lr: 0.000001  loss: 0.4944 (0.5271)  labels_encoder: 0.4944 (0.5271)  labels_encoder_unscaled: 0.4944 (0.5271)  time: 0.1359  data: 0.0003  max mem: 1206
Epoch: [3]  [1509/1510]  eta: 0:00:00  lr: 0.000001  loss: 0.5242 (0.5272)  labels_encoder: 0.5242 (0.5272)  labels_encoder_unscaled: 0.5242 (0.5272)  time: 0.1237  data: 0.0003  max mem: 1206
Epoch: [3] Total time: 0:03:30 (0.1395 s / it)
Averaged stats: lr: 0.000001  loss: 0.5242 (0.5272)  labels_encoder: 0.5242 (0.5272)  labels_encoder_unscaled: 0.5242 (0.5272)
Test:  [  0/559]  eta: 0:13:41  loss: 0.5443 (0.5443)  labels_encoder: 0.5443 (0.5443)  labels_encoder_unscaled: 0.5443 (0.5443)  time: 1.4693  data: 1.3975  max mem: 1206
Test:  [ 50/559]  eta: 0:00:45  loss: 0.7749 (0.8750)  labels_encoder: 0.7749 (0.8750)  labels_encoder_unscaled: 0.7749 (0.8750)  time: 0.0586  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:34  loss: 0.5967 (0.8465)  labels_encoder: 0.5967 (0.8465)  labels_encoder_unscaled: 0.5967 (0.8465)  time: 0.0613  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.5094 (0.8682)  labels_encoder: 0.5094 (0.8682)  labels_encoder_unscaled: 0.5094 (0.8682)  time: 0.0562  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.6291 (0.8110)  labels_encoder: 0.6291 (0.8110)  labels_encoder_unscaled: 0.6291 (0.8110)  time: 0.0576  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.6907 (0.9238)  labels_encoder: 0.6907 (0.9238)  labels_encoder_unscaled: 0.6907 (0.9238)  time: 0.0594  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.0742 (0.9806)  labels_encoder: 1.0742 (0.9806)  labels_encoder_unscaled: 1.0742 (0.9806)  time: 0.0553  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7504 (1.0044)  labels_encoder: 0.7504 (1.0044)  labels_encoder_unscaled: 0.7504 (1.0044)  time: 0.0562  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3453 (0.9769)  labels_encoder: 0.3453 (0.9769)  labels_encoder_unscaled: 0.3453 (0.9769)  time: 0.0637  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4314 (0.9518)  labels_encoder: 0.4314 (0.9518)  labels_encoder_unscaled: 0.4314 (0.9518)  time: 0.0641  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6159 (0.9319)  labels_encoder: 0.6159 (0.9319)  labels_encoder_unscaled: 0.6159 (0.9319)  time: 0.0554  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7276 (0.9068)  labels_encoder: 0.7276 (0.9068)  labels_encoder_unscaled: 0.7276 (0.9068)  time: 0.0512  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3707 (0.9003)  labels_encoder: 0.3707 (0.9003)  labels_encoder_unscaled: 0.3707 (0.9003)  time: 0.0423  data: 0.0001  max mem: 1206
Test: Total time: 0:00:34 (0.0622 s / it)
Averaged stats: loss: 0.3707 (0.9003)  labels_encoder: 0.3707 (0.9003)  labels_encoder_unscaled: 0.3707 (0.9003)
(21, 71496)
(21, 71496)
[Epoch-3] [IDU-tvseries_anet_features.pickle] mAP: 0.1295, mcAP: 0.8822

BaseballPitch: 0.0443
BasketballDunk: 0.1157
Billiards: 0.0045
CleanAndJerk: 0.4184
CliffDiving: 0.3967
CricketBowling: 0.0504
CricketShot: 0.0884
Diving: 0.0072
FrisbeeCatch: 0.1097
GolfSwing: 0.0496
HammerThrow: 0.1119
HighJump: 0.0635
JavelinThrow: 0.0574
LongJump: 0.3105
PoleVault: 0.1002
Shotput: 0.1383
SoccerPenalty: 0.0503
TennisSwing: 0.1750
ThrowDiscus: 0.1469
VolleyballSpiking: 0.1513
Epoch: [4]  [   0/1510]  eta: 0:44:16  lr: 0.000000  loss: 0.5129 (0.5129)  labels_encoder: 0.5129 (0.5129)  labels_encoder_unscaled: 0.5129 (0.5129)  time: 1.7593  data: 1.6327  max mem: 1206
Epoch: [4]  [  50/1510]  eta: 0:04:13  lr: 0.000000  loss: 0.5303 (0.5265)  labels_encoder: 0.5303 (0.5265)  labels_encoder_unscaled: 0.5303 (0.5265)  time: 0.1363  data: 0.0002  max mem: 1206
Epoch: [4]  [ 100/1510]  eta: 0:03:39  lr: 0.000000  loss: 0.5415 (0.5287)  labels_encoder: 0.5415 (0.5287)  labels_encoder_unscaled: 0.5415 (0.5287)  time: 0.1370  data: 0.0002  max mem: 1206
Epoch: [4]  [ 150/1510]  eta: 0:03:17  lr: 0.000000  loss: 0.4372 (0.5192)  labels_encoder: 0.4372 (0.5192)  labels_encoder_unscaled: 0.4372 (0.5192)  time: 0.1243  data: 0.0002  max mem: 1206
Epoch: [4]  [ 200/1510]  eta: 0:03:05  lr: 0.000000  loss: 0.4567 (0.5164)  labels_encoder: 0.4567 (0.5164)  labels_encoder_unscaled: 0.4567 (0.5164)  time: 0.1320  data: 0.0002  max mem: 1206
Epoch: [4]  [ 250/1510]  eta: 0:02:56  lr: 0.000000  loss: 0.4977 (0.5207)  labels_encoder: 0.4977 (0.5207)  labels_encoder_unscaled: 0.4977 (0.5207)  time: 0.1333  data: 0.0002  max mem: 1206
Epoch: [4]  [ 300/1510]  eta: 0:02:49  lr: 0.000000  loss: 0.5223 (0.5225)  labels_encoder: 0.5223 (0.5225)  labels_encoder_unscaled: 0.5223 (0.5225)  time: 0.1436  data: 0.0002  max mem: 1206
Epoch: [4]  [ 350/1510]  eta: 0:02:42  lr: 0.000000  loss: 0.5226 (0.5199)  labels_encoder: 0.5226 (0.5199)  labels_encoder_unscaled: 0.5226 (0.5199)  time: 0.1406  data: 0.0002  max mem: 1206
Epoch: [4]  [ 400/1510]  eta: 0:02:36  lr: 0.000000  loss: 0.5327 (0.5199)  labels_encoder: 0.5327 (0.5199)  labels_encoder_unscaled: 0.5327 (0.5199)  time: 0.1416  data: 0.0002  max mem: 1206
Epoch: [4]  [ 450/1510]  eta: 0:02:29  lr: 0.000000  loss: 0.4857 (0.5202)  labels_encoder: 0.4857 (0.5202)  labels_encoder_unscaled: 0.4857 (0.5202)  time: 0.1426  data: 0.0002  max mem: 1206
Epoch: [4]  [ 500/1510]  eta: 0:02:22  lr: 0.000000  loss: 0.4921 (0.5206)  labels_encoder: 0.4921 (0.5206)  labels_encoder_unscaled: 0.4921 (0.5206)  time: 0.1389  data: 0.0002  max mem: 1206
Epoch: [4]  [ 550/1510]  eta: 0:02:14  lr: 0.000000  loss: 0.5120 (0.5206)  labels_encoder: 0.5120 (0.5206)  labels_encoder_unscaled: 0.5120 (0.5206)  time: 0.1401  data: 0.0002  max mem: 1206
Epoch: [4]  [ 600/1510]  eta: 0:02:07  lr: 0.000000  loss: 0.4592 (0.5206)  labels_encoder: 0.4592 (0.5206)  labels_encoder_unscaled: 0.4592 (0.5206)  time: 0.1399  data: 0.0002  max mem: 1206
Epoch: [4]  [ 650/1510]  eta: 0:02:00  lr: 0.000000  loss: 0.4786 (0.5212)  labels_encoder: 0.4786 (0.5212)  labels_encoder_unscaled: 0.4786 (0.5212)  time: 0.1420  data: 0.0002  max mem: 1206
Epoch: [4]  [ 700/1510]  eta: 0:01:53  lr: 0.000000  loss: 0.4963 (0.5219)  labels_encoder: 0.4963 (0.5219)  labels_encoder_unscaled: 0.4963 (0.5219)  time: 0.1343  data: 0.0002  max mem: 1206
Epoch: [4]  [ 750/1510]  eta: 0:01:46  lr: 0.000000  loss: 0.5437 (0.5217)  labels_encoder: 0.5437 (0.5217)  labels_encoder_unscaled: 0.5437 (0.5217)  time: 0.1428  data: 0.0002  max mem: 1206
Epoch: [4]  [ 800/1510]  eta: 0:01:39  lr: 0.000000  loss: 0.5041 (0.5223)  labels_encoder: 0.5041 (0.5223)  labels_encoder_unscaled: 0.5041 (0.5223)  time: 0.1309  data: 0.0002  max mem: 1206
Epoch: [4]  [ 850/1510]  eta: 0:01:32  lr: 0.000000  loss: 0.4731 (0.5217)  labels_encoder: 0.4731 (0.5217)  labels_encoder_unscaled: 0.4731 (0.5217)  time: 0.1424  data: 0.0002  max mem: 1206
Epoch: [4]  [ 900/1510]  eta: 0:01:25  lr: 0.000000  loss: 0.5321 (0.5228)  labels_encoder: 0.5321 (0.5228)  labels_encoder_unscaled: 0.5321 (0.5228)  time: 0.1436  data: 0.0002  max mem: 1206
Epoch: [4]  [ 950/1510]  eta: 0:01:18  lr: 0.000000  loss: 0.4917 (0.5231)  labels_encoder: 0.4917 (0.5231)  labels_encoder_unscaled: 0.4917 (0.5231)  time: 0.1422  data: 0.0002  max mem: 1206
Epoch: [4]  [1000/1510]  eta: 0:01:11  lr: 0.000000  loss: 0.5497 (0.5240)  labels_encoder: 0.5497 (0.5240)  labels_encoder_unscaled: 0.5497 (0.5240)  time: 0.1261  data: 0.0002  max mem: 1206
Epoch: [4]  [1050/1510]  eta: 0:01:03  lr: 0.000000  loss: 0.4480 (0.5233)  labels_encoder: 0.4480 (0.5233)  labels_encoder_unscaled: 0.4480 (0.5233)  time: 0.1267  data: 0.0002  max mem: 1206
Epoch: [4]  [1100/1510]  eta: 0:00:56  lr: 0.000000  loss: 0.5176 (0.5225)  labels_encoder: 0.5176 (0.5225)  labels_encoder_unscaled: 0.5176 (0.5225)  time: 0.1271  data: 0.0002  max mem: 1206
Epoch: [4]  [1150/1510]  eta: 0:00:49  lr: 0.000000  loss: 0.5008 (0.5221)  labels_encoder: 0.5008 (0.5221)  labels_encoder_unscaled: 0.5008 (0.5221)  time: 0.1338  data: 0.0002  max mem: 1206
Epoch: [4]  [1200/1510]  eta: 0:00:42  lr: 0.000000  loss: 0.5066 (0.5221)  labels_encoder: 0.5066 (0.5221)  labels_encoder_unscaled: 0.5066 (0.5221)  time: 0.1311  data: 0.0002  max mem: 1206
Epoch: [4]  [1250/1510]  eta: 0:00:35  lr: 0.000000  loss: 0.4950 (0.5217)  labels_encoder: 0.4950 (0.5217)  labels_encoder_unscaled: 0.4950 (0.5217)  time: 0.1448  data: 0.0002  max mem: 1206
Epoch: [4]  [1300/1510]  eta: 0:00:28  lr: 0.000000  loss: 0.4761 (0.5214)  labels_encoder: 0.4761 (0.5214)  labels_encoder_unscaled: 0.4761 (0.5214)  time: 0.1385  data: 0.0002  max mem: 1206
Epoch: [4]  [1350/1510]  eta: 0:00:22  lr: 0.000000  loss: 0.5273 (0.5213)  labels_encoder: 0.5273 (0.5213)  labels_encoder_unscaled: 0.5273 (0.5213)  time: 0.1434  data: 0.0002  max mem: 1206
Epoch: [4]  [1400/1510]  eta: 0:00:15  lr: 0.000000  loss: 0.5116 (0.5209)  labels_encoder: 0.5116 (0.5209)  labels_encoder_unscaled: 0.5116 (0.5209)  time: 0.1450  data: 0.0002  max mem: 1206
Epoch: [4]  [1450/1510]  eta: 0:00:08  lr: 0.000000  loss: 0.5531 (0.5207)  labels_encoder: 0.5531 (0.5207)  labels_encoder_unscaled: 0.5531 (0.5207)  time: 0.1436  data: 0.0002  max mem: 1206
Epoch: [4]  [1500/1510]  eta: 0:00:01  lr: 0.000000  loss: 0.5290 (0.5212)  labels_encoder: 0.5290 (0.5212)  labels_encoder_unscaled: 0.5290 (0.5212)  time: 0.1217  data: 0.0002  max mem: 1206
Epoch: [4]  [1509/1510]  eta: 0:00:00  lr: 0.000000  loss: 0.4624 (0.5209)  labels_encoder: 0.4624 (0.5209)  labels_encoder_unscaled: 0.4624 (0.5209)  time: 0.1107  data: 0.0002  max mem: 1206
Epoch: [4] Total time: 0:03:28 (0.1379 s / it)
Averaged stats: lr: 0.000000  loss: 0.4624 (0.5209)  labels_encoder: 0.4624 (0.5209)  labels_encoder_unscaled: 0.4624 (0.5209)
Test:  [  0/559]  eta: 0:14:35  loss: 0.5741 (0.5741)  labels_encoder: 0.5741 (0.5741)  labels_encoder_unscaled: 0.5741 (0.5741)  time: 1.5664  data: 1.5318  max mem: 1206
Test:  [ 50/559]  eta: 0:00:49  loss: 0.8753 (0.9067)  labels_encoder: 0.8753 (0.9067)  labels_encoder_unscaled: 0.8753 (0.9067)  time: 0.0650  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:36  loss: 0.5229 (0.8485)  labels_encoder: 0.5229 (0.8485)  labels_encoder_unscaled: 0.5229 (0.8485)  time: 0.0642  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:30  loss: 0.5213 (0.8711)  labels_encoder: 0.5213 (0.8711)  labels_encoder_unscaled: 0.5213 (0.8711)  time: 0.0619  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:25  loss: 0.5746 (0.8136)  labels_encoder: 0.5746 (0.8136)  labels_encoder_unscaled: 0.5746 (0.8136)  time: 0.0585  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.7055 (0.9275)  labels_encoder: 0.7055 (0.9275)  labels_encoder_unscaled: 0.7055 (0.9275)  time: 0.0564  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.2040 (0.9811)  labels_encoder: 1.2040 (0.9811)  labels_encoder_unscaled: 1.2040 (0.9811)  time: 0.0645  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7764 (1.0049)  labels_encoder: 0.7764 (1.0049)  labels_encoder_unscaled: 0.7764 (1.0049)  time: 0.0620  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3285 (0.9786)  labels_encoder: 0.3285 (0.9786)  labels_encoder_unscaled: 0.3285 (0.9786)  time: 0.0612  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:07  loss: 0.4615 (0.9525)  labels_encoder: 0.4615 (0.9525)  labels_encoder_unscaled: 0.4615 (0.9525)  time: 0.0639  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6266 (0.9321)  labels_encoder: 0.6266 (0.9321)  labels_encoder_unscaled: 0.6266 (0.9321)  time: 0.0634  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7245 (0.9069)  labels_encoder: 0.7245 (0.9069)  labels_encoder_unscaled: 0.7245 (0.9069)  time: 0.0537  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3609 (0.9006)  labels_encoder: 0.3609 (0.9006)  labels_encoder_unscaled: 0.3609 (0.9006)  time: 0.0406  data: 0.0001  max mem: 1206
Test: Total time: 0:00:36 (0.0644 s / it)
Averaged stats: loss: 0.3609 (0.9006)  labels_encoder: 0.3609 (0.9006)  labels_encoder_unscaled: 0.3609 (0.9006)
(21, 71496)
(21, 71496)
[Epoch-4] [IDU-tvseries_anet_features.pickle] mAP: 0.1283, mcAP: 0.8813

BaseballPitch: 0.0644
BasketballDunk: 0.1072
Billiards: 0.0045
CleanAndJerk: 0.4044
CliffDiving: 0.4199
CricketBowling: 0.0547
CricketShot: 0.0980
Diving: 0.0075
FrisbeeCatch: 0.1141
GolfSwing: 0.0562
HammerThrow: 0.1223
HighJump: 0.0486
JavelinThrow: 0.0537
LongJump: 0.3115
PoleVault: 0.0981
Shotput: 0.1361
SoccerPenalty: 0.0513
TennisSwing: 0.1623
ThrowDiscus: 0.0959
VolleyballSpiking: 0.1558
Training time 0:16:04
