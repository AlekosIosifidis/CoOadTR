Unable to compile CoConv C++ implementation. Falling back to Python version.
[Errno 2] No such file or directory: '/home/lh/.conda/envs/oadtr/lib/python3.8/site-packages/continual/conv.cpp'
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'conv_flops_counter_hook'
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'MODULES_MAPPING'
Not using distributed mode
lr:0.0001
batch_size:128
weight_decay:0.0001
epochs:5
resize_feature:False
lr_drop:1
clip_max_norm:1.0
dataparallel:False
removelog:False
version:v3
query_num:8
decoder_layers:5
decoder_embedding_dim:1024
decoder_embedding_dim_out:1024
decoder_attn_dropout_rate:0.1
decoder_num_heads:4
classification_pred_loss_coef:0.5
enc_layers:64
lr_backbone:0.0001
feature:tvseries_kin_features.pickle
dim_feature:4096
patch_dim:1
embedding_dim:1024
num_heads:8
num_layers:2
attn_dropout_rate:0.1
positional_encoding_type:recycling_fixed
num_embeddings:127
hidden_dim:1024
dropout_rate:0.1
numclass:31
classification_x_loss_coef:0.3
classification_h_loss_coef:1
similar_loss_coef:0.1
margin:1.0
dataset:tvseries
dataset_file:data/data_info_new.json
frozen_weights:None
thumos_data_path:/home/dancer/mycode/Temporal.Online.Detection/Online.TRN.Pytorch/preprocess/
thumos_anno_path:data/thumos_{}_anno.pickle
remove_difficult:False
device:cuda
output_dir:models
seed:2
resume:
start_epoch:1
eval:False
num_workers:8
world_size:1
dist_url:tcp://127.0.0.1:12342
train_session_set:['24_ep1', '24_ep2', '24_ep3', 'Breaking_Bad_ep1', 'Breaking_Bad_ep2', 'How_I_Met_Your_Mother_ep1', 'How_I_Met_Your_Mother_ep2', 'How_I_Met_Your_Mother_ep3', 'How_I_Met_Your_Mother_ep4', 'How_I_Met_Your_Mother_ep5', 'How_I_Met_Your_Mother_ep6', 'Mad_Men_ep1', 'Mad_Men_ep2', 'Modern_Family_ep1', 'Modern_Family_ep2', 'Modern_Family_ep3', 'Modern_Family_ep4', 'Modern_Family_ep6', 'Sons_of_Anarchy_ep1', 'Sons_of_Anarchy_ep2']
test_session_set:['24_ep4', 'Breaking_Bad_ep3', 'Mad_Men_ep3', 'How_I_Met_Your_Mother_ep7', 'How_I_Met_Your_Mother_ep8', 'Modern_Family_ep5', 'Sons_of_Anarchy_ep3']
class_index:['background', 'Pick something up', 'Point', 'Drink', 'Stand up', 'Run', 'Sit down', 'Read', 'Smoke', 'Drive car', 'Open door', 'Give something', 'Use computer', 'Write', 'Go down stairway', 'Close door', 'Throw something', 'Go up stairway', 'Get in/out of car', 'Hang up phone', 'Eat', 'Answer phone', 'Dress up', 'Clap', 'Undress', 'Kiss', 'Fall/trip', 'Wave', 'Pour', 'Punch', 'Fire weapon']
distributed:False
position encoding : recycling_fixed
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Sequential(
  16.814 M, 99.939% Params, 0.412 GMac, 100.000% MACs, 
  (0): Linear(4.195 M, 24.936% Params, 0.004 GMac, 1.018% MACs, in_features=4096, out_features=1024, bias=True, channel_dim=1)
  (1): RecyclingPositionalEncoding(
    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 
    (pe): CyclicPositionalEncoding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
  )
  (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
  (3): Sequential(
    12.587 M, 74.814% Params, 0.408 GMac, 98.974% MACs, 
    (0): Sequential(
      6.294 M, 37.407% Params, 0.205 GMac, 49.804% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, reduce=reduce_sum
        (0): RetroactiveUnity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 63)
        (1): CoReMultiheadAttention(
          4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(Sequential(
        2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
        (0): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
        (1): Residual(
          2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
          (fn): Sequential(
            2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
            (0): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
            (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
            (3): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          )
        )
        (2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
      ))
    )
    (1): Lambda(Sequential(
      6.294 M, 37.407% Params, 0.203 GMac, 49.170% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, reduce=sum_last_pairs
        (0): SelectOrDelay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): CoSiMultiheadAttention(
          4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
      (2): BroadcastReduce(
        2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, reduce=reduce_sum
        (0): Delay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): Sequential(
          2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, 
          (0): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
          (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          (3): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
        )
      )
      (3): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
    ), takes_time=True)
    (2): Lambda(unity, squeeze_last, squeeze_last, takes_time=True)
  )
  (4): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
  (5): Linear(0.032 M, 0.189% Params, 0.0 GMac, 0.008% MACs, in_features=1024, out_features=31, bias=True, channel_dim=1)
)
Model FLOPs: 411997217.0
Model params: 16824351
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| Active memory         |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   83968 KB |   83968 KB |   83968 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16445 KB |   16445 KB |  302954 KB |  286508 KB |
|---------------------------------------------------------------------------|
| Allocations           |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| Active allocs         |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |    1458    |    1448    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Memory state pre, max, post inference: 69143040 71518720 69143040
Loaded tvseries_kin_features.pickle
Loaded tvseries_kin_features.pickle
Start training
Epoch: [1]  [   0/1510]  eta: 1:17:51  lr: 0.000100  loss: 3.6220 (3.6220)  labels_encoder: 3.6220 (3.6220)  labels_encoder_unscaled: 3.6220 (3.6220)  time: 3.0937  data: 2.4397  max mem: 1013
Epoch: [1]  [  50/1510]  eta: 0:04:34  lr: 0.000100  loss: 0.9698 (1.1477)  labels_encoder: 0.9698 (1.1477)  labels_encoder_unscaled: 0.9698 (1.1477)  time: 0.1146  data: 0.0002  max mem: 1206
Epoch: [1]  [ 100/1510]  eta: 0:03:40  lr: 0.000100  loss: 1.0103 (1.0799)  labels_encoder: 1.0103 (1.0799)  labels_encoder_unscaled: 1.0103 (1.0799)  time: 0.1314  data: 0.0002  max mem: 1206
Epoch: [1]  [ 150/1510]  eta: 0:03:21  lr: 0.000100  loss: 0.8764 (1.0245)  labels_encoder: 0.8764 (1.0245)  labels_encoder_unscaled: 0.8764 (1.0245)  time: 0.1335  data: 0.0002  max mem: 1206
Epoch: [1]  [ 200/1510]  eta: 0:03:08  lr: 0.000100  loss: 0.8704 (1.0005)  labels_encoder: 0.8704 (1.0005)  labels_encoder_unscaled: 0.8704 (1.0005)  time: 0.1354  data: 0.0002  max mem: 1206
Epoch: [1]  [ 250/1510]  eta: 0:02:59  lr: 0.000100  loss: 0.8296 (0.9700)  labels_encoder: 0.8296 (0.9700)  labels_encoder_unscaled: 0.8296 (0.9700)  time: 0.1330  data: 0.0002  max mem: 1206
Epoch: [1]  [ 300/1510]  eta: 0:02:50  lr: 0.000100  loss: 0.8541 (0.9534)  labels_encoder: 0.8541 (0.9534)  labels_encoder_unscaled: 0.8541 (0.9534)  time: 0.1234  data: 0.0002  max mem: 1206
Epoch: [1]  [ 350/1510]  eta: 0:02:42  lr: 0.000100  loss: 0.8257 (0.9357)  labels_encoder: 0.8257 (0.9357)  labels_encoder_unscaled: 0.8257 (0.9357)  time: 0.1358  data: 0.0002  max mem: 1206
Epoch: [1]  [ 400/1510]  eta: 0:02:34  lr: 0.000100  loss: 0.7901 (0.9266)  labels_encoder: 0.7901 (0.9266)  labels_encoder_unscaled: 0.7901 (0.9266)  time: 0.1327  data: 0.0002  max mem: 1206
Epoch: [1]  [ 450/1510]  eta: 0:02:26  lr: 0.000100  loss: 0.7391 (0.9128)  labels_encoder: 0.7391 (0.9128)  labels_encoder_unscaled: 0.7391 (0.9128)  time: 0.1356  data: 0.0003  max mem: 1206
Epoch: [1]  [ 500/1510]  eta: 0:02:19  lr: 0.000100  loss: 0.7847 (0.9018)  labels_encoder: 0.7847 (0.9018)  labels_encoder_unscaled: 0.7847 (0.9018)  time: 0.1322  data: 0.0002  max mem: 1206
Epoch: [1]  [ 550/1510]  eta: 0:02:12  lr: 0.000100  loss: 0.6958 (0.8876)  labels_encoder: 0.6958 (0.8876)  labels_encoder_unscaled: 0.6958 (0.8876)  time: 0.1350  data: 0.0002  max mem: 1206
Epoch: [1]  [ 600/1510]  eta: 0:02:06  lr: 0.000100  loss: 0.7306 (0.8736)  labels_encoder: 0.7306 (0.8736)  labels_encoder_unscaled: 0.7306 (0.8736)  time: 0.1398  data: 0.0002  max mem: 1206
Epoch: [1]  [ 650/1510]  eta: 0:01:58  lr: 0.000100  loss: 0.7313 (0.8668)  labels_encoder: 0.7313 (0.8668)  labels_encoder_unscaled: 0.7313 (0.8668)  time: 0.1305  data: 0.0002  max mem: 1206
Epoch: [1]  [ 700/1510]  eta: 0:01:51  lr: 0.000100  loss: 0.7318 (0.8581)  labels_encoder: 0.7318 (0.8581)  labels_encoder_unscaled: 0.7318 (0.8581)  time: 0.1353  data: 0.0002  max mem: 1206
Epoch: [1]  [ 750/1510]  eta: 0:01:44  lr: 0.000100  loss: 0.7260 (0.8514)  labels_encoder: 0.7260 (0.8514)  labels_encoder_unscaled: 0.7260 (0.8514)  time: 0.1298  data: 0.0002  max mem: 1206
Epoch: [1]  [ 800/1510]  eta: 0:01:37  lr: 0.000100  loss: 0.7084 (0.8438)  labels_encoder: 0.7084 (0.8438)  labels_encoder_unscaled: 0.7084 (0.8438)  time: 0.1414  data: 0.0002  max mem: 1206
Epoch: [1]  [ 850/1510]  eta: 0:01:30  lr: 0.000100  loss: 0.6984 (0.8388)  labels_encoder: 0.6984 (0.8388)  labels_encoder_unscaled: 0.6984 (0.8388)  time: 0.1299  data: 0.0002  max mem: 1206
Epoch: [1]  [ 900/1510]  eta: 0:01:23  lr: 0.000100  loss: 0.6991 (0.8311)  labels_encoder: 0.6991 (0.8311)  labels_encoder_unscaled: 0.6991 (0.8311)  time: 0.1442  data: 0.0002  max mem: 1206
Epoch: [1]  [ 950/1510]  eta: 0:01:16  lr: 0.000100  loss: 0.6906 (0.8251)  labels_encoder: 0.6906 (0.8251)  labels_encoder_unscaled: 0.6906 (0.8251)  time: 0.1274  data: 0.0002  max mem: 1206
Epoch: [1]  [1000/1510]  eta: 0:01:09  lr: 0.000100  loss: 0.7048 (0.8188)  labels_encoder: 0.7048 (0.8188)  labels_encoder_unscaled: 0.7048 (0.8188)  time: 0.1282  data: 0.0002  max mem: 1206
Epoch: [1]  [1050/1510]  eta: 0:01:02  lr: 0.000100  loss: 0.7216 (0.8142)  labels_encoder: 0.7216 (0.8142)  labels_encoder_unscaled: 0.7216 (0.8142)  time: 0.1361  data: 0.0002  max mem: 1206
Epoch: [1]  [1100/1510]  eta: 0:00:55  lr: 0.000100  loss: 0.6800 (0.8093)  labels_encoder: 0.6800 (0.8093)  labels_encoder_unscaled: 0.6800 (0.8093)  time: 0.1275  data: 0.0002  max mem: 1206
Epoch: [1]  [1150/1510]  eta: 0:00:49  lr: 0.000100  loss: 0.7418 (0.8040)  labels_encoder: 0.7418 (0.8040)  labels_encoder_unscaled: 0.7418 (0.8040)  time: 0.1477  data: 0.0002  max mem: 1206
Epoch: [1]  [1200/1510]  eta: 0:00:42  lr: 0.000100  loss: 0.7166 (0.7987)  labels_encoder: 0.7166 (0.7987)  labels_encoder_unscaled: 0.7166 (0.7987)  time: 0.1345  data: 0.0002  max mem: 1206
Epoch: [1]  [1250/1510]  eta: 0:00:35  lr: 0.000100  loss: 0.7076 (0.7949)  labels_encoder: 0.7076 (0.7949)  labels_encoder_unscaled: 0.7076 (0.7949)  time: 0.1362  data: 0.0003  max mem: 1206
Epoch: [1]  [1300/1510]  eta: 0:00:28  lr: 0.000100  loss: 0.6236 (0.7901)  labels_encoder: 0.6236 (0.7901)  labels_encoder_unscaled: 0.6236 (0.7901)  time: 0.1404  data: 0.0002  max mem: 1206
Epoch: [1]  [1350/1510]  eta: 0:00:21  lr: 0.000100  loss: 0.6299 (0.7850)  labels_encoder: 0.6299 (0.7850)  labels_encoder_unscaled: 0.6299 (0.7850)  time: 0.1296  data: 0.0002  max mem: 1206
Epoch: [1]  [1400/1510]  eta: 0:00:14  lr: 0.000100  loss: 0.5473 (0.7800)  labels_encoder: 0.5473 (0.7800)  labels_encoder_unscaled: 0.5473 (0.7800)  time: 0.1366  data: 0.0003  max mem: 1206
Epoch: [1]  [1450/1510]  eta: 0:00:08  lr: 0.000100  loss: 0.6429 (0.7755)  labels_encoder: 0.6429 (0.7755)  labels_encoder_unscaled: 0.6429 (0.7755)  time: 0.1279  data: 0.0002  max mem: 1206
Epoch: [1]  [1500/1510]  eta: 0:00:01  lr: 0.000100  loss: 0.7043 (0.7722)  labels_encoder: 0.7043 (0.7722)  labels_encoder_unscaled: 0.7043 (0.7722)  time: 0.1276  data: 0.0003  max mem: 1206
Epoch: [1]  [1509/1510]  eta: 0:00:00  lr: 0.000100  loss: 0.6578 (0.7711)  labels_encoder: 0.6578 (0.7711)  labels_encoder_unscaled: 0.6578 (0.7711)  time: 0.1207  data: 0.0003  max mem: 1206
Epoch: [1] Total time: 0:03:25 (0.1359 s / it)
Averaged stats: lr: 0.000100  loss: 0.6578 (0.7711)  labels_encoder: 0.6578 (0.7711)  labels_encoder_unscaled: 0.6578 (0.7711)
Test:  [  0/559]  eta: 0:16:21  loss: 0.6000 (0.6000)  labels_encoder: 0.6000 (0.6000)  labels_encoder_unscaled: 0.6000 (0.6000)  time: 1.7557  data: 1.6893  max mem: 1206
Test:  [ 50/559]  eta: 0:00:53  loss: 1.0643 (0.9948)  labels_encoder: 1.0643 (0.9948)  labels_encoder_unscaled: 1.0643 (0.9948)  time: 0.0636  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:38  loss: 0.5358 (0.9412)  labels_encoder: 0.5358 (0.9412)  labels_encoder_unscaled: 0.5358 (0.9412)  time: 0.0642  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:31  loss: 0.5273 (0.9417)  labels_encoder: 0.5273 (0.9417)  labels_encoder_unscaled: 0.5273 (0.9417)  time: 0.0653  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:25  loss: 0.5233 (0.8718)  labels_encoder: 0.5233 (0.8718)  labels_encoder_unscaled: 0.5233 (0.8718)  time: 0.0575  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:21  loss: 1.1111 (0.9717)  labels_encoder: 1.1111 (0.9717)  labels_encoder_unscaled: 1.1111 (0.9717)  time: 0.0643  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.1969 (1.0140)  labels_encoder: 1.1969 (1.0140)  labels_encoder_unscaled: 1.1969 (1.0140)  time: 0.0638  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:14  loss: 0.6405 (1.0338)  labels_encoder: 0.6405 (1.0338)  labels_encoder_unscaled: 0.6405 (1.0338)  time: 0.0626  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3949 (1.0122)  labels_encoder: 0.3949 (1.0122)  labels_encoder_unscaled: 0.3949 (1.0122)  time: 0.0571  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:07  loss: 0.4394 (0.9902)  labels_encoder: 0.4394 (0.9902)  labels_encoder_unscaled: 0.4394 (0.9902)  time: 0.0588  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.8189 (0.9766)  labels_encoder: 0.8189 (0.9766)  labels_encoder_unscaled: 0.8189 (0.9766)  time: 0.0622  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.6182 (0.9468)  labels_encoder: 0.6182 (0.9468)  labels_encoder_unscaled: 0.6182 (0.9468)  time: 0.0515  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3736 (0.9401)  labels_encoder: 0.3736 (0.9401)  labels_encoder_unscaled: 0.3736 (0.9401)  time: 0.0436  data: 0.0001  max mem: 1206
Test: Total time: 0:00:36 (0.0654 s / it)
Averaged stats: loss: 0.3736 (0.9401)  labels_encoder: 0.3736 (0.9401)  labels_encoder_unscaled: 0.3736 (0.9401)
(21, 71496)
(21, 71496)
[Epoch-1] [IDU-tvseries_kin_features.pickle] mAP: 0.1257, mcAP: 0.8667

BaseballPitch: 0.0626
BasketballDunk: 0.1315
Billiards: 0.0053
CleanAndJerk: 0.3923
CliffDiving: 0.5399
CricketBowling: 0.0661
CricketShot: 0.0868
Diving: 0.0036
FrisbeeCatch: 0.0625
GolfSwing: 0.0472
HammerThrow: 0.0777
HighJump: 0.0222
JavelinThrow: 0.0604
LongJump: 0.3258
PoleVault: 0.0843
Shotput: 0.1357
SoccerPenalty: 0.0373
TennisSwing: 0.2043
ThrowDiscus: 0.0462
VolleyballSpiking: 0.1214
Epoch: [2]  [   0/1510]  eta: 0:50:33  lr: 0.000010  loss: 0.3730 (0.3730)  labels_encoder: 0.3730 (0.3730)  labels_encoder_unscaled: 0.3730 (0.3730)  time: 2.0092  data: 1.8474  max mem: 1206
Epoch: [2]  [  50/1510]  eta: 0:04:08  lr: 0.000010  loss: 0.5708 (0.6041)  labels_encoder: 0.5708 (0.6041)  labels_encoder_unscaled: 0.5708 (0.6041)  time: 0.1307  data: 0.0002  max mem: 1206
Epoch: [2]  [ 100/1510]  eta: 0:03:36  lr: 0.000010  loss: 0.5632 (0.6052)  labels_encoder: 0.5632 (0.6052)  labels_encoder_unscaled: 0.5632 (0.6052)  time: 0.1351  data: 0.0002  max mem: 1206
Epoch: [2]  [ 150/1510]  eta: 0:03:15  lr: 0.000010  loss: 0.5926 (0.6064)  labels_encoder: 0.5926 (0.6064)  labels_encoder_unscaled: 0.5926 (0.6064)  time: 0.1304  data: 0.0002  max mem: 1206
Epoch: [2]  [ 200/1510]  eta: 0:03:02  lr: 0.000010  loss: 0.5321 (0.6038)  labels_encoder: 0.5321 (0.6038)  labels_encoder_unscaled: 0.5321 (0.6038)  time: 0.1201  data: 0.0002  max mem: 1206
Epoch: [2]  [ 250/1510]  eta: 0:02:52  lr: 0.000010  loss: 0.5466 (0.6008)  labels_encoder: 0.5466 (0.6008)  labels_encoder_unscaled: 0.5466 (0.6008)  time: 0.1269  data: 0.0002  max mem: 1206
Epoch: [2]  [ 300/1510]  eta: 0:02:43  lr: 0.000010  loss: 0.5972 (0.5962)  labels_encoder: 0.5972 (0.5962)  labels_encoder_unscaled: 0.5972 (0.5962)  time: 0.1275  data: 0.0002  max mem: 1206
Epoch: [2]  [ 350/1510]  eta: 0:02:36  lr: 0.000010  loss: 0.5317 (0.5921)  labels_encoder: 0.5317 (0.5921)  labels_encoder_unscaled: 0.5317 (0.5921)  time: 0.1307  data: 0.0002  max mem: 1206
Epoch: [2]  [ 400/1510]  eta: 0:02:28  lr: 0.000010  loss: 0.5381 (0.5907)  labels_encoder: 0.5381 (0.5907)  labels_encoder_unscaled: 0.5381 (0.5907)  time: 0.1259  data: 0.0002  max mem: 1206
Epoch: [2]  [ 450/1510]  eta: 0:02:21  lr: 0.000010  loss: 0.5654 (0.5883)  labels_encoder: 0.5654 (0.5883)  labels_encoder_unscaled: 0.5654 (0.5883)  time: 0.1398  data: 0.0002  max mem: 1206
Epoch: [2]  [ 500/1510]  eta: 0:02:15  lr: 0.000010  loss: 0.5475 (0.5849)  labels_encoder: 0.5475 (0.5849)  labels_encoder_unscaled: 0.5475 (0.5849)  time: 0.1389  data: 0.0002  max mem: 1206
Epoch: [2]  [ 550/1510]  eta: 0:02:09  lr: 0.000010  loss: 0.6158 (0.5852)  labels_encoder: 0.6158 (0.5852)  labels_encoder_unscaled: 0.6158 (0.5852)  time: 0.1380  data: 0.0002  max mem: 1206
Epoch: [2]  [ 600/1510]  eta: 0:02:01  lr: 0.000010  loss: 0.5415 (0.5826)  labels_encoder: 0.5415 (0.5826)  labels_encoder_unscaled: 0.5415 (0.5826)  time: 0.1241  data: 0.0002  max mem: 1206
Epoch: [2]  [ 650/1510]  eta: 0:01:54  lr: 0.000010  loss: 0.5627 (0.5825)  labels_encoder: 0.5627 (0.5825)  labels_encoder_unscaled: 0.5627 (0.5825)  time: 0.1242  data: 0.0002  max mem: 1206
Epoch: [2]  [ 700/1510]  eta: 0:01:47  lr: 0.000010  loss: 0.5657 (0.5809)  labels_encoder: 0.5657 (0.5809)  labels_encoder_unscaled: 0.5657 (0.5809)  time: 0.1341  data: 0.0002  max mem: 1206
Epoch: [2]  [ 750/1510]  eta: 0:01:41  lr: 0.000010  loss: 0.5236 (0.5797)  labels_encoder: 0.5236 (0.5797)  labels_encoder_unscaled: 0.5236 (0.5797)  time: 0.1416  data: 0.0002  max mem: 1206
Epoch: [2]  [ 800/1510]  eta: 0:01:35  lr: 0.000010  loss: 0.4857 (0.5772)  labels_encoder: 0.4857 (0.5772)  labels_encoder_unscaled: 0.4857 (0.5772)  time: 0.1405  data: 0.0002  max mem: 1206
Epoch: [2]  [ 850/1510]  eta: 0:01:28  lr: 0.000010  loss: 0.5226 (0.5769)  labels_encoder: 0.5226 (0.5769)  labels_encoder_unscaled: 0.5226 (0.5769)  time: 0.1308  data: 0.0002  max mem: 1206
Epoch: [2]  [ 900/1510]  eta: 0:01:21  lr: 0.000010  loss: 0.5039 (0.5745)  labels_encoder: 0.5039 (0.5745)  labels_encoder_unscaled: 0.5039 (0.5745)  time: 0.1222  data: 0.0002  max mem: 1206
Epoch: [2]  [ 950/1510]  eta: 0:01:14  lr: 0.000010  loss: 0.5325 (0.5725)  labels_encoder: 0.5325 (0.5725)  labels_encoder_unscaled: 0.5325 (0.5725)  time: 0.1389  data: 0.0002  max mem: 1206
Epoch: [2]  [1000/1510]  eta: 0:01:08  lr: 0.000010  loss: 0.5503 (0.5719)  labels_encoder: 0.5503 (0.5719)  labels_encoder_unscaled: 0.5503 (0.5719)  time: 0.1432  data: 0.0002  max mem: 1206
Epoch: [2]  [1050/1510]  eta: 0:01:02  lr: 0.000010  loss: 0.5310 (0.5702)  labels_encoder: 0.5310 (0.5702)  labels_encoder_unscaled: 0.5310 (0.5702)  time: 0.1391  data: 0.0002  max mem: 1206
Epoch: [2]  [1100/1510]  eta: 0:00:55  lr: 0.000010  loss: 0.5497 (0.5683)  labels_encoder: 0.5497 (0.5683)  labels_encoder_unscaled: 0.5497 (0.5683)  time: 0.1385  data: 0.0002  max mem: 1206
Epoch: [2]  [1150/1510]  eta: 0:00:48  lr: 0.000010  loss: 0.5812 (0.5667)  labels_encoder: 0.5812 (0.5667)  labels_encoder_unscaled: 0.5812 (0.5667)  time: 0.1378  data: 0.0002  max mem: 1206
Epoch: [2]  [1200/1510]  eta: 0:00:41  lr: 0.000010  loss: 0.5449 (0.5656)  labels_encoder: 0.5449 (0.5656)  labels_encoder_unscaled: 0.5449 (0.5656)  time: 0.1362  data: 0.0002  max mem: 1206
Epoch: [2]  [1250/1510]  eta: 0:00:34  lr: 0.000010  loss: 0.4963 (0.5644)  labels_encoder: 0.4963 (0.5644)  labels_encoder_unscaled: 0.4963 (0.5644)  time: 0.1238  data: 0.0002  max mem: 1206
Epoch: [2]  [1300/1510]  eta: 0:00:28  lr: 0.000010  loss: 0.4879 (0.5623)  labels_encoder: 0.4879 (0.5623)  labels_encoder_unscaled: 0.4879 (0.5623)  time: 0.1353  data: 0.0002  max mem: 1206
Epoch: [2]  [1350/1510]  eta: 0:00:21  lr: 0.000010  loss: 0.5145 (0.5610)  labels_encoder: 0.5145 (0.5610)  labels_encoder_unscaled: 0.5145 (0.5610)  time: 0.1343  data: 0.0002  max mem: 1206
Epoch: [2]  [1400/1510]  eta: 0:00:14  lr: 0.000010  loss: 0.5128 (0.5605)  labels_encoder: 0.5128 (0.5605)  labels_encoder_unscaled: 0.5128 (0.5605)  time: 0.1360  data: 0.0003  max mem: 1206
Epoch: [2]  [1450/1510]  eta: 0:00:08  lr: 0.000010  loss: 0.4930 (0.5599)  labels_encoder: 0.4930 (0.5599)  labels_encoder_unscaled: 0.4930 (0.5599)  time: 0.1284  data: 0.0002  max mem: 1206
Epoch: [2]  [1500/1510]  eta: 0:00:01  lr: 0.000010  loss: 0.4756 (0.5586)  labels_encoder: 0.4756 (0.5586)  labels_encoder_unscaled: 0.4756 (0.5586)  time: 0.1265  data: 0.0004  max mem: 1206
Epoch: [2]  [1509/1510]  eta: 0:00:00  lr: 0.000010  loss: 0.4964 (0.5582)  labels_encoder: 0.4964 (0.5582)  labels_encoder_unscaled: 0.4964 (0.5582)  time: 0.1231  data: 0.0004  max mem: 1206
Epoch: [2] Total time: 0:03:22 (0.1341 s / it)
Averaged stats: lr: 0.000010  loss: 0.4964 (0.5582)  labels_encoder: 0.4964 (0.5582)  labels_encoder_unscaled: 0.4964 (0.5582)
Test:  [  0/559]  eta: 0:18:17  loss: 0.5645 (0.5645)  labels_encoder: 0.5645 (0.5645)  labels_encoder_unscaled: 0.5645 (0.5645)  time: 1.9630  data: 1.9122  max mem: 1206
Test:  [ 50/559]  eta: 0:00:50  loss: 0.9845 (0.8828)  labels_encoder: 0.9845 (0.8828)  labels_encoder_unscaled: 0.9845 (0.8828)  time: 0.0601  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:35  loss: 0.7113 (0.8510)  labels_encoder: 0.7113 (0.8510)  labels_encoder_unscaled: 0.7113 (0.8510)  time: 0.0539  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:29  loss: 0.4257 (0.8700)  labels_encoder: 0.4257 (0.8700)  labels_encoder_unscaled: 0.4257 (0.8700)  time: 0.0601  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:24  loss: 0.7194 (0.8150)  labels_encoder: 0.7194 (0.8150)  labels_encoder_unscaled: 0.7194 (0.8150)  time: 0.0542  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.7844 (0.9260)  labels_encoder: 0.7844 (0.9260)  labels_encoder_unscaled: 0.7844 (0.9260)  time: 0.0657  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.1212 (0.9748)  labels_encoder: 1.1212 (0.9748)  labels_encoder_unscaled: 1.1212 (0.9748)  time: 0.0592  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.8117 (0.9953)  labels_encoder: 0.8117 (0.9953)  labels_encoder_unscaled: 0.8117 (0.9953)  time: 0.0595  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3484 (0.9735)  labels_encoder: 0.3484 (0.9735)  labels_encoder_unscaled: 0.3484 (0.9735)  time: 0.0537  data: 0.0001  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.3930 (0.9501)  labels_encoder: 0.3930 (0.9501)  labels_encoder_unscaled: 0.3930 (0.9501)  time: 0.0563  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5997 (0.9315)  labels_encoder: 0.5997 (0.9315)  labels_encoder_unscaled: 0.5997 (0.9315)  time: 0.0665  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7999 (0.9098)  labels_encoder: 0.7999 (0.9098)  labels_encoder_unscaled: 0.7999 (0.9098)  time: 0.0546  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3694 (0.9030)  labels_encoder: 0.3694 (0.9030)  labels_encoder_unscaled: 0.3694 (0.9030)  time: 0.0479  data: 0.0001  max mem: 1206
Test: Total time: 0:00:34 (0.0624 s / it)
Averaged stats: loss: 0.3694 (0.9030)  labels_encoder: 0.3694 (0.9030)  labels_encoder_unscaled: 0.3694 (0.9030)
(21, 71496)
(21, 71496)
[Epoch-2] [IDU-tvseries_kin_features.pickle] mAP: 0.1335, mcAP: 0.8791

BaseballPitch: 0.0656
BasketballDunk: 0.1299
Billiards: 0.0045
CleanAndJerk: 0.3910
CliffDiving: 0.4499
CricketBowling: 0.0687
CricketShot: 0.1021
Diving: 0.0027
FrisbeeCatch: 0.1311
GolfSwing: 0.0807
HammerThrow: 0.1158
HighJump: 0.0434
JavelinThrow: 0.0467
LongJump: 0.3761
PoleVault: 0.1007
Shotput: 0.1446
SoccerPenalty: 0.0538
TennisSwing: 0.1674
ThrowDiscus: 0.0469
VolleyballSpiking: 0.1477
Epoch: [3]  [   0/1510]  eta: 0:59:12  lr: 0.000001  loss: 0.5310 (0.5310)  labels_encoder: 0.5310 (0.5310)  labels_encoder_unscaled: 0.5310 (0.5310)  time: 2.3530  data: 2.0313  max mem: 1206
Epoch: [3]  [  50/1510]  eta: 0:04:07  lr: 0.000001  loss: 0.5170 (0.5286)  labels_encoder: 0.5170 (0.5286)  labels_encoder_unscaled: 0.5170 (0.5286)  time: 0.1178  data: 0.0002  max mem: 1206
Epoch: [3]  [ 100/1510]  eta: 0:03:32  lr: 0.000001  loss: 0.5344 (0.5271)  labels_encoder: 0.5344 (0.5271)  labels_encoder_unscaled: 0.5344 (0.5271)  time: 0.1310  data: 0.0002  max mem: 1206
Epoch: [3]  [ 150/1510]  eta: 0:03:15  lr: 0.000001  loss: 0.5333 (0.5293)  labels_encoder: 0.5333 (0.5293)  labels_encoder_unscaled: 0.5333 (0.5293)  time: 0.1246  data: 0.0002  max mem: 1206
Epoch: [3]  [ 200/1510]  eta: 0:03:03  lr: 0.000001  loss: 0.4791 (0.5280)  labels_encoder: 0.4791 (0.5280)  labels_encoder_unscaled: 0.4791 (0.5280)  time: 0.1327  data: 0.0002  max mem: 1206
Epoch: [3]  [ 250/1510]  eta: 0:02:53  lr: 0.000001  loss: 0.4742 (0.5266)  labels_encoder: 0.4742 (0.5266)  labels_encoder_unscaled: 0.4742 (0.5266)  time: 0.1212  data: 0.0002  max mem: 1206
Epoch: [3]  [ 300/1510]  eta: 0:02:46  lr: 0.000001  loss: 0.4936 (0.5233)  labels_encoder: 0.4936 (0.5233)  labels_encoder_unscaled: 0.4936 (0.5233)  time: 0.1438  data: 0.0002  max mem: 1206
Epoch: [3]  [ 350/1510]  eta: 0:02:39  lr: 0.000001  loss: 0.5269 (0.5233)  labels_encoder: 0.5269 (0.5233)  labels_encoder_unscaled: 0.5269 (0.5233)  time: 0.1312  data: 0.0002  max mem: 1206
Epoch: [3]  [ 400/1510]  eta: 0:02:32  lr: 0.000001  loss: 0.5095 (0.5223)  labels_encoder: 0.5095 (0.5223)  labels_encoder_unscaled: 0.5095 (0.5223)  time: 0.1326  data: 0.0002  max mem: 1206
Epoch: [3]  [ 450/1510]  eta: 0:02:24  lr: 0.000001  loss: 0.4585 (0.5192)  labels_encoder: 0.4585 (0.5192)  labels_encoder_unscaled: 0.4585 (0.5192)  time: 0.1279  data: 0.0002  max mem: 1206
Epoch: [3]  [ 500/1510]  eta: 0:02:16  lr: 0.000001  loss: 0.5033 (0.5187)  labels_encoder: 0.5033 (0.5187)  labels_encoder_unscaled: 0.5033 (0.5187)  time: 0.1247  data: 0.0002  max mem: 1206
Epoch: [3]  [ 550/1510]  eta: 0:02:09  lr: 0.000001  loss: 0.4667 (0.5174)  labels_encoder: 0.4667 (0.5174)  labels_encoder_unscaled: 0.4667 (0.5174)  time: 0.1387  data: 0.0002  max mem: 1206
Epoch: [3]  [ 600/1510]  eta: 0:02:03  lr: 0.000001  loss: 0.4835 (0.5160)  labels_encoder: 0.4835 (0.5160)  labels_encoder_unscaled: 0.4835 (0.5160)  time: 0.1425  data: 0.0002  max mem: 1206
Epoch: [3]  [ 650/1510]  eta: 0:01:57  lr: 0.000001  loss: 0.5368 (0.5170)  labels_encoder: 0.5368 (0.5170)  labels_encoder_unscaled: 0.5368 (0.5170)  time: 0.1424  data: 0.0002  max mem: 1206
Epoch: [3]  [ 700/1510]  eta: 0:01:49  lr: 0.000001  loss: 0.4899 (0.5159)  labels_encoder: 0.4899 (0.5159)  labels_encoder_unscaled: 0.4899 (0.5159)  time: 0.1288  data: 0.0002  max mem: 1206
Epoch: [3]  [ 750/1510]  eta: 0:01:43  lr: 0.000001  loss: 0.5010 (0.5153)  labels_encoder: 0.5010 (0.5153)  labels_encoder_unscaled: 0.5010 (0.5153)  time: 0.1306  data: 0.0002  max mem: 1206
Epoch: [3]  [ 800/1510]  eta: 0:01:36  lr: 0.000001  loss: 0.4864 (0.5156)  labels_encoder: 0.4864 (0.5156)  labels_encoder_unscaled: 0.4864 (0.5156)  time: 0.1294  data: 0.0002  max mem: 1206
Epoch: [3]  [ 850/1510]  eta: 0:01:29  lr: 0.000001  loss: 0.5363 (0.5153)  labels_encoder: 0.5363 (0.5153)  labels_encoder_unscaled: 0.5363 (0.5153)  time: 0.1373  data: 0.0002  max mem: 1206
Epoch: [3]  [ 900/1510]  eta: 0:01:22  lr: 0.000001  loss: 0.4864 (0.5151)  labels_encoder: 0.4864 (0.5151)  labels_encoder_unscaled: 0.4864 (0.5151)  time: 0.1239  data: 0.0002  max mem: 1206
Epoch: [3]  [ 950/1510]  eta: 0:01:15  lr: 0.000001  loss: 0.4763 (0.5140)  labels_encoder: 0.4763 (0.5140)  labels_encoder_unscaled: 0.4763 (0.5140)  time: 0.1330  data: 0.0002  max mem: 1206
Epoch: [3]  [1000/1510]  eta: 0:01:08  lr: 0.000001  loss: 0.5229 (0.5140)  labels_encoder: 0.5229 (0.5140)  labels_encoder_unscaled: 0.5229 (0.5140)  time: 0.1336  data: 0.0002  max mem: 1206
Epoch: [3]  [1050/1510]  eta: 0:01:02  lr: 0.000001  loss: 0.5457 (0.5137)  labels_encoder: 0.5457 (0.5137)  labels_encoder_unscaled: 0.5457 (0.5137)  time: 0.1288  data: 0.0002  max mem: 1206
Epoch: [3]  [1100/1510]  eta: 0:00:55  lr: 0.000001  loss: 0.5365 (0.5137)  labels_encoder: 0.5365 (0.5137)  labels_encoder_unscaled: 0.5365 (0.5137)  time: 0.1289  data: 0.0002  max mem: 1206
Epoch: [3]  [1150/1510]  eta: 0:00:48  lr: 0.000001  loss: 0.4802 (0.5134)  labels_encoder: 0.4802 (0.5134)  labels_encoder_unscaled: 0.4802 (0.5134)  time: 0.1426  data: 0.0002  max mem: 1206
Epoch: [3]  [1200/1510]  eta: 0:00:41  lr: 0.000001  loss: 0.4757 (0.5132)  labels_encoder: 0.4757 (0.5132)  labels_encoder_unscaled: 0.4757 (0.5132)  time: 0.1446  data: 0.0002  max mem: 1206
Epoch: [3]  [1250/1510]  eta: 0:00:35  lr: 0.000001  loss: 0.5058 (0.5125)  labels_encoder: 0.5058 (0.5125)  labels_encoder_unscaled: 0.5058 (0.5125)  time: 0.1366  data: 0.0002  max mem: 1206
Epoch: [3]  [1300/1510]  eta: 0:00:28  lr: 0.000001  loss: 0.5402 (0.5134)  labels_encoder: 0.5402 (0.5134)  labels_encoder_unscaled: 0.5402 (0.5134)  time: 0.1417  data: 0.0002  max mem: 1206
Epoch: [3]  [1350/1510]  eta: 0:00:21  lr: 0.000001  loss: 0.4824 (0.5135)  labels_encoder: 0.4824 (0.5135)  labels_encoder_unscaled: 0.4824 (0.5135)  time: 0.1394  data: 0.0002  max mem: 1206
Epoch: [3]  [1400/1510]  eta: 0:00:14  lr: 0.000001  loss: 0.4998 (0.5136)  labels_encoder: 0.4998 (0.5136)  labels_encoder_unscaled: 0.4998 (0.5136)  time: 0.1259  data: 0.0002  max mem: 1206
Epoch: [3]  [1450/1510]  eta: 0:00:08  lr: 0.000001  loss: 0.4996 (0.5137)  labels_encoder: 0.4996 (0.5137)  labels_encoder_unscaled: 0.4996 (0.5137)  time: 0.1304  data: 0.0002  max mem: 1206
Epoch: [3]  [1500/1510]  eta: 0:00:01  lr: 0.000001  loss: 0.5431 (0.5135)  labels_encoder: 0.5431 (0.5135)  labels_encoder_unscaled: 0.5431 (0.5135)  time: 0.1339  data: 0.0004  max mem: 1206
Epoch: [3]  [1509/1510]  eta: 0:00:00  lr: 0.000001  loss: 0.5086 (0.5132)  labels_encoder: 0.5086 (0.5132)  labels_encoder_unscaled: 0.5086 (0.5132)  time: 0.1245  data: 0.0003  max mem: 1206
Epoch: [3] Total time: 0:03:24 (0.1353 s / it)
Averaged stats: lr: 0.000001  loss: 0.5086 (0.5132)  labels_encoder: 0.5086 (0.5132)  labels_encoder_unscaled: 0.5086 (0.5132)
Test:  [  0/559]  eta: 0:17:20  loss: 0.6346 (0.6346)  labels_encoder: 0.6346 (0.6346)  labels_encoder_unscaled: 0.6346 (0.6346)  time: 1.8607  data: 1.8000  max mem: 1206
Test:  [ 50/559]  eta: 0:00:50  loss: 0.9857 (0.8751)  labels_encoder: 0.9857 (0.8751)  labels_encoder_unscaled: 0.9857 (0.8751)  time: 0.0610  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:36  loss: 0.5163 (0.8375)  labels_encoder: 0.5163 (0.8375)  labels_encoder_unscaled: 0.5163 (0.8375)  time: 0.0596  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:30  loss: 0.4304 (0.8637)  labels_encoder: 0.4304 (0.8637)  labels_encoder_unscaled: 0.4304 (0.8637)  time: 0.0632  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:25  loss: 0.7043 (0.8085)  labels_encoder: 0.7043 (0.8085)  labels_encoder_unscaled: 0.7043 (0.8085)  time: 0.0610  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:21  loss: 0.7848 (0.9219)  labels_encoder: 0.7848 (0.9219)  labels_encoder_unscaled: 0.7848 (0.9219)  time: 0.0659  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.0312 (0.9781)  labels_encoder: 1.0312 (0.9781)  labels_encoder_unscaled: 1.0312 (0.9781)  time: 0.0645  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.8001 (1.0038)  labels_encoder: 0.8001 (1.0038)  labels_encoder_unscaled: 0.8001 (1.0038)  time: 0.0599  data: 0.0001  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3487 (0.9788)  labels_encoder: 0.3487 (0.9788)  labels_encoder_unscaled: 0.3487 (0.9788)  time: 0.0487  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.3847 (0.9549)  labels_encoder: 0.3847 (0.9549)  labels_encoder_unscaled: 0.3847 (0.9549)  time: 0.0582  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6092 (0.9360)  labels_encoder: 0.6092 (0.9360)  labels_encoder_unscaled: 0.6092 (0.9360)  time: 0.0562  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.6958 (0.9120)  labels_encoder: 0.6958 (0.9120)  labels_encoder_unscaled: 0.6958 (0.9120)  time: 0.0538  data: 0.0002  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3506 (0.9055)  labels_encoder: 0.3506 (0.9055)  labels_encoder_unscaled: 0.3506 (0.9055)  time: 0.0425  data: 0.0001  max mem: 1206
Test: Total time: 0:00:34 (0.0621 s / it)
Averaged stats: loss: 0.3506 (0.9055)  labels_encoder: 0.3506 (0.9055)  labels_encoder_unscaled: 0.3506 (0.9055)
(21, 71496)
(21, 71496)
[Epoch-3] [IDU-tvseries_kin_features.pickle] mAP: 0.1272, mcAP: 0.8794

BaseballPitch: 0.0237
BasketballDunk: 0.1283
Billiards: 0.0044
CleanAndJerk: 0.3824
CliffDiving: 0.3946
CricketBowling: 0.0542
CricketShot: 0.0866
Diving: 0.0050
FrisbeeCatch: 0.1347
GolfSwing: 0.0676
HammerThrow: 0.1113
HighJump: 0.0574
JavelinThrow: 0.0539
LongJump: 0.3562
PoleVault: 0.1106
Shotput: 0.1494
SoccerPenalty: 0.0503
TennisSwing: 0.1612
ThrowDiscus: 0.0541
VolleyballSpiking: 0.1573
Epoch: [4]  [   0/1510]  eta: 0:46:11  lr: 0.000000  loss: 0.6216 (0.6216)  labels_encoder: 0.6216 (0.6216)  labels_encoder_unscaled: 0.6216 (0.6216)  time: 1.8357  data: 1.6217  max mem: 1206
Epoch: [4]  [  50/1510]  eta: 0:04:02  lr: 0.000000  loss: 0.5247 (0.5043)  labels_encoder: 0.5247 (0.5043)  labels_encoder_unscaled: 0.5247 (0.5043)  time: 0.1335  data: 0.0002  max mem: 1206
Epoch: [4]  [ 100/1510]  eta: 0:03:28  lr: 0.000000  loss: 0.5438 (0.5103)  labels_encoder: 0.5438 (0.5103)  labels_encoder_unscaled: 0.5438 (0.5103)  time: 0.1260  data: 0.0002  max mem: 1206
Epoch: [4]  [ 150/1510]  eta: 0:03:12  lr: 0.000000  loss: 0.4990 (0.5106)  labels_encoder: 0.4990 (0.5106)  labels_encoder_unscaled: 0.4990 (0.5106)  time: 0.1348  data: 0.0002  max mem: 1206
Epoch: [4]  [ 200/1510]  eta: 0:03:01  lr: 0.000000  loss: 0.4575 (0.5094)  labels_encoder: 0.4575 (0.5094)  labels_encoder_unscaled: 0.4575 (0.5094)  time: 0.1299  data: 0.0002  max mem: 1206
Epoch: [4]  [ 250/1510]  eta: 0:02:51  lr: 0.000000  loss: 0.4988 (0.5142)  labels_encoder: 0.4988 (0.5142)  labels_encoder_unscaled: 0.4988 (0.5142)  time: 0.1258  data: 0.0002  max mem: 1206
Epoch: [4]  [ 300/1510]  eta: 0:02:45  lr: 0.000000  loss: 0.4612 (0.5058)  labels_encoder: 0.4612 (0.5058)  labels_encoder_unscaled: 0.4612 (0.5058)  time: 0.1403  data: 0.0002  max mem: 1206
Epoch: [4]  [ 350/1510]  eta: 0:02:39  lr: 0.000000  loss: 0.4415 (0.5049)  labels_encoder: 0.4415 (0.5049)  labels_encoder_unscaled: 0.4415 (0.5049)  time: 0.1438  data: 0.0002  max mem: 1206
Epoch: [4]  [ 400/1510]  eta: 0:02:31  lr: 0.000000  loss: 0.4853 (0.5044)  labels_encoder: 0.4853 (0.5044)  labels_encoder_unscaled: 0.4853 (0.5044)  time: 0.1437  data: 0.0002  max mem: 1206
Epoch: [4]  [ 450/1510]  eta: 0:02:26  lr: 0.000000  loss: 0.4916 (0.5054)  labels_encoder: 0.4916 (0.5054)  labels_encoder_unscaled: 0.4916 (0.5054)  time: 0.1449  data: 0.0002  max mem: 1206
Epoch: [4]  [ 500/1510]  eta: 0:02:19  lr: 0.000000  loss: 0.5280 (0.5056)  labels_encoder: 0.5280 (0.5056)  labels_encoder_unscaled: 0.5280 (0.5056)  time: 0.1372  data: 0.0002  max mem: 1206
Epoch: [4]  [ 550/1510]  eta: 0:02:11  lr: 0.000000  loss: 0.5059 (0.5042)  labels_encoder: 0.5059 (0.5042)  labels_encoder_unscaled: 0.5059 (0.5042)  time: 0.1371  data: 0.0003  max mem: 1206
Epoch: [4]  [ 600/1510]  eta: 0:02:05  lr: 0.000000  loss: 0.4554 (0.5044)  labels_encoder: 0.4554 (0.5044)  labels_encoder_unscaled: 0.4554 (0.5044)  time: 0.1408  data: 0.0002  max mem: 1206
Epoch: [4]  [ 650/1510]  eta: 0:01:58  lr: 0.000000  loss: 0.4615 (0.5042)  labels_encoder: 0.4615 (0.5042)  labels_encoder_unscaled: 0.4615 (0.5042)  time: 0.1445  data: 0.0002  max mem: 1206
Epoch: [4]  [ 700/1510]  eta: 0:01:51  lr: 0.000000  loss: 0.5021 (0.5057)  labels_encoder: 0.5021 (0.5057)  labels_encoder_unscaled: 0.5021 (0.5057)  time: 0.1390  data: 0.0002  max mem: 1206
Epoch: [4]  [ 750/1510]  eta: 0:01:45  lr: 0.000000  loss: 0.5121 (0.5046)  labels_encoder: 0.5121 (0.5046)  labels_encoder_unscaled: 0.5121 (0.5046)  time: 0.1434  data: 0.0002  max mem: 1206
Epoch: [4]  [ 800/1510]  eta: 0:01:38  lr: 0.000000  loss: 0.4951 (0.5058)  labels_encoder: 0.4951 (0.5058)  labels_encoder_unscaled: 0.4951 (0.5058)  time: 0.1453  data: 0.0002  max mem: 1206
Epoch: [4]  [ 850/1510]  eta: 0:01:31  lr: 0.000000  loss: 0.5024 (0.5061)  labels_encoder: 0.5024 (0.5061)  labels_encoder_unscaled: 0.5024 (0.5061)  time: 0.1334  data: 0.0002  max mem: 1206
Epoch: [4]  [ 900/1510]  eta: 0:01:24  lr: 0.000000  loss: 0.5481 (0.5070)  labels_encoder: 0.5481 (0.5070)  labels_encoder_unscaled: 0.5481 (0.5070)  time: 0.1435  data: 0.0002  max mem: 1206
Epoch: [4]  [ 950/1510]  eta: 0:01:17  lr: 0.000000  loss: 0.4636 (0.5064)  labels_encoder: 0.4636 (0.5064)  labels_encoder_unscaled: 0.4636 (0.5064)  time: 0.1429  data: 0.0002  max mem: 1206
Epoch: [4]  [1000/1510]  eta: 0:01:10  lr: 0.000000  loss: 0.5226 (0.5079)  labels_encoder: 0.5226 (0.5079)  labels_encoder_unscaled: 0.5226 (0.5079)  time: 0.1365  data: 0.0002  max mem: 1206
Epoch: [4]  [1050/1510]  eta: 0:01:03  lr: 0.000000  loss: 0.4864 (0.5083)  labels_encoder: 0.4864 (0.5083)  labels_encoder_unscaled: 0.4864 (0.5083)  time: 0.1392  data: 0.0004  max mem: 1206
Epoch: [4]  [1100/1510]  eta: 0:00:56  lr: 0.000000  loss: 0.5166 (0.5087)  labels_encoder: 0.5166 (0.5087)  labels_encoder_unscaled: 0.5166 (0.5087)  time: 0.1377  data: 0.0002  max mem: 1206
Epoch: [4]  [1150/1510]  eta: 0:00:49  lr: 0.000000  loss: 0.4859 (0.5083)  labels_encoder: 0.4859 (0.5083)  labels_encoder_unscaled: 0.4859 (0.5083)  time: 0.1275  data: 0.0002  max mem: 1206
Epoch: [4]  [1200/1510]  eta: 0:00:42  lr: 0.000000  loss: 0.5390 (0.5078)  labels_encoder: 0.5390 (0.5078)  labels_encoder_unscaled: 0.5390 (0.5078)  time: 0.1376  data: 0.0002  max mem: 1206
Epoch: [4]  [1250/1510]  eta: 0:00:35  lr: 0.000000  loss: 0.5177 (0.5076)  labels_encoder: 0.5177 (0.5076)  labels_encoder_unscaled: 0.5177 (0.5076)  time: 0.1411  data: 0.0002  max mem: 1206
Epoch: [4]  [1300/1510]  eta: 0:00:29  lr: 0.000000  loss: 0.5169 (0.5071)  labels_encoder: 0.5169 (0.5071)  labels_encoder_unscaled: 0.5169 (0.5071)  time: 0.1406  data: 0.0002  max mem: 1206
Epoch: [4]  [1350/1510]  eta: 0:00:22  lr: 0.000000  loss: 0.5284 (0.5073)  labels_encoder: 0.5284 (0.5073)  labels_encoder_unscaled: 0.5284 (0.5073)  time: 0.1471  data: 0.0003  max mem: 1206
Epoch: [4]  [1400/1510]  eta: 0:00:15  lr: 0.000000  loss: 0.4958 (0.5077)  labels_encoder: 0.4958 (0.5077)  labels_encoder_unscaled: 0.4958 (0.5077)  time: 0.1463  data: 0.0002  max mem: 1206
Epoch: [4]  [1450/1510]  eta: 0:00:08  lr: 0.000000  loss: 0.5439 (0.5087)  labels_encoder: 0.5439 (0.5087)  labels_encoder_unscaled: 0.5439 (0.5087)  time: 0.1301  data: 0.0002  max mem: 1206
Epoch: [4]  [1500/1510]  eta: 0:00:01  lr: 0.000000  loss: 0.5079 (0.5082)  labels_encoder: 0.5079 (0.5082)  labels_encoder_unscaled: 0.5079 (0.5082)  time: 0.1274  data: 0.0004  max mem: 1206
Epoch: [4]  [1509/1510]  eta: 0:00:00  lr: 0.000000  loss: 0.5096 (0.5083)  labels_encoder: 0.5096 (0.5083)  labels_encoder_unscaled: 0.5096 (0.5083)  time: 0.1172  data: 0.0003  max mem: 1206
Epoch: [4] Total time: 0:03:28 (0.1381 s / it)
Averaged stats: lr: 0.000000  loss: 0.5096 (0.5083)  labels_encoder: 0.5096 (0.5083)  labels_encoder_unscaled: 0.5096 (0.5083)
Test:  [  0/559]  eta: 0:15:27  loss: 0.6401 (0.6401)  labels_encoder: 0.6401 (0.6401)  labels_encoder_unscaled: 0.6401 (0.6401)  time: 1.6585  data: 1.5855  max mem: 1206
Test:  [ 50/559]  eta: 0:00:53  loss: 1.0490 (0.8955)  labels_encoder: 1.0490 (0.8955)  labels_encoder_unscaled: 1.0490 (0.8955)  time: 0.0667  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:39  loss: 0.6421 (0.8584)  labels_encoder: 0.6421 (0.8584)  labels_encoder_unscaled: 0.6421 (0.8584)  time: 0.0666  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:31  loss: 0.5025 (0.8811)  labels_encoder: 0.5025 (0.8811)  labels_encoder_unscaled: 0.5025 (0.8811)  time: 0.0647  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:26  loss: 0.7960 (0.8206)  labels_encoder: 0.7960 (0.8206)  labels_encoder_unscaled: 0.7960 (0.8206)  time: 0.0568  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:22  loss: 0.7665 (0.9414)  labels_encoder: 0.7665 (0.9414)  labels_encoder_unscaled: 0.7665 (0.9414)  time: 0.0667  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:18  loss: 1.1422 (0.9964)  labels_encoder: 1.1422 (0.9964)  labels_encoder_unscaled: 1.1422 (0.9964)  time: 0.0657  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:14  loss: 0.8305 (1.0172)  labels_encoder: 0.8305 (1.0172)  labels_encoder_unscaled: 0.8305 (1.0172)  time: 0.0600  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3467 (0.9903)  labels_encoder: 0.3467 (0.9903)  labels_encoder_unscaled: 0.3467 (0.9903)  time: 0.0675  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:07  loss: 0.4078 (0.9650)  labels_encoder: 0.4078 (0.9650)  labels_encoder_unscaled: 0.4078 (0.9650)  time: 0.0579  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6249 (0.9440)  labels_encoder: 0.6249 (0.9440)  labels_encoder_unscaled: 0.6249 (0.9440)  time: 0.0617  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7198 (0.9198)  labels_encoder: 0.7198 (0.9198)  labels_encoder_unscaled: 0.7198 (0.9198)  time: 0.0462  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3529 (0.9130)  labels_encoder: 0.3529 (0.9130)  labels_encoder_unscaled: 0.3529 (0.9130)  time: 0.0385  data: 0.0001  max mem: 1206
Test: Total time: 0:00:36 (0.0658 s / it)
Averaged stats: loss: 0.3529 (0.9130)  labels_encoder: 0.3529 (0.9130)  labels_encoder_unscaled: 0.3529 (0.9130)
(21, 71496)
(21, 71496)
[Epoch-4] [IDU-tvseries_kin_features.pickle] mAP: 0.1235, mcAP: 0.8767

BaseballPitch: 0.0299
BasketballDunk: 0.1407
Billiards: 0.0045
CleanAndJerk: 0.4013
CliffDiving: 0.4033
CricketBowling: 0.0593
CricketShot: 0.0777
Diving: 0.0047
FrisbeeCatch: 0.1219
GolfSwing: 0.0652
HammerThrow: 0.0900
HighJump: 0.0348
JavelinThrow: 0.0511
LongJump: 0.3518
PoleVault: 0.1128
Shotput: 0.1499
SoccerPenalty: 0.0523
TennisSwing: 0.1233
ThrowDiscus: 0.0444
VolleyballSpiking: 0.1507
Training time 0:16:15
