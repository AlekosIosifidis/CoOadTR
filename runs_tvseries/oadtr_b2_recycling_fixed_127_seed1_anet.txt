Unable to compile CoConv C++ implementation. Falling back to Python version.
[Errno 2] No such file or directory: '/home/lh/.conda/envs/oadtr/lib/python3.8/site-packages/continual/conv.cpp'
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'conv_flops_counter_hook'
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
dropout_p is not supported yet and will be skipped
Failed to add flops_counter_hook: module 'ptflops.flops_counter' has no attribute 'MODULES_MAPPING'
Not using distributed mode
lr:0.0001
batch_size:128
weight_decay:0.0001
epochs:5
resize_feature:False
lr_drop:1
clip_max_norm:1.0
dataparallel:False
removelog:False
version:v3
query_num:8
decoder_layers:5
decoder_embedding_dim:1024
decoder_embedding_dim_out:1024
decoder_attn_dropout_rate:0.1
decoder_num_heads:4
classification_pred_loss_coef:0.5
enc_layers:64
lr_backbone:0.0001
feature:tvseries_anet_features.pickle
dim_feature:4096
patch_dim:1
embedding_dim:1024
num_heads:8
num_layers:2
attn_dropout_rate:0.1
positional_encoding_type:recycling_fixed
num_embeddings:127
hidden_dim:1024
dropout_rate:0.1
numclass:31
classification_x_loss_coef:0.3
classification_h_loss_coef:1
similar_loss_coef:0.1
margin:1.0
dataset:tvseries
dataset_file:data/data_info_new.json
frozen_weights:None
thumos_data_path:/home/dancer/mycode/Temporal.Online.Detection/Online.TRN.Pytorch/preprocess/
thumos_anno_path:data/thumos_{}_anno.pickle
remove_difficult:False
device:cuda
output_dir:models
seed:1
resume:
start_epoch:1
eval:False
num_workers:8
world_size:1
dist_url:tcp://127.0.0.1:12342
train_session_set:['24_ep1', '24_ep2', '24_ep3', 'Breaking_Bad_ep1', 'Breaking_Bad_ep2', 'How_I_Met_Your_Mother_ep1', 'How_I_Met_Your_Mother_ep2', 'How_I_Met_Your_Mother_ep3', 'How_I_Met_Your_Mother_ep4', 'How_I_Met_Your_Mother_ep5', 'How_I_Met_Your_Mother_ep6', 'Mad_Men_ep1', 'Mad_Men_ep2', 'Modern_Family_ep1', 'Modern_Family_ep2', 'Modern_Family_ep3', 'Modern_Family_ep4', 'Modern_Family_ep6', 'Sons_of_Anarchy_ep1', 'Sons_of_Anarchy_ep2']
test_session_set:['24_ep4', 'Breaking_Bad_ep3', 'Mad_Men_ep3', 'How_I_Met_Your_Mother_ep7', 'How_I_Met_Your_Mother_ep8', 'Modern_Family_ep5', 'Sons_of_Anarchy_ep3']
class_index:['background', 'Pick something up', 'Point', 'Drink', 'Stand up', 'Run', 'Sit down', 'Read', 'Smoke', 'Drive car', 'Open door', 'Give something', 'Use computer', 'Write', 'Go down stairway', 'Close door', 'Throw something', 'Go up stairway', 'Get in/out of car', 'Hang up phone', 'Eat', 'Answer phone', 'Dress up', 'Clap', 'Undress', 'Kiss', 'Fall/trip', 'Wave', 'Pour', 'Punch', 'Fire weapon']
distributed:False
position encoding : recycling_fixed
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Sequential(
  16.814 M, 99.939% Params, 0.412 GMac, 100.000% MACs, 
  (0): Linear(4.195 M, 24.936% Params, 0.004 GMac, 1.018% MACs, in_features=4096, out_features=1024, bias=True, channel_dim=1)
  (1): RecyclingPositionalEncoding(
    0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 
    (pe): CyclicPositionalEncoding(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )
  )
  (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
  (3): Sequential(
    12.587 M, 74.814% Params, 0.408 GMac, 98.974% MACs, 
    (0): Sequential(
      6.294 M, 37.407% Params, 0.205 GMac, 49.804% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, reduce=reduce_sum
        (0): RetroactiveUnity(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 63)
        (1): CoReMultiheadAttention(
          4.194 M, 24.930% Params, 0.071 GMac, 17.179% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(Sequential(
        2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
        (0): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
        (1): Residual(
          2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
          (fn): Sequential(
            2.099 M, 12.477% Params, 0.134 GMac, 32.625% MACs, 
            (0): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
            (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
            (3): Linear(1.05 M, 6.239% Params, 0.067 GMac, 16.305% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
            (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          )
        )
        (2): LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True)
      ))
    )
    (1): Lambda(Sequential(
      6.294 M, 37.407% Params, 0.203 GMac, 49.170% MACs, 
      (0): BroadcastReduce(
        4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, reduce=sum_last_pairs
        (0): SelectOrDelay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): CoSiMultiheadAttention(
          4.194 M, 24.930% Params, 0.203 GMac, 49.153% MACs, 
          (out_proj): NonDynamicallyQuantizableLinear(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=False)
        )
      )
      (1): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
      (2): BroadcastReduce(
        2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, reduce=reduce_sum
        (0): Delay(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, 0)
        (1): Sequential(
          2.099 M, 12.477% Params, 0.0 GMac, 0.017% MACs, 
          (0): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (1): GELU(0.0 M, 0.000% Params, 0.0 GMac, 0.016% MACs, )
          (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
          (3): Linear(1.05 M, 6.239% Params, 0.0 GMac, 0.000% MACs, in_features=1024, out_features=1024, bias=True, channel_dim=1)
          (4): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.1, inplace=False)
        )
      )
      (3): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
    ), takes_time=True)
    (2): Lambda(unity, squeeze_last, squeeze_last, takes_time=True)
  )
  (4): Lambda(LayerNorm(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, (1024,), eps=1e-05, elementwise_affine=True))
  (5): Linear(0.032 M, 0.189% Params, 0.0 GMac, 0.008% MACs, in_features=1024, out_features=31, bias=True, channel_dim=1)
)
Model FLOPs: 411997217.0
Model params: 16824351
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| Active memory         |   67522 KB |   69842 KB |  260138 KB |  192616 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   83968 KB |   83968 KB |   83968 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16445 KB |   16445 KB |  302954 KB |  286508 KB |
|---------------------------------------------------------------------------|
| Allocations           |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| Active allocs         |      34    |     103    |    3333    |    3299    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      12    |    1458    |    1448    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Memory state pre, max, post inference: 69143040 71518720 69143040
Loaded tvseries_anet_features.pickle
Loaded tvseries_anet_features.pickle
Start training
Epoch: [1]  [   0/1511]  eta: 1:09:23  lr: 0.000100  loss: 2.6369 (2.6369)  labels_encoder: 2.6369 (2.6369)  labels_encoder_unscaled: 2.6369 (2.6369)  time: 2.7552  data: 2.1167  max mem: 1013
Epoch: [1]  [  50/1511]  eta: 0:04:27  lr: 0.000100  loss: 1.0148 (1.1121)  labels_encoder: 1.0148 (1.1121)  labels_encoder_unscaled: 1.0148 (1.1121)  time: 0.1183  data: 0.0002  max mem: 1206
Epoch: [1]  [ 100/1511]  eta: 0:03:35  lr: 0.000100  loss: 0.9774 (1.0542)  labels_encoder: 0.9774 (1.0542)  labels_encoder_unscaled: 0.9774 (1.0542)  time: 0.1216  data: 0.0002  max mem: 1206
Epoch: [1]  [ 150/1511]  eta: 0:03:11  lr: 0.000100  loss: 0.9190 (1.0070)  labels_encoder: 0.9190 (1.0070)  labels_encoder_unscaled: 0.9190 (1.0070)  time: 0.1166  data: 0.0002  max mem: 1206
Epoch: [1]  [ 200/1511]  eta: 0:02:58  lr: 0.000100  loss: 0.8480 (0.9728)  labels_encoder: 0.8480 (0.9728)  labels_encoder_unscaled: 0.8480 (0.9728)  time: 0.1284  data: 0.0002  max mem: 1206
Epoch: [1]  [ 250/1511]  eta: 0:02:51  lr: 0.000100  loss: 0.8302 (0.9521)  labels_encoder: 0.8302 (0.9521)  labels_encoder_unscaled: 0.8302 (0.9521)  time: 0.1387  data: 0.0002  max mem: 1206
Epoch: [1]  [ 300/1511]  eta: 0:02:44  lr: 0.000100  loss: 0.9061 (0.9432)  labels_encoder: 0.9061 (0.9432)  labels_encoder_unscaled: 0.9061 (0.9432)  time: 0.1408  data: 0.0002  max mem: 1206
Epoch: [1]  [ 350/1511]  eta: 0:02:36  lr: 0.000100  loss: 0.8642 (0.9315)  labels_encoder: 0.8642 (0.9315)  labels_encoder_unscaled: 0.8642 (0.9315)  time: 0.1229  data: 0.0002  max mem: 1206
Epoch: [1]  [ 400/1511]  eta: 0:02:28  lr: 0.000100  loss: 0.7978 (0.9183)  labels_encoder: 0.7978 (0.9183)  labels_encoder_unscaled: 0.7978 (0.9183)  time: 0.1233  data: 0.0002  max mem: 1206
Epoch: [1]  [ 450/1511]  eta: 0:02:21  lr: 0.000100  loss: 0.7533 (0.9079)  labels_encoder: 0.7533 (0.9079)  labels_encoder_unscaled: 0.7533 (0.9079)  time: 0.1236  data: 0.0002  max mem: 1206
Epoch: [1]  [ 500/1511]  eta: 0:02:14  lr: 0.000100  loss: 0.8138 (0.8986)  labels_encoder: 0.8138 (0.8986)  labels_encoder_unscaled: 0.8138 (0.8986)  time: 0.1246  data: 0.0002  max mem: 1206
Epoch: [1]  [ 550/1511]  eta: 0:02:07  lr: 0.000100  loss: 0.7903 (0.8882)  labels_encoder: 0.7903 (0.8882)  labels_encoder_unscaled: 0.7903 (0.8882)  time: 0.1215  data: 0.0002  max mem: 1206
Epoch: [1]  [ 600/1511]  eta: 0:01:59  lr: 0.000100  loss: 0.7864 (0.8803)  labels_encoder: 0.7864 (0.8803)  labels_encoder_unscaled: 0.7864 (0.8803)  time: 0.1200  data: 0.0002  max mem: 1206
Epoch: [1]  [ 650/1511]  eta: 0:01:52  lr: 0.000100  loss: 0.7284 (0.8731)  labels_encoder: 0.7284 (0.8731)  labels_encoder_unscaled: 0.7284 (0.8731)  time: 0.1217  data: 0.0002  max mem: 1206
Epoch: [1]  [ 700/1511]  eta: 0:01:46  lr: 0.000100  loss: 0.7297 (0.8653)  labels_encoder: 0.7297 (0.8653)  labels_encoder_unscaled: 0.7297 (0.8653)  time: 0.1403  data: 0.0003  max mem: 1206
Epoch: [1]  [ 750/1511]  eta: 0:01:39  lr: 0.000100  loss: 0.7943 (0.8606)  labels_encoder: 0.7943 (0.8606)  labels_encoder_unscaled: 0.7943 (0.8606)  time: 0.1226  data: 0.0002  max mem: 1206
Epoch: [1]  [ 800/1511]  eta: 0:01:33  lr: 0.000100  loss: 0.6967 (0.8517)  labels_encoder: 0.6967 (0.8517)  labels_encoder_unscaled: 0.6967 (0.8517)  time: 0.1268  data: 0.0002  max mem: 1206
Epoch: [1]  [ 850/1511]  eta: 0:01:27  lr: 0.000100  loss: 0.7277 (0.8454)  labels_encoder: 0.7277 (0.8454)  labels_encoder_unscaled: 0.7277 (0.8454)  time: 0.1412  data: 0.0002  max mem: 1206
Epoch: [1]  [ 900/1511]  eta: 0:01:20  lr: 0.000100  loss: 0.6780 (0.8374)  labels_encoder: 0.6780 (0.8374)  labels_encoder_unscaled: 0.6780 (0.8374)  time: 0.1344  data: 0.0002  max mem: 1206
Epoch: [1]  [ 950/1511]  eta: 0:01:13  lr: 0.000100  loss: 0.6433 (0.8308)  labels_encoder: 0.6433 (0.8308)  labels_encoder_unscaled: 0.6433 (0.8308)  time: 0.1262  data: 0.0002  max mem: 1206
Epoch: [1]  [1000/1511]  eta: 0:01:07  lr: 0.000100  loss: 0.7089 (0.8236)  labels_encoder: 0.7089 (0.8236)  labels_encoder_unscaled: 0.7089 (0.8236)  time: 0.1240  data: 0.0002  max mem: 1206
Epoch: [1]  [1050/1511]  eta: 0:01:00  lr: 0.000100  loss: 0.7216 (0.8185)  labels_encoder: 0.7216 (0.8185)  labels_encoder_unscaled: 0.7216 (0.8185)  time: 0.1325  data: 0.0002  max mem: 1206
Epoch: [1]  [1100/1511]  eta: 0:00:54  lr: 0.000100  loss: 0.6911 (0.8142)  labels_encoder: 0.6911 (0.8142)  labels_encoder_unscaled: 0.6911 (0.8142)  time: 0.1248  data: 0.0002  max mem: 1206
Epoch: [1]  [1150/1511]  eta: 0:00:47  lr: 0.000100  loss: 0.6622 (0.8082)  labels_encoder: 0.6622 (0.8082)  labels_encoder_unscaled: 0.6622 (0.8082)  time: 0.1349  data: 0.0003  max mem: 1206
Epoch: [1]  [1200/1511]  eta: 0:00:40  lr: 0.000100  loss: 0.6760 (0.8035)  labels_encoder: 0.6760 (0.8035)  labels_encoder_unscaled: 0.6760 (0.8035)  time: 0.1156  data: 0.0002  max mem: 1206
Epoch: [1]  [1250/1511]  eta: 0:00:34  lr: 0.000100  loss: 0.7001 (0.7993)  labels_encoder: 0.7001 (0.7993)  labels_encoder_unscaled: 0.7001 (0.7993)  time: 0.1286  data: 0.0002  max mem: 1206
Epoch: [1]  [1300/1511]  eta: 0:00:27  lr: 0.000100  loss: 0.7197 (0.7956)  labels_encoder: 0.7197 (0.7956)  labels_encoder_unscaled: 0.7197 (0.7956)  time: 0.1448  data: 0.0003  max mem: 1206
Epoch: [1]  [1350/1511]  eta: 0:00:21  lr: 0.000100  loss: 0.6449 (0.7914)  labels_encoder: 0.6449 (0.7914)  labels_encoder_unscaled: 0.6449 (0.7914)  time: 0.1231  data: 0.0002  max mem: 1206
Epoch: [1]  [1400/1511]  eta: 0:00:14  lr: 0.000100  loss: 0.7079 (0.7877)  labels_encoder: 0.7079 (0.7877)  labels_encoder_unscaled: 0.7079 (0.7877)  time: 0.1225  data: 0.0002  max mem: 1206
Epoch: [1]  [1450/1511]  eta: 0:00:07  lr: 0.000100  loss: 0.6932 (0.7837)  labels_encoder: 0.6932 (0.7837)  labels_encoder_unscaled: 0.6932 (0.7837)  time: 0.1391  data: 0.0002  max mem: 1206
Epoch: [1]  [1500/1511]  eta: 0:00:01  lr: 0.000100  loss: 0.6134 (0.7788)  labels_encoder: 0.6134 (0.7788)  labels_encoder_unscaled: 0.6134 (0.7788)  time: 0.1179  data: 0.0003  max mem: 1206
Epoch: [1]  [1510/1511]  eta: 0:00:00  lr: 0.000100  loss: 0.6337 (0.7781)  labels_encoder: 0.6337 (0.7781)  labels_encoder_unscaled: 0.6337 (0.7781)  time: 0.1154  data: 0.0003  max mem: 1206
Epoch: [1] Total time: 0:03:17 (0.1306 s / it)
Averaged stats: lr: 0.000100  loss: 0.6337 (0.7781)  labels_encoder: 0.6337 (0.7781)  labels_encoder_unscaled: 0.6337 (0.7781)
Test:  [  0/559]  eta: 0:18:05  loss: 0.6125 (0.6125)  labels_encoder: 0.6125 (0.6125)  labels_encoder_unscaled: 0.6125 (0.6125)  time: 1.9420  data: 1.9077  max mem: 1206
Test:  [ 50/559]  eta: 0:00:53  loss: 0.8783 (0.9224)  labels_encoder: 0.8783 (0.9224)  labels_encoder_unscaled: 0.8783 (0.9224)  time: 0.0601  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:39  loss: 0.3527 (0.8659)  labels_encoder: 0.3527 (0.8659)  labels_encoder_unscaled: 0.3527 (0.8659)  time: 0.0601  data: 0.0066  max mem: 1206
Test:  [150/559]  eta: 0:00:32  loss: 0.5362 (0.8834)  labels_encoder: 0.5362 (0.8834)  labels_encoder_unscaled: 0.5362 (0.8834)  time: 0.0708  data: 0.0105  max mem: 1206
Test:  [200/559]  eta: 0:00:27  loss: 0.7912 (0.8347)  labels_encoder: 0.7912 (0.8347)  labels_encoder_unscaled: 0.7912 (0.8347)  time: 0.0626  data: 0.0234  max mem: 1206
Test:  [250/559]  eta: 0:00:22  loss: 0.8014 (0.9517)  labels_encoder: 0.8014 (0.9517)  labels_encoder_unscaled: 0.8014 (0.9517)  time: 0.0609  data: 0.0175  max mem: 1206
Test:  [300/559]  eta: 0:00:18  loss: 1.2434 (1.0128)  labels_encoder: 1.2434 (1.0128)  labels_encoder_unscaled: 1.2434 (1.0128)  time: 0.0722  data: 0.0133  max mem: 1206
Test:  [350/559]  eta: 0:00:14  loss: 0.7707 (1.0392)  labels_encoder: 0.7707 (1.0392)  labels_encoder_unscaled: 0.7707 (1.0392)  time: 0.0689  data: 0.0278  max mem: 1206
Test:  [400/559]  eta: 0:00:11  loss: 0.3794 (1.0166)  labels_encoder: 0.3794 (1.0166)  labels_encoder_unscaled: 0.3794 (1.0166)  time: 0.0689  data: 0.0221  max mem: 1206
Test:  [450/559]  eta: 0:00:07  loss: 0.5122 (0.9947)  labels_encoder: 0.5122 (0.9947)  labels_encoder_unscaled: 0.5122 (0.9947)  time: 0.0689  data: 0.0327  max mem: 1206
Test:  [500/559]  eta: 0:00:04  loss: 0.5484 (0.9726)  labels_encoder: 0.5484 (0.9726)  labels_encoder_unscaled: 0.5484 (0.9726)  time: 0.0635  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7584 (0.9439)  labels_encoder: 0.7584 (0.9439)  labels_encoder_unscaled: 0.7584 (0.9439)  time: 0.0568  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.4479 (0.9369)  labels_encoder: 0.4479 (0.9369)  labels_encoder_unscaled: 0.4479 (0.9369)  time: 0.0497  data: 0.0001  max mem: 1206
Test: Total time: 0:00:38 (0.0690 s / it)
Averaged stats: loss: 0.4479 (0.9369)  labels_encoder: 0.4479 (0.9369)  labels_encoder_unscaled: 0.4479 (0.9369)
(21, 71496)
(21, 71496)
[Epoch-1] [IDU-tvseries_anet_features.pickle] mAP: 0.1113, mcAP: 0.8699

BaseballPitch: 0.0249
BasketballDunk: 0.0867
Billiards: 0.0054
CleanAndJerk: 0.3985
CliffDiving: 0.4690
CricketBowling: 0.0483
CricketShot: 0.0644
Diving: 0.0080
FrisbeeCatch: 0.1078
GolfSwing: 0.0603
HammerThrow: 0.0761
HighJump: 0.0209
JavelinThrow: 0.0506
LongJump: 0.2186
PoleVault: 0.0818
Shotput: 0.1134
SoccerPenalty: 0.0390
TennisSwing: 0.1889
ThrowDiscus: 0.0395
VolleyballSpiking: 0.1248
Epoch: [2]  [   0/1511]  eta: 0:56:12  lr: 0.000010  loss: 0.8210 (0.8210)  labels_encoder: 0.8210 (0.8210)  labels_encoder_unscaled: 0.8210 (0.8210)  time: 2.2319  data: 2.0503  max mem: 1206
Epoch: [2]  [  50/1511]  eta: 0:04:07  lr: 0.000010  loss: 0.5731 (0.6036)  labels_encoder: 0.5731 (0.6036)  labels_encoder_unscaled: 0.5731 (0.6036)  time: 0.1235  data: 0.0002  max mem: 1206
Epoch: [2]  [ 100/1511]  eta: 0:03:29  lr: 0.000010  loss: 0.6179 (0.6240)  labels_encoder: 0.6179 (0.6240)  labels_encoder_unscaled: 0.6179 (0.6240)  time: 0.1273  data: 0.0002  max mem: 1206
Epoch: [2]  [ 150/1511]  eta: 0:03:10  lr: 0.000010  loss: 0.6069 (0.6219)  labels_encoder: 0.6069 (0.6219)  labels_encoder_unscaled: 0.6069 (0.6219)  time: 0.1222  data: 0.0002  max mem: 1206
Epoch: [2]  [ 200/1511]  eta: 0:02:58  lr: 0.000010  loss: 0.5654 (0.6171)  labels_encoder: 0.5654 (0.6171)  labels_encoder_unscaled: 0.5654 (0.6171)  time: 0.1268  data: 0.0002  max mem: 1206
Epoch: [2]  [ 250/1511]  eta: 0:02:47  lr: 0.000010  loss: 0.6194 (0.6142)  labels_encoder: 0.6194 (0.6142)  labels_encoder_unscaled: 0.6194 (0.6142)  time: 0.1168  data: 0.0002  max mem: 1206
Epoch: [2]  [ 300/1511]  eta: 0:02:41  lr: 0.000010  loss: 0.5557 (0.6094)  labels_encoder: 0.5557 (0.6094)  labels_encoder_unscaled: 0.5557 (0.6094)  time: 0.1485  data: 0.0003  max mem: 1206
Epoch: [2]  [ 350/1511]  eta: 0:02:34  lr: 0.000010  loss: 0.6052 (0.6091)  labels_encoder: 0.6052 (0.6091)  labels_encoder_unscaled: 0.6052 (0.6091)  time: 0.1322  data: 0.0002  max mem: 1206
Epoch: [2]  [ 400/1511]  eta: 0:02:27  lr: 0.000010  loss: 0.5452 (0.6060)  labels_encoder: 0.5452 (0.6060)  labels_encoder_unscaled: 0.5452 (0.6060)  time: 0.1355  data: 0.0002  max mem: 1206
Epoch: [2]  [ 450/1511]  eta: 0:02:19  lr: 0.000010  loss: 0.5358 (0.6013)  labels_encoder: 0.5358 (0.6013)  labels_encoder_unscaled: 0.5358 (0.6013)  time: 0.1208  data: 0.0002  max mem: 1206
Epoch: [2]  [ 500/1511]  eta: 0:02:11  lr: 0.000010  loss: 0.5196 (0.5989)  labels_encoder: 0.5196 (0.5989)  labels_encoder_unscaled: 0.5196 (0.5989)  time: 0.1211  data: 0.0002  max mem: 1206
Epoch: [2]  [ 550/1511]  eta: 0:02:04  lr: 0.000010  loss: 0.5534 (0.5962)  labels_encoder: 0.5534 (0.5962)  labels_encoder_unscaled: 0.5534 (0.5962)  time: 0.1257  data: 0.0002  max mem: 1206
Epoch: [2]  [ 600/1511]  eta: 0:01:58  lr: 0.000010  loss: 0.6204 (0.5949)  labels_encoder: 0.6204 (0.5949)  labels_encoder_unscaled: 0.6204 (0.5949)  time: 0.1327  data: 0.0002  max mem: 1206
Epoch: [2]  [ 650/1511]  eta: 0:01:51  lr: 0.000010  loss: 0.6331 (0.5969)  labels_encoder: 0.6331 (0.5969)  labels_encoder_unscaled: 0.6331 (0.5969)  time: 0.1273  data: 0.0002  max mem: 1206
Epoch: [2]  [ 700/1511]  eta: 0:01:44  lr: 0.000010  loss: 0.5730 (0.5943)  labels_encoder: 0.5730 (0.5943)  labels_encoder_unscaled: 0.5730 (0.5943)  time: 0.1225  data: 0.0002  max mem: 1206
Epoch: [2]  [ 750/1511]  eta: 0:01:38  lr: 0.000010  loss: 0.5708 (0.5928)  labels_encoder: 0.5708 (0.5928)  labels_encoder_unscaled: 0.5708 (0.5928)  time: 0.1226  data: 0.0002  max mem: 1206
Epoch: [2]  [ 800/1511]  eta: 0:01:31  lr: 0.000010  loss: 0.5347 (0.5919)  labels_encoder: 0.5347 (0.5919)  labels_encoder_unscaled: 0.5347 (0.5919)  time: 0.1225  data: 0.0002  max mem: 1206
Epoch: [2]  [ 850/1511]  eta: 0:01:25  lr: 0.000010  loss: 0.5932 (0.5924)  labels_encoder: 0.5932 (0.5924)  labels_encoder_unscaled: 0.5932 (0.5924)  time: 0.1351  data: 0.0002  max mem: 1206
Epoch: [2]  [ 900/1511]  eta: 0:01:18  lr: 0.000010  loss: 0.5253 (0.5900)  labels_encoder: 0.5253 (0.5900)  labels_encoder_unscaled: 0.5253 (0.5900)  time: 0.1278  data: 0.0002  max mem: 1206
Epoch: [2]  [ 950/1511]  eta: 0:01:12  lr: 0.000010  loss: 0.5120 (0.5882)  labels_encoder: 0.5120 (0.5882)  labels_encoder_unscaled: 0.5120 (0.5882)  time: 0.1307  data: 0.0002  max mem: 1206
Epoch: [2]  [1000/1511]  eta: 0:01:05  lr: 0.000010  loss: 0.5251 (0.5867)  labels_encoder: 0.5251 (0.5867)  labels_encoder_unscaled: 0.5251 (0.5867)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [2]  [1050/1511]  eta: 0:00:59  lr: 0.000010  loss: 0.5751 (0.5858)  labels_encoder: 0.5751 (0.5858)  labels_encoder_unscaled: 0.5751 (0.5858)  time: 0.1301  data: 0.0002  max mem: 1206
Epoch: [2]  [1100/1511]  eta: 0:00:52  lr: 0.000010  loss: 0.5425 (0.5844)  labels_encoder: 0.5425 (0.5844)  labels_encoder_unscaled: 0.5425 (0.5844)  time: 0.1376  data: 0.0002  max mem: 1206
Epoch: [2]  [1150/1511]  eta: 0:00:46  lr: 0.000010  loss: 0.5669 (0.5840)  labels_encoder: 0.5669 (0.5840)  labels_encoder_unscaled: 0.5669 (0.5840)  time: 0.1325  data: 0.0002  max mem: 1206
Epoch: [2]  [1200/1511]  eta: 0:00:40  lr: 0.000010  loss: 0.4750 (0.5832)  labels_encoder: 0.4750 (0.5832)  labels_encoder_unscaled: 0.4750 (0.5832)  time: 0.1398  data: 0.0002  max mem: 1206
Epoch: [2]  [1250/1511]  eta: 0:00:33  lr: 0.000010  loss: 0.5569 (0.5829)  labels_encoder: 0.5569 (0.5829)  labels_encoder_unscaled: 0.5569 (0.5829)  time: 0.1290  data: 0.0002  max mem: 1206
Epoch: [2]  [1300/1511]  eta: 0:00:27  lr: 0.000010  loss: 0.5636 (0.5825)  labels_encoder: 0.5636 (0.5825)  labels_encoder_unscaled: 0.5636 (0.5825)  time: 0.1234  data: 0.0002  max mem: 1206
Epoch: [2]  [1350/1511]  eta: 0:00:20  lr: 0.000010  loss: 0.5484 (0.5812)  labels_encoder: 0.5484 (0.5812)  labels_encoder_unscaled: 0.5484 (0.5812)  time: 0.1225  data: 0.0002  max mem: 1206
Epoch: [2]  [1400/1511]  eta: 0:00:14  lr: 0.000010  loss: 0.5288 (0.5799)  labels_encoder: 0.5288 (0.5799)  labels_encoder_unscaled: 0.5288 (0.5799)  time: 0.1217  data: 0.0002  max mem: 1206
Epoch: [2]  [1450/1511]  eta: 0:00:07  lr: 0.000010  loss: 0.5559 (0.5795)  labels_encoder: 0.5559 (0.5795)  labels_encoder_unscaled: 0.5559 (0.5795)  time: 0.1306  data: 0.0002  max mem: 1206
Epoch: [2]  [1500/1511]  eta: 0:00:01  lr: 0.000010  loss: 0.5762 (0.5788)  labels_encoder: 0.5762 (0.5788)  labels_encoder_unscaled: 0.5762 (0.5788)  time: 0.1303  data: 0.0004  max mem: 1206
Epoch: [2]  [1510/1511]  eta: 0:00:00  lr: 0.000010  loss: 0.5517 (0.5785)  labels_encoder: 0.5517 (0.5785)  labels_encoder_unscaled: 0.5517 (0.5785)  time: 0.1209  data: 0.0003  max mem: 1206
Epoch: [2] Total time: 0:03:15 (0.1294 s / it)
Averaged stats: lr: 0.000010  loss: 0.5517 (0.5785)  labels_encoder: 0.5517 (0.5785)  labels_encoder_unscaled: 0.5517 (0.5785)
Test:  [  0/559]  eta: 0:17:39  loss: 0.7483 (0.7483)  labels_encoder: 0.7483 (0.7483)  labels_encoder_unscaled: 0.7483 (0.7483)  time: 1.8962  data: 1.8357  max mem: 1206
Test:  [ 50/559]  eta: 0:00:48  loss: 0.8165 (0.8951)  labels_encoder: 0.8165 (0.8951)  labels_encoder_unscaled: 0.8165 (0.8951)  time: 0.0568  data: 0.0010  max mem: 1206
Test:  [100/559]  eta: 0:00:35  loss: 0.4130 (0.8503)  labels_encoder: 0.4130 (0.8503)  labels_encoder_unscaled: 0.4130 (0.8503)  time: 0.0606  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:29  loss: 0.5315 (0.8756)  labels_encoder: 0.5315 (0.8756)  labels_encoder_unscaled: 0.5315 (0.8756)  time: 0.0629  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:24  loss: 0.5671 (0.8202)  labels_encoder: 0.5671 (0.8202)  labels_encoder_unscaled: 0.5671 (0.8202)  time: 0.0518  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:20  loss: 0.6711 (0.9409)  labels_encoder: 0.6711 (0.9409)  labels_encoder_unscaled: 0.6711 (0.9409)  time: 0.0637  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.2818 (0.9986)  labels_encoder: 1.2818 (0.9986)  labels_encoder_unscaled: 1.2818 (0.9986)  time: 0.0637  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7407 (1.0172)  labels_encoder: 0.7407 (1.0172)  labels_encoder_unscaled: 0.7407 (1.0172)  time: 0.0578  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3142 (0.9906)  labels_encoder: 0.3142 (0.9906)  labels_encoder_unscaled: 0.3142 (0.9906)  time: 0.0568  data: 0.0010  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.3989 (0.9653)  labels_encoder: 0.3989 (0.9653)  labels_encoder_unscaled: 0.3989 (0.9653)  time: 0.0587  data: 0.0011  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.5662 (0.9432)  labels_encoder: 0.5662 (0.9432)  labels_encoder_unscaled: 0.5662 (0.9432)  time: 0.0522  data: 0.0001  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7242 (0.9189)  labels_encoder: 0.7242 (0.9189)  labels_encoder_unscaled: 0.7242 (0.9189)  time: 0.0468  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.4374 (0.9126)  labels_encoder: 0.4374 (0.9126)  labels_encoder_unscaled: 0.4374 (0.9126)  time: 0.0387  data: 0.0001  max mem: 1206
Test: Total time: 0:00:34 (0.0612 s / it)
Averaged stats: loss: 0.4374 (0.9126)  labels_encoder: 0.4374 (0.9126)  labels_encoder_unscaled: 0.4374 (0.9126)
(21, 71496)
(21, 71496)
[Epoch-2] [IDU-tvseries_anet_features.pickle] mAP: 0.1256, mcAP: 0.8837

BaseballPitch: 0.0324
BasketballDunk: 0.0910
Billiards: 0.0056
CleanAndJerk: 0.3785
CliffDiving: 0.4088
CricketBowling: 0.1063
CricketShot: 0.0987
Diving: 0.0057
FrisbeeCatch: 0.1475
GolfSwing: 0.0699
HammerThrow: 0.1572
HighJump: 0.0461
JavelinThrow: 0.0707
LongJump: 0.2410
PoleVault: 0.0968
Shotput: 0.1311
SoccerPenalty: 0.0504
TennisSwing: 0.2064
ThrowDiscus: 0.0458
VolleyballSpiking: 0.1228
Epoch: [3]  [   0/1511]  eta: 0:50:07  lr: 0.000001  loss: 0.4239 (0.4239)  labels_encoder: 0.4239 (0.4239)  labels_encoder_unscaled: 0.4239 (0.4239)  time: 1.9906  data: 1.8489  max mem: 1206
Epoch: [3]  [  50/1511]  eta: 0:04:05  lr: 0.000001  loss: 0.5592 (0.5462)  labels_encoder: 0.5592 (0.5462)  labels_encoder_unscaled: 0.5592 (0.5462)  time: 0.1247  data: 0.0002  max mem: 1206
Epoch: [3]  [ 100/1511]  eta: 0:03:24  lr: 0.000001  loss: 0.5241 (0.5357)  labels_encoder: 0.5241 (0.5357)  labels_encoder_unscaled: 0.5241 (0.5357)  time: 0.1243  data: 0.0002  max mem: 1206
Epoch: [3]  [ 150/1511]  eta: 0:03:06  lr: 0.000001  loss: 0.5544 (0.5461)  labels_encoder: 0.5544 (0.5461)  labels_encoder_unscaled: 0.5544 (0.5461)  time: 0.1199  data: 0.0002  max mem: 1206
Epoch: [3]  [ 200/1511]  eta: 0:02:56  lr: 0.000001  loss: 0.4924 (0.5376)  labels_encoder: 0.4924 (0.5376)  labels_encoder_unscaled: 0.4924 (0.5376)  time: 0.1314  data: 0.0002  max mem: 1206
Epoch: [3]  [ 250/1511]  eta: 0:02:48  lr: 0.000001  loss: 0.5473 (0.5369)  labels_encoder: 0.5473 (0.5369)  labels_encoder_unscaled: 0.5473 (0.5369)  time: 0.1370  data: 0.0002  max mem: 1206
Epoch: [3]  [ 300/1511]  eta: 0:02:43  lr: 0.000001  loss: 0.5337 (0.5386)  labels_encoder: 0.5337 (0.5386)  labels_encoder_unscaled: 0.5337 (0.5386)  time: 0.1426  data: 0.0002  max mem: 1206
Epoch: [3]  [ 350/1511]  eta: 0:02:36  lr: 0.000001  loss: 0.5209 (0.5398)  labels_encoder: 0.5209 (0.5398)  labels_encoder_unscaled: 0.5209 (0.5398)  time: 0.1325  data: 0.0002  max mem: 1206
Epoch: [3]  [ 400/1511]  eta: 0:02:30  lr: 0.000001  loss: 0.4714 (0.5388)  labels_encoder: 0.4714 (0.5388)  labels_encoder_unscaled: 0.4714 (0.5388)  time: 0.1425  data: 0.0002  max mem: 1206
Epoch: [3]  [ 450/1511]  eta: 0:02:24  lr: 0.000001  loss: 0.5256 (0.5381)  labels_encoder: 0.5256 (0.5381)  labels_encoder_unscaled: 0.5256 (0.5381)  time: 0.1413  data: 0.0002  max mem: 1206
Epoch: [3]  [ 500/1511]  eta: 0:02:17  lr: 0.000001  loss: 0.4799 (0.5361)  labels_encoder: 0.4799 (0.5361)  labels_encoder_unscaled: 0.4799 (0.5361)  time: 0.1405  data: 0.0002  max mem: 1206
Epoch: [3]  [ 550/1511]  eta: 0:02:11  lr: 0.000001  loss: 0.5673 (0.5402)  labels_encoder: 0.5673 (0.5402)  labels_encoder_unscaled: 0.5673 (0.5402)  time: 0.1423  data: 0.0002  max mem: 1206
Epoch: [3]  [ 600/1511]  eta: 0:02:04  lr: 0.000001  loss: 0.5500 (0.5405)  labels_encoder: 0.5500 (0.5405)  labels_encoder_unscaled: 0.5500 (0.5405)  time: 0.1363  data: 0.0002  max mem: 1206
Epoch: [3]  [ 650/1511]  eta: 0:01:57  lr: 0.000001  loss: 0.5336 (0.5391)  labels_encoder: 0.5336 (0.5391)  labels_encoder_unscaled: 0.5336 (0.5391)  time: 0.1330  data: 0.0002  max mem: 1206
Epoch: [3]  [ 700/1511]  eta: 0:01:50  lr: 0.000001  loss: 0.4941 (0.5368)  labels_encoder: 0.4941 (0.5368)  labels_encoder_unscaled: 0.4941 (0.5368)  time: 0.1414  data: 0.0002  max mem: 1206
Epoch: [3]  [ 750/1511]  eta: 0:01:44  lr: 0.000001  loss: 0.5408 (0.5374)  labels_encoder: 0.5408 (0.5374)  labels_encoder_unscaled: 0.5408 (0.5374)  time: 0.1591  data: 0.0003  max mem: 1206
Epoch: [3]  [ 800/1511]  eta: 0:01:37  lr: 0.000001  loss: 0.5234 (0.5372)  labels_encoder: 0.5234 (0.5372)  labels_encoder_unscaled: 0.5234 (0.5372)  time: 0.1290  data: 0.0002  max mem: 1206
Epoch: [3]  [ 850/1511]  eta: 0:01:30  lr: 0.000001  loss: 0.5228 (0.5378)  labels_encoder: 0.5228 (0.5378)  labels_encoder_unscaled: 0.5228 (0.5378)  time: 0.1192  data: 0.0002  max mem: 1206
Epoch: [3]  [ 900/1511]  eta: 0:01:23  lr: 0.000001  loss: 0.5245 (0.5372)  labels_encoder: 0.5245 (0.5372)  labels_encoder_unscaled: 0.5245 (0.5372)  time: 0.1237  data: 0.0002  max mem: 1206
Epoch: [3]  [ 950/1511]  eta: 0:01:16  lr: 0.000001  loss: 0.5439 (0.5368)  labels_encoder: 0.5439 (0.5368)  labels_encoder_unscaled: 0.5439 (0.5368)  time: 0.1268  data: 0.0002  max mem: 1206
Epoch: [3]  [1000/1511]  eta: 0:01:09  lr: 0.000001  loss: 0.5265 (0.5360)  labels_encoder: 0.5265 (0.5360)  labels_encoder_unscaled: 0.5265 (0.5360)  time: 0.1363  data: 0.0002  max mem: 1206
Epoch: [3]  [1050/1511]  eta: 0:01:02  lr: 0.000001  loss: 0.5389 (0.5375)  labels_encoder: 0.5389 (0.5375)  labels_encoder_unscaled: 0.5389 (0.5375)  time: 0.1282  data: 0.0002  max mem: 1206
Epoch: [3]  [1100/1511]  eta: 0:00:55  lr: 0.000001  loss: 0.4965 (0.5370)  labels_encoder: 0.4965 (0.5370)  labels_encoder_unscaled: 0.4965 (0.5370)  time: 0.1317  data: 0.0002  max mem: 1206
Epoch: [3]  [1150/1511]  eta: 0:00:48  lr: 0.000001  loss: 0.5310 (0.5375)  labels_encoder: 0.5310 (0.5375)  labels_encoder_unscaled: 0.5310 (0.5375)  time: 0.1158  data: 0.0002  max mem: 1206
Epoch: [3]  [1200/1511]  eta: 0:00:41  lr: 0.000001  loss: 0.4781 (0.5365)  labels_encoder: 0.4781 (0.5365)  labels_encoder_unscaled: 0.4781 (0.5365)  time: 0.1364  data: 0.0002  max mem: 1206
Epoch: [3]  [1250/1511]  eta: 0:00:35  lr: 0.000001  loss: 0.5384 (0.5370)  labels_encoder: 0.5384 (0.5370)  labels_encoder_unscaled: 0.5384 (0.5370)  time: 0.1297  data: 0.0002  max mem: 1206
Epoch: [3]  [1300/1511]  eta: 0:00:28  lr: 0.000001  loss: 0.4837 (0.5365)  labels_encoder: 0.4837 (0.5365)  labels_encoder_unscaled: 0.4837 (0.5365)  time: 0.1334  data: 0.0002  max mem: 1206
Epoch: [3]  [1350/1511]  eta: 0:00:21  lr: 0.000001  loss: 0.5214 (0.5366)  labels_encoder: 0.5214 (0.5366)  labels_encoder_unscaled: 0.5214 (0.5366)  time: 0.1331  data: 0.0002  max mem: 1206
Epoch: [3]  [1400/1511]  eta: 0:00:14  lr: 0.000001  loss: 0.4978 (0.5362)  labels_encoder: 0.4978 (0.5362)  labels_encoder_unscaled: 0.4978 (0.5362)  time: 0.1221  data: 0.0002  max mem: 1206
Epoch: [3]  [1450/1511]  eta: 0:00:08  lr: 0.000001  loss: 0.5243 (0.5365)  labels_encoder: 0.5243 (0.5365)  labels_encoder_unscaled: 0.5243 (0.5365)  time: 0.1336  data: 0.0002  max mem: 1206
Epoch: [3]  [1500/1511]  eta: 0:00:01  lr: 0.000001  loss: 0.5114 (0.5367)  labels_encoder: 0.5114 (0.5367)  labels_encoder_unscaled: 0.5114 (0.5367)  time: 0.1286  data: 0.0003  max mem: 1206
Epoch: [3]  [1510/1511]  eta: 0:00:00  lr: 0.000001  loss: 0.5114 (0.5369)  labels_encoder: 0.5114 (0.5369)  labels_encoder_unscaled: 0.5114 (0.5369)  time: 0.1155  data: 0.0002  max mem: 1206
Epoch: [3] Total time: 0:03:23 (0.1344 s / it)
Averaged stats: lr: 0.000001  loss: 0.5114 (0.5369)  labels_encoder: 0.5114 (0.5369)  labels_encoder_unscaled: 0.5114 (0.5369)
Test:  [  0/559]  eta: 0:20:35  loss: 0.7068 (0.7068)  labels_encoder: 0.7068 (0.7068)  labels_encoder_unscaled: 0.7068 (0.7068)  time: 2.2095  data: 2.1599  max mem: 1206
Test:  [ 50/559]  eta: 0:00:50  loss: 0.8433 (0.8836)  labels_encoder: 0.8433 (0.8836)  labels_encoder_unscaled: 0.8433 (0.8836)  time: 0.0554  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:36  loss: 0.5422 (0.8426)  labels_encoder: 0.5422 (0.8426)  labels_encoder_unscaled: 0.5422 (0.8426)  time: 0.0602  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:30  loss: 0.5734 (0.8656)  labels_encoder: 0.5734 (0.8656)  labels_encoder_unscaled: 0.5734 (0.8656)  time: 0.0663  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:25  loss: 0.7175 (0.8087)  labels_encoder: 0.7175 (0.8087)  labels_encoder_unscaled: 0.7175 (0.8087)  time: 0.0469  data: 0.0001  max mem: 1206
Test:  [250/559]  eta: 0:00:21  loss: 0.6942 (0.9251)  labels_encoder: 0.6942 (0.9251)  labels_encoder_unscaled: 0.6942 (0.9251)  time: 0.0542  data: 0.0014  max mem: 1206
Test:  [300/559]  eta: 0:00:17  loss: 1.1021 (0.9814)  labels_encoder: 1.1021 (0.9814)  labels_encoder_unscaled: 1.1021 (0.9814)  time: 0.0627  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7384 (1.0045)  labels_encoder: 0.7384 (1.0045)  labels_encoder_unscaled: 0.7384 (1.0045)  time: 0.0507  data: 0.0001  max mem: 1206
Test:  [400/559]  eta: 0:00:10  loss: 0.3373 (0.9767)  labels_encoder: 0.3373 (0.9767)  labels_encoder_unscaled: 0.3373 (0.9767)  time: 0.0597  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4106 (0.9520)  labels_encoder: 0.4106 (0.9520)  labels_encoder_unscaled: 0.4106 (0.9520)  time: 0.0632  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6022 (0.9329)  labels_encoder: 0.6022 (0.9329)  labels_encoder_unscaled: 0.6022 (0.9329)  time: 0.0605  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.8006 (0.9066)  labels_encoder: 0.8006 (0.9066)  labels_encoder_unscaled: 0.8006 (0.9066)  time: 0.0494  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3735 (0.9003)  labels_encoder: 0.3735 (0.9003)  labels_encoder_unscaled: 0.3735 (0.9003)  time: 0.0420  data: 0.0001  max mem: 1206
Test: Total time: 0:00:34 (0.0619 s / it)
Averaged stats: loss: 0.3735 (0.9003)  labels_encoder: 0.3735 (0.9003)  labels_encoder_unscaled: 0.3735 (0.9003)
(21, 71496)
(21, 71496)
[Epoch-3] [IDU-tvseries_anet_features.pickle] mAP: 0.1299, mcAP: 0.8821

BaseballPitch: 0.0516
BasketballDunk: 0.0947
Billiards: 0.0050
CleanAndJerk: 0.3986
CliffDiving: 0.4516
CricketBowling: 0.0795
CricketShot: 0.0939
Diving: 0.0056
FrisbeeCatch: 0.1391
GolfSwing: 0.0569
HammerThrow: 0.1127
HighJump: 0.0548
JavelinThrow: 0.0500
LongJump: 0.3037
PoleVault: 0.0939
Shotput: 0.1454
SoccerPenalty: 0.0523
TennisSwing: 0.2016
ThrowDiscus: 0.0547
VolleyballSpiking: 0.1522
Epoch: [4]  [   0/1511]  eta: 0:51:04  lr: 0.000000  loss: 0.7098 (0.7098)  labels_encoder: 0.7098 (0.7098)  labels_encoder_unscaled: 0.7098 (0.7098)  time: 2.0281  data: 1.8721  max mem: 1206
Epoch: [4]  [  50/1511]  eta: 0:03:54  lr: 0.000000  loss: 0.5339 (0.5246)  labels_encoder: 0.5339 (0.5246)  labels_encoder_unscaled: 0.5339 (0.5246)  time: 0.1202  data: 0.0002  max mem: 1206
Epoch: [4]  [ 100/1511]  eta: 0:03:19  lr: 0.000000  loss: 0.5821 (0.5359)  labels_encoder: 0.5821 (0.5359)  labels_encoder_unscaled: 0.5821 (0.5359)  time: 0.1179  data: 0.0002  max mem: 1206
Epoch: [4]  [ 150/1511]  eta: 0:03:04  lr: 0.000000  loss: 0.5494 (0.5340)  labels_encoder: 0.5494 (0.5340)  labels_encoder_unscaled: 0.5494 (0.5340)  time: 0.1222  data: 0.0002  max mem: 1206
Epoch: [4]  [ 200/1511]  eta: 0:02:53  lr: 0.000000  loss: 0.5143 (0.5341)  labels_encoder: 0.5143 (0.5341)  labels_encoder_unscaled: 0.5143 (0.5341)  time: 0.1248  data: 0.0002  max mem: 1206
Epoch: [4]  [ 250/1511]  eta: 0:02:44  lr: 0.000000  loss: 0.5236 (0.5319)  labels_encoder: 0.5236 (0.5319)  labels_encoder_unscaled: 0.5236 (0.5319)  time: 0.1263  data: 0.0002  max mem: 1206
Epoch: [4]  [ 300/1511]  eta: 0:02:36  lr: 0.000000  loss: 0.4802 (0.5287)  labels_encoder: 0.4802 (0.5287)  labels_encoder_unscaled: 0.4802 (0.5287)  time: 0.1231  data: 0.0002  max mem: 1206
Epoch: [4]  [ 350/1511]  eta: 0:02:30  lr: 0.000000  loss: 0.5389 (0.5311)  labels_encoder: 0.5389 (0.5311)  labels_encoder_unscaled: 0.5389 (0.5311)  time: 0.1385  data: 0.0002  max mem: 1206
Epoch: [4]  [ 400/1511]  eta: 0:02:24  lr: 0.000000  loss: 0.5024 (0.5289)  labels_encoder: 0.5024 (0.5289)  labels_encoder_unscaled: 0.5024 (0.5289)  time: 0.1303  data: 0.0004  max mem: 1206
Epoch: [4]  [ 450/1511]  eta: 0:02:17  lr: 0.000000  loss: 0.5348 (0.5278)  labels_encoder: 0.5348 (0.5278)  labels_encoder_unscaled: 0.5348 (0.5278)  time: 0.1298  data: 0.0002  max mem: 1206
Epoch: [4]  [ 500/1511]  eta: 0:02:11  lr: 0.000000  loss: 0.5433 (0.5289)  labels_encoder: 0.5433 (0.5289)  labels_encoder_unscaled: 0.5433 (0.5289)  time: 0.1253  data: 0.0002  max mem: 1206
Epoch: [4]  [ 550/1511]  eta: 0:02:04  lr: 0.000000  loss: 0.5198 (0.5283)  labels_encoder: 0.5198 (0.5283)  labels_encoder_unscaled: 0.5198 (0.5283)  time: 0.1234  data: 0.0002  max mem: 1206
Epoch: [4]  [ 600/1511]  eta: 0:01:57  lr: 0.000000  loss: 0.5140 (0.5264)  labels_encoder: 0.5140 (0.5264)  labels_encoder_unscaled: 0.5140 (0.5264)  time: 0.1197  data: 0.0002  max mem: 1206
Epoch: [4]  [ 650/1511]  eta: 0:01:50  lr: 0.000000  loss: 0.4918 (0.5269)  labels_encoder: 0.4918 (0.5269)  labels_encoder_unscaled: 0.4918 (0.5269)  time: 0.1209  data: 0.0002  max mem: 1206
Epoch: [4]  [ 700/1511]  eta: 0:01:44  lr: 0.000000  loss: 0.5286 (0.5271)  labels_encoder: 0.5286 (0.5271)  labels_encoder_unscaled: 0.5286 (0.5271)  time: 0.1444  data: 0.0002  max mem: 1206
Epoch: [4]  [ 750/1511]  eta: 0:01:38  lr: 0.000000  loss: 0.5115 (0.5268)  labels_encoder: 0.5115 (0.5268)  labels_encoder_unscaled: 0.5115 (0.5268)  time: 0.1422  data: 0.0002  max mem: 1206
Epoch: [4]  [ 800/1511]  eta: 0:01:32  lr: 0.000000  loss: 0.5420 (0.5256)  labels_encoder: 0.5420 (0.5256)  labels_encoder_unscaled: 0.5420 (0.5256)  time: 0.1342  data: 0.0002  max mem: 1206
Epoch: [4]  [ 850/1511]  eta: 0:01:26  lr: 0.000000  loss: 0.5425 (0.5260)  labels_encoder: 0.5425 (0.5260)  labels_encoder_unscaled: 0.5425 (0.5260)  time: 0.1303  data: 0.0002  max mem: 1206
Epoch: [4]  [ 900/1511]  eta: 0:01:19  lr: 0.000000  loss: 0.5492 (0.5265)  labels_encoder: 0.5492 (0.5265)  labels_encoder_unscaled: 0.5492 (0.5265)  time: 0.1324  data: 0.0002  max mem: 1206
Epoch: [4]  [ 950/1511]  eta: 0:01:13  lr: 0.000000  loss: 0.5006 (0.5253)  labels_encoder: 0.5006 (0.5253)  labels_encoder_unscaled: 0.5006 (0.5253)  time: 0.1301  data: 0.0002  max mem: 1206
Epoch: [4]  [1000/1511]  eta: 0:01:06  lr: 0.000000  loss: 0.5371 (0.5252)  labels_encoder: 0.5371 (0.5252)  labels_encoder_unscaled: 0.5371 (0.5252)  time: 0.1304  data: 0.0002  max mem: 1206
Epoch: [4]  [1050/1511]  eta: 0:01:00  lr: 0.000000  loss: 0.5530 (0.5261)  labels_encoder: 0.5530 (0.5261)  labels_encoder_unscaled: 0.5530 (0.5261)  time: 0.1381  data: 0.0002  max mem: 1206
Epoch: [4]  [1100/1511]  eta: 0:00:53  lr: 0.000000  loss: 0.4910 (0.5257)  labels_encoder: 0.4910 (0.5257)  labels_encoder_unscaled: 0.4910 (0.5257)  time: 0.1389  data: 0.0002  max mem: 1206
Epoch: [4]  [1150/1511]  eta: 0:00:47  lr: 0.000000  loss: 0.4731 (0.5253)  labels_encoder: 0.4731 (0.5253)  labels_encoder_unscaled: 0.4731 (0.5253)  time: 0.1437  data: 0.0002  max mem: 1206
Epoch: [4]  [1200/1511]  eta: 0:00:40  lr: 0.000000  loss: 0.5437 (0.5254)  labels_encoder: 0.5437 (0.5254)  labels_encoder_unscaled: 0.5437 (0.5254)  time: 0.1266  data: 0.0002  max mem: 1206
Epoch: [4]  [1250/1511]  eta: 0:00:34  lr: 0.000000  loss: 0.5263 (0.5259)  labels_encoder: 0.5263 (0.5259)  labels_encoder_unscaled: 0.5263 (0.5259)  time: 0.1221  data: 0.0002  max mem: 1206
Epoch: [4]  [1300/1511]  eta: 0:00:27  lr: 0.000000  loss: 0.4903 (0.5256)  labels_encoder: 0.4903 (0.5256)  labels_encoder_unscaled: 0.4903 (0.5256)  time: 0.1351  data: 0.0002  max mem: 1206
Epoch: [4]  [1350/1511]  eta: 0:00:21  lr: 0.000000  loss: 0.5286 (0.5263)  labels_encoder: 0.5286 (0.5263)  labels_encoder_unscaled: 0.5286 (0.5263)  time: 0.1374  data: 0.0002  max mem: 1206
Epoch: [4]  [1400/1511]  eta: 0:00:14  lr: 0.000000  loss: 0.5186 (0.5270)  labels_encoder: 0.5186 (0.5270)  labels_encoder_unscaled: 0.5186 (0.5270)  time: 0.1468  data: 0.0002  max mem: 1206
Epoch: [4]  [1450/1511]  eta: 0:00:08  lr: 0.000000  loss: 0.5631 (0.5272)  labels_encoder: 0.5631 (0.5272)  labels_encoder_unscaled: 0.5631 (0.5272)  time: 0.1336  data: 0.0002  max mem: 1206
Epoch: [4]  [1500/1511]  eta: 0:00:01  lr: 0.000000  loss: 0.5199 (0.5275)  labels_encoder: 0.5199 (0.5275)  labels_encoder_unscaled: 0.5199 (0.5275)  time: 0.1260  data: 0.0003  max mem: 1206
Epoch: [4]  [1510/1511]  eta: 0:00:00  lr: 0.000000  loss: 0.5056 (0.5273)  labels_encoder: 0.5056 (0.5273)  labels_encoder_unscaled: 0.5056 (0.5273)  time: 0.1129  data: 0.0002  max mem: 1206
Epoch: [4] Total time: 0:03:19 (0.1323 s / it)
Averaged stats: lr: 0.000000  loss: 0.5056 (0.5273)  labels_encoder: 0.5056 (0.5273)  labels_encoder_unscaled: 0.5056 (0.5273)
Test:  [  0/559]  eta: 0:17:26  loss: 0.7032 (0.7032)  labels_encoder: 0.7032 (0.7032)  labels_encoder_unscaled: 0.7032 (0.7032)  time: 1.8727  data: 1.8100  max mem: 1206
Test:  [ 50/559]  eta: 0:00:48  loss: 0.7609 (0.8840)  labels_encoder: 0.7609 (0.8840)  labels_encoder_unscaled: 0.7609 (0.8840)  time: 0.0561  data: 0.0002  max mem: 1206
Test:  [100/559]  eta: 0:00:35  loss: 0.5359 (0.8472)  labels_encoder: 0.5359 (0.8472)  labels_encoder_unscaled: 0.5359 (0.8472)  time: 0.0589  data: 0.0002  max mem: 1206
Test:  [150/559]  eta: 0:00:28  loss: 0.5491 (0.8663)  labels_encoder: 0.5491 (0.8663)  labels_encoder_unscaled: 0.5491 (0.8663)  time: 0.0548  data: 0.0002  max mem: 1206
Test:  [200/559]  eta: 0:00:23  loss: 0.5832 (0.8079)  labels_encoder: 0.5832 (0.8079)  labels_encoder_unscaled: 0.5832 (0.8079)  time: 0.0521  data: 0.0002  max mem: 1206
Test:  [250/559]  eta: 0:00:19  loss: 0.7300 (0.9228)  labels_encoder: 0.7300 (0.9228)  labels_encoder_unscaled: 0.7300 (0.9228)  time: 0.0636  data: 0.0002  max mem: 1206
Test:  [300/559]  eta: 0:00:16  loss: 1.1099 (0.9796)  labels_encoder: 1.1099 (0.9796)  labels_encoder_unscaled: 1.1099 (0.9796)  time: 0.0637  data: 0.0002  max mem: 1206
Test:  [350/559]  eta: 0:00:13  loss: 0.7842 (1.0009)  labels_encoder: 0.7842 (1.0009)  labels_encoder_unscaled: 0.7842 (1.0009)  time: 0.0611  data: 0.0002  max mem: 1206
Test:  [400/559]  eta: 0:00:09  loss: 0.3094 (0.9740)  labels_encoder: 0.3094 (0.9740)  labels_encoder_unscaled: 0.3094 (0.9740)  time: 0.0592  data: 0.0002  max mem: 1206
Test:  [450/559]  eta: 0:00:06  loss: 0.4209 (0.9494)  labels_encoder: 0.4209 (0.9494)  labels_encoder_unscaled: 0.4209 (0.9494)  time: 0.0608  data: 0.0002  max mem: 1206
Test:  [500/559]  eta: 0:00:03  loss: 0.6868 (0.9296)  labels_encoder: 0.6868 (0.9296)  labels_encoder_unscaled: 0.6868 (0.9296)  time: 0.0502  data: 0.0002  max mem: 1206
Test:  [550/559]  eta: 0:00:00  loss: 0.7737 (0.9030)  labels_encoder: 0.7737 (0.9030)  labels_encoder_unscaled: 0.7737 (0.9030)  time: 0.0498  data: 0.0001  max mem: 1206
Test:  [558/559]  eta: 0:00:00  loss: 0.3443 (0.8969)  labels_encoder: 0.3443 (0.8969)  labels_encoder_unscaled: 0.3443 (0.8969)  time: 0.0380  data: 0.0001  max mem: 1206
Test: Total time: 0:00:33 (0.0606 s / it)
Averaged stats: loss: 0.3443 (0.8969)  labels_encoder: 0.3443 (0.8969)  labels_encoder_unscaled: 0.3443 (0.8969)
(21, 71496)
(21, 71496)
[Epoch-4] [IDU-tvseries_anet_features.pickle] mAP: 0.1289, mcAP: 0.8836

BaseballPitch: 0.0395
BasketballDunk: 0.1020
Billiards: 0.0048
CleanAndJerk: 0.4009
CliffDiving: 0.4560
CricketBowling: 0.0887
CricketShot: 0.0999
Diving: 0.0060
FrisbeeCatch: 0.1256
GolfSwing: 0.0702
HammerThrow: 0.1339
HighJump: 0.0592
JavelinThrow: 0.0564
LongJump: 0.2679
PoleVault: 0.0937
Shotput: 0.1499
SoccerPenalty: 0.0546
TennisSwing: 0.1968
ThrowDiscus: 0.0330
VolleyballSpiking: 0.1393
Training time 0:15:49
